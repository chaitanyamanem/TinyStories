{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db4ff6cb",
   "metadata": {},
   "source": [
    "## Rotary Positional Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202b728",
   "metadata": {},
   "source": [
    "https://nn.labml.ai/transformers/rope/index.html\n",
    "\n",
    "https://blog.eleuther.ai/rotary-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e2c60c",
   "metadata": {},
   "source": [
    "### Transfromers Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5c4c35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b0b3d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:    \n",
    "    vocab_size = 4026\n",
    "    dim = 552\n",
    "    n_heads = 12\n",
    "    head_size = dim // n_heads\n",
    "    n_layers = 12\n",
    "    n_kv_heads = 3\n",
    "    seq_len = 1024\n",
    "    multiple_of = 256\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68c76d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RoPE(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.cos_mthetas = None\n",
    "        self.sin_mthetas = None\n",
    "        self.__set_thetas()\n",
    "        \n",
    "    def __set_thetas(self):\n",
    "        \"\"\"This sets the parameters of the rope as per the formula\n",
    "        Θ = {θi = 10000−2(i−1)/d, i ∈ [1, 2, ..., d/2]}\n",
    "        \"\"\"\n",
    "        assert self.config.head_size % 2 == 0, f\"Head size:{self.config.head_size} shouls be even number\"\n",
    "        self.d = self.config.head_size\n",
    "        self.m = self.config.seq_len\n",
    "        \n",
    "        i_s = torch.tensor(range(1,self.d//2+1))\n",
    "        i_s = torch.cat([i_s,i_s], axis=-1)\n",
    "        m_s = torch.tensor(range(self.m)).unsqueeze(axis=-1)\n",
    "        \n",
    "        thetas = 10000 ** (-2*(i_s-1)/self.d)\n",
    "        self.cos_mthetas = torch.cos(m_s * thetas).to(\"cuda\")\n",
    "        self.sin_mthetas = torch.sin(m_s * thetas).to(\"cuda\")\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" assumes x in the format of \"\"\"\n",
    "        d  = x.shape[-1]\n",
    "        print(f\"shapes in RoPE in oredr x, : {x.shape, self.cos_mthetas.shape},d:{d}\")\n",
    "        assert d == self.d\n",
    "        \n",
    "        x = self.cos_mthetas * x + self.sin_mthetas * torch.cat([-1 * x[:,:,:,self.d//2:],x[:,:,:,:self.d//2]], axis=-1)\n",
    "        return x\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "239bfac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = config.head_size\n",
    "m = config.seq_len\n",
    "\n",
    "i_s = torch.tensor(range(1,d//2+1))\n",
    "i_s = torch.cat([i_s,i_s], axis=-1)\n",
    "m_s = torch.tensor(range(m)).unsqueeze(axis=-1)\n",
    "\n",
    "thetas = 10000 ** (-2*(i_s-1)/d)\n",
    "cos_mthetas = torch.cos(m_s * thetas)\n",
    "sin_mthetas = torch.sin(m_s * thetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c06da1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1024, 1]),\n",
       " torch.Size([46]),\n",
       " torch.Size([1024, 46]),\n",
       " torch.Size([1024, 46]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_s.shape, thetas.shape, cos_mthetas.shape, sin_mthetas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b9b447f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(size=(4, 12, 16, d))\n",
    "d  = x.shape[-1]\n",
    "t = x.shape[-2]\n",
    "x = cos_mthetas[:t,:] * x + sin_mthetas[:t,:] * torch.cat([-1 * x[:,:,:,d//2:],x[:,:,:,:d//2]], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b152d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 12, 16, 46])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5801d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_rep = self.config.n_heads // self.config.n_kv_heads\n",
    "        self.query = nn.Linear(self.config.dim, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        self.key = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.value = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.rope = RoPE(self.config)\n",
    "        self.proj = nn.Linear(self.config.n_heads * self.config.head_size, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        \n",
    "    def forward(self, x, y=None):\n",
    "        b,t,d = x.shape\n",
    "        q = self.query(x).view(b,t,self.config.n_heads,self.config.head_size)\n",
    "        k = self.key(x).view(b,t,self.config.n_kv_heads,self.config.head_size)\n",
    "        v = self.value(x).view(b,t,self.config.n_kv_heads,self.config.head_size)\n",
    "        print(f\"Shapes of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
    "        \n",
    "        ## Add rotary embeddings        \n",
    "        q = self.rope(q.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        k = self.rope(k.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        \n",
    "        ##GQA \n",
    "        k = repeat_kv(k,self.n_rep)\n",
    "        v = repeat_kv(v,self.n_rep)\n",
    "        print(f\"Shapes of Q, K, V: {q.shape, k.shape, v.shape}\")\n",
    "        # make heads into a batch dimension\n",
    "        q = q.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        ## Flash attention\n",
    "        x = nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "         # restore time as batch dimension and concat heads\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, -1)\n",
    "\n",
    "        # final projection into the residual stream\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b9cddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwordNetwork(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config        \n",
    "        hidden_units = int(self.config.dim * 4 * 2/3) ##\n",
    "        hidden_units = int(hidden_units - (hidden_units % self.config.multiple_of) + self.config.multiple_of)\n",
    "        \n",
    "        self.w = nn.Linear(self.config.dim, hidden_units)\n",
    "        self.v = nn.Linear(self.config.dim, hidden_units)\n",
    "        self.w2 = nn.Linear(hidden_units, self.config.dim)\n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.w2(self.silu(self.w(x)) * self.v(x))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43ef5958",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.dim = config.dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91314226",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.mha = MultiHeadAttention(self.config)\n",
    "        self.ffn = FeedForwordNetwork(self.config)\n",
    "        self.anorm = RMSNorm(config)\n",
    "        self.fnorm = RMSNorm(config)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x + self.mha(self.anorm(x))\n",
    "        x = x + self.ffn(self.fnorm(x))\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d366db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.embedding_layer = nn.Embedding(self.config.vocab_size, self.config.dim)\n",
    "        self.layers = nn.Sequential(*[AttentionLayer(self.config) for _ in range(self.config.n_layers)])\n",
    "        self.hnorm = RMSNorm(config)\n",
    "        self.clf_head = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.layers(x)\n",
    "        x = self.hnorm(x)\n",
    "        x = self.clf_head(x)\n",
    "        return x\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "6be5f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n",
      "1365\n",
      "1536\n"
     ]
    }
   ],
   "source": [
    "multiple_of = 256\n",
    "hidden_dim = 4 * 512\n",
    "print(hidden_dim)\n",
    "hidden_dim = int(2 * hidden_dim / 3)\n",
    "print(hidden_dim)\n",
    "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
    "print(hidden_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397f70d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f74aa12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3e1c7679",
   "metadata": {},
   "source": [
    "## Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "529e9cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b8b903dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.0"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "516 / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "24ce3f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "552"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "46 * 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "cdb7fc88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "552 / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "548b3ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = Attention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "5717e951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(16, config.seq_len, config.dim)) * 1.0\n",
    "out = attention(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "c5a4994e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024, 552]), torch.Size([16, 1024, 552]))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "3446f826",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff = FeedForword(config)\n",
    "ffout = ff(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "cc29eedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024, 552]),\n",
       " torch.Size([16, 1024, 552]),\n",
       " torch.Size([16, 1024, 552]))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, out.shape, ffout.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e0f8eec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 8, 64])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.view(16, config.seq_len, config.n_heads, config.head_size)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737e0c8d",
   "metadata": {},
   "source": [
    "## AttentionLayer Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "22923d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "torch.Size([16, 1024, 552]) torch.Size([16, 1024, 552])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(16, config.seq_len, config.dim)) * 1.0\n",
    "layer = AttentionLayer(config)\n",
    "out = layer(x)\n",
    "print(x.shape, out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c3c833",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fa67888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 3, 46]), torch.Size([16, 1024, 3, 46]))\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 12, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "shapes in RoPE in oredr x, : (torch.Size([16, 3, 1024, 46]), torch.Size([1024, 46])),d:46\n",
      "Shapes of Q, K, V: (torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]), torch.Size([16, 1024, 12, 46]))\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.16 GiB is allocated by PyTorch, and 131.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m, config\u001b[38;5;241m.\u001b[39mseq_len), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(config)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape, out\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, y)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x)\n\u001b[1;32m---> 12\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhnorm(x)\n\u001b[0;32m     14\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf_head(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 11\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 11\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfnorm(x))\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB. GPU 0 has a total capacty of 6.00 GiB of which 0 bytes is free. Of the allocated memory 5.16 GiB is allocated by PyTorch, and 131.97 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "x = torch.ones(size=(16, config.seq_len), dtype=torch.int32).to(\"cuda\")\n",
    "model = Model(config).to(\"cuda\")\n",
    "out = model(x)\n",
    "print(x.shape, out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce82b0ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "#######Total parameter of the model: 44.17053\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\n#######Total parameter of the model: {total_params * 1e-6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf50bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "218cf000",
   "metadata": {},
   "outputs": [],
   "source": [
    "xq = torch.rand(size=(16, config.seq_len, config.n_heads, config.head_size))\n",
    "xq_r, xq_i = xq.float().reshape(xq.shape[:-1] + (-1, 2)).unbind(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "71748ac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 8, -1, 2])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.shape[:-1] + (-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "4473b4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024, 8, 32, 2])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xq.float().reshape(xq.shape[:-1] + (-1, 2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa3936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b99a10b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rope = RoPE(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "01843ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(size=(16, config.seq_len, config.dim)) * 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "2d2c02c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 1024, 512]), torch.Size([16, 1024, 8, 64]))"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x.view(16, config.seq_len, config.n_heads, config.head_size).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9b500585",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.view(16, config.seq_len, config.n_heads, config.head_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a7c3265f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.permute(0, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "943f08ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 8, 1024, 64])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c44aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfec69fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "337db8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_rope = rope(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "27f44d35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16, 8, 1024, 64]), torch.Size([16, 8, 1024, 64]))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, x_rope.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7b1d9131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1.]), tensor([1., 1.]))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,0,:], x_rope[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "43e4df5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 2])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rope[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c519eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "996ec059",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(x=x_rope[0,:1,0], y=x_rope[0,:1,1])\n",
    "plt.xlim((-2,2))\n",
    "plt.ylim((-2,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2e9fa0a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 2])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_rope[0,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "bb46c192",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[(0.0, 0.0, i),tuple(point +[i,])] for i,point in enumerate(x_rope[0,:,:].numpy().tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "497d7ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.0, 0.0, 0),\n",
       " (1.0, 1.0, 0),\n",
       " (0.0, 0.0, 1),\n",
       " (-0.30116862058639526, 1.3817732334136963, 1),\n",
       " (0.0, 0.0, 2)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [row for sub in data for row in sub]\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b14ead7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>t</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.301169</td>\n",
       "      <td>1.381773</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2  t\n",
       "0  0.000000  0.000000  0\n",
       "1  1.000000  1.000000  0\n",
       "2  0.000000  0.000000  1\n",
       "3 -0.301169  1.381773  1\n",
       "4  0.000000  0.000000  2"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data=data, columns=[\"x1\",\"x2\",\"t\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "341163f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lineplot(x=\"x1\", y=\"x2\",hue=\"t\",data=df)\n",
    "plt.xlim((-2,2))\n",
    "plt.ylim((-2,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9d1202b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.31338733434677124, -1.3790533542633057)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple( [0.31338733434677124, -1.3790533542633057])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "34ff6c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.301169</td>\n",
       "      <td>1.381773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.325444</td>\n",
       "      <td>0.493151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.131112</td>\n",
       "      <td>-0.848872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.103159</td>\n",
       "      <td>-1.410446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>-0.469731</td>\n",
       "      <td>1.333924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>-1.376255</td>\n",
       "      <td>0.325457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>-1.017456</td>\n",
       "      <td>-0.982233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>0.276787</td>\n",
       "      <td>-1.386863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>1.316554</td>\n",
       "      <td>-0.516417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1024 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            x1        x2\n",
       "0     1.000000  1.000000\n",
       "1    -0.301169  1.381773\n",
       "2    -1.325444  0.493151\n",
       "3    -1.131112 -0.848872\n",
       "4     0.103159 -1.410446\n",
       "...        ...       ...\n",
       "1019 -0.469731  1.333924\n",
       "1020 -1.376255  0.325457\n",
       "1021 -1.017456 -0.982233\n",
       "1022  0.276787 -1.386863\n",
       "1023  1.316554 -0.516417\n",
       "\n",
       "[1024 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame(x_rope[0,:,:], columns=['x1','x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a2524461",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "507aab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nn.Linear(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a750f03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(5, 2)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "62d4bc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = l(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1be4c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f15d4032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('weight', Parameter containing:\n",
      "tensor([[ 0.6814, -0.5939]], requires_grad=True))\n",
      "('bias', Parameter containing:\n",
      "tensor([-0.5288], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "for param  in l.named_parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "27879489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5985, 6.5285]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7b6b0e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ToyModel, self).__init__()\n",
    "        self.net1 = nn.Linear(10, 15)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.net2 = nn.Linear(15, 5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net2(self.relu(self.net1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "cb1c12f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ToyModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0610b673",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0299,  0.1744], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net1.weight[0,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81741375",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(32, 10)\n",
    "y = model(x)\n",
    "y.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d62138e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([15, 10]), torch.Size([15, 10]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net1.weight.grad.shape, model.net1.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "6985cfe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.9570, -2.0191])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.net1.weight.grad[0,:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9fef0c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ef698994",
   "metadata": {},
   "outputs": [],
   "source": [
    "## configuration settings\n",
    "class Config:\n",
    "    \n",
    "    vocab_size = 4096\n",
    "    dim = 552\n",
    "    n_heads = 12\n",
    "    head_size = dim // n_heads\n",
    "    n_layers = 12\n",
    "    n_kv_heads = 3\n",
    "    seq_len = 1024\n",
    "    multiple_of = 256                \n",
    "    batch_size = 16 \n",
    "    global_batch_size = 100000 # number of tokens per update\n",
    "    grad_accumulation_steps = int(global_batch_size/(seq_len * batch_size))\n",
    "    learning_rate=5e-4\n",
    "    total_params = 0\n",
    "    tokenizer_path = \"saved_artifacts/tokenizers\"\n",
    "    n_train_examples = -1\n",
    "    n_val_examples = -1\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0b367b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4096"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Config.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90eaa1a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chaitanyamanem\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\huggingface_hub\\repocard.py:105: UserWarning: Repo card metadata block was not found. Setting CardData to empty.\n",
      "  warnings.warn(\"Repo card metadata block was not found. Setting CardData to empty.\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cd176c73c9428685106d26ab76dd82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/2119719 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dcf109433214505b443bacc053f15c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tinystories_dataset import TinyStories\n",
    "dataste = TinyStories(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3067b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloder = dataste.getTrainDataLoader(ddp=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb48211e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloder:\n",
    "    print(batch[\"inputs\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2fd0bb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iters now: 30254\n",
      "Iters earlier: 121016\n"
     ]
    }
   ],
   "source": [
    "print(f\"Iters now: {len(train_dataloder)}\")\n",
    "print(f\"Iters earlier: { 4 * len(train_dataloder)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c393c2cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(150_000/(1024 * 16 * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4eb9265d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98304"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3  * 1024 * 16 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03b16f3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 * 16 * 2 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d33e9948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 10.010102272033691 seconds ---\n"
     ]
    }
   ],
   "source": [
    "##\n",
    "import logging\n",
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "duration_secs = time.sleep(10)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ac5e47c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 61, 3690)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_secs = 3690\n",
    "duration_secs // (60*60), duration_secs // 60, duration_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dfa00a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "120 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a68538ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 30)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duration_secs // (60*60), (duration_secs % (60*60)) // 60, (duration_secs % (60*60)) % 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "014e75f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3691"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "61 * 60 + 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "44cefae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from datetime import datetime\n",
    "import os\n",
    "file_name = os.path.join(\"saved_artifacts\",\"logs\",datetime.now().strftime('log_%Y_%m_%d_%H_%M_%S.log'))\n",
    "logging.basicConfig(level=logging.DEBUG, filename=file_name, filemode='w', format='%(name)s - %(levelname)s - %(message)s')\n",
    "logging.info('Execution started at ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ec861bb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'strftime'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrftime\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlog_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM_\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mS.log\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'float' object has no attribute 'strftime'"
     ]
    }
   ],
   "source": [
    "time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c45e74d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'log_2024_02_10_10_44_39.log'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('log_%Y_%m_%d_%H_%M_%S.log')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "35d82731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:05.010611\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "time.sleep(5)\n",
    "print(datetime.now()-start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f621a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "100 - 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2af9b718",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15127.0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "30254 / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f1a1c7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67.8"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "60 * 1.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dd5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "1 step \n",
    "68/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "99661607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10286.36"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15127 * 0.68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1df089de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171.43933333333334"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10286.36 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8df0fd1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3276800000"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 * 16 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f60fb824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60508"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15127 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "91f45913",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1638400000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "400_000 * 4 * 1024 = ? * 16 * 1024 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "56f43f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000.0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1638400000 / (16 * 1024 * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7ade7ea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.128"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.208 - 0.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "76b8e817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dot product:26.0\n",
      "dot product:53.0\n",
      "dot product:36.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "vectors = np.array([\n",
    "    [1,0,0],\n",
    "    [1,3.0,4.0],\n",
    "    \n",
    "    [2,0,0],\n",
    "    [2,4.0,3.0],\n",
    "    \n",
    "    [3,0,0],\n",
    "    [3,10.0,5.0],\n",
    "    \n",
    "    [4,0,0],\n",
    "    [4,10.0,0.5],     \n",
    "    \n",
    "\n",
    "    \n",
    "])\n",
    "\n",
    "#similarity\n",
    "vector1 = vectors[1,:]\n",
    "for i in range(3,vectors.shape[0]):\n",
    "    if (i+1)%2 == 0:\n",
    "        vector2 = vectors[i,:]\n",
    "        print(f\"dot product:{(torch.tensor(vector1).unsqueeze(0) @ torch.tensor(vector2).unsqueeze(0).transpose(-2,-1)).item()}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "eb9e2b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# importing package \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "# plot lines \n",
    "sns.lineplot(x=vectors[:,1], y=vectors[:,2], hue=vectors[:,0]) \n",
    "plt.legend() \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "bbe8327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16.]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c48198d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0667, 0.6802]]), tensor([[0.5924, 0.9558]]))"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1, vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c7719468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2315]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector1 @ torch.transpose(vector1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b2cc1774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3271, 0.8240]]), tensor([[0.6256, 0.8163]]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "afa03737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cosine_similarity(vector1,vector1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9a76ba9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[32.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector1 @ torch.transpose(vector1,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "875290a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57e7be71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "warmup_iters = 2000\n",
    "max_iters = 100_000\n",
    "lr_decay_iters = max_iters\n",
    "learning_rate = 7e-4\n",
    "min_lr = learning_rate / 10\n",
    "\n",
    "\n",
    "def get_lr(it):\n",
    "    # 1) linear warmup for warmup_iters steps\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "    # 2) if it > lr_decay_iters, return min learning rate\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    # 3) in between, use cosine decay down to min learning rate\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))  # coeff ranges 0..1\n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1292c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "lrs = [get_lr(it) for it in range(max_iters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b47284ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.lineplot(x=list(range(len(lrs))), y=lrs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "508c0355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.5e-07, 7e-07, 1.05e-06, 1.4e-06]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrs[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53695411",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5e-05"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5e-4 / 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4771cfa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.array([4,3]) @ np.array([5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "82448d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.75"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87ae215b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def unit_vector(vector):\n",
    "    \"\"\" Returns the unit vector of the vector.  \"\"\"\n",
    "    return vector / np.linalg.norm(vector)\n",
    "\n",
    "def angle_between(v1, v2):\n",
    "    \"\"\" Returns the angle in radians between vectors 'v1' and 'v2'::\n",
    "\n",
    "            >>> angle_between((1, 0, 0), (0, 1, 0))\n",
    "            1.5707963267948966\n",
    "            >>> angle_between((1, 0, 0), (1, 0, 0))\n",
    "            0.0\n",
    "            >>> angle_between((1, 0, 0), (-1, 0, 0))\n",
    "            3.141592653589793\n",
    "    \"\"\"\n",
    "    v1_u = unit_vector(v1)\n",
    "    v2_u = unit_vector(v2)\n",
    "    return np.arccos(np.clip(np.dot(v1_u, v2_u), -1.0, 1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c22792da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4461055489434035"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_between(np.array([4,3]), np.array([5,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cfd4ca95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0990195135927845, 5.1478150704935)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "math.sqrt((5 ** 2 + 1 ** 2)), math.sqrt((4.5 ** 2 + 2.5 ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "602e451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([4,3]) @ np.array([5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8df37902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.5"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([4,3]) @ np.array([2.5,4.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1d4f6522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.346153846153847"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((23/25.495097567963924) * 5) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "41551e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.782"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20.346/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "547aa07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.0, 3.111269837220809)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(2.4**2 + 1.8**2), math.sqrt(2.2**2 + 2.2 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cb83a0bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.9841204734393685, 3.110029099542318)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.sqrt(-0.217**2 + 2.992**2), math.sqrt((-2.915)**2 + 1.084 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a8244bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.110029099542318"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7beb7db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.000244140625"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "1/4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d3e59383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0002441704297478"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0.000244140625)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79b098b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491.52"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1024 * 16 * 30000) / 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d946df10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49152"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1024 * 16 * 30000) / 1_000_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5bd0a897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40960.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.5 * 16 * 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de597496",
   "metadata": {},
   "source": [
    "## Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5c0901",
   "metadata": {},
   "source": [
    "##### Batch Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a25a9aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self, world_size):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 64 \n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = world_size\n",
    "        self.grad_accumulation_steps = int(self.global_batch_size/(self.seq_len * self.batch_size * self.world_size))\n",
    "        self.grad_clip = 1.0\n",
    "        self.learning_rate=1e-4\n",
    "        self.min_lr = self.learning_rate / 10\n",
    "        self.warmup_steps = 1000\n",
    "        self.total_params = 0\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"\n",
    "        self.saved_checkpoint_path = \"saved_artifacts/models/model240221\"\n",
    "        self.rank = 0\n",
    "        \n",
    "config = Config(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747280ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import tqdm\n",
    "from automodel import AutoModel\n",
    "\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor(model_file='saved_artifacts/tokenizers/tok_4096.model')    \n",
    "model = Model(config)\n",
    "model = AutoModel(model, config, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9751698d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Hello\",\n",
    "    \"Once there was a girl named Lilly\",\n",
    "    \"Rowdy baby\"\n",
    "]\n",
    "tokenized_prompts = []\n",
    "max_seq_len = 0\n",
    "for prompt in prompts:\n",
    "    tokenized_prompt = tokenizer.tokenize(prompt, add_bos=True, add_eos=True)\n",
    "    max_seq_len = max(len(tokenized_prompt) ,max_seq_len)\n",
    "    tokenized_prompts.append(tokenized_prompt)\n",
    "\n",
    "tokenized_prompts = [tokenized_prompt + [tokenizer.eos_id()]*(max_seq_len-len(tokenized_prompt))for tokenized_prompt in tokenized_prompts]\n",
    "    \n",
    "x = torch.tensor(tokenized_prompts, dtype=torch.long).to(\"cuda\")\n",
    "\n",
    "generation_config = {\n",
    "    'padding_token': tokenizer.eos_id(),\n",
    "    'bos_id': tokenizer.bos_id(),\n",
    "    'max_new_tokens': 500,\n",
    "    'temperature': 0.1\n",
    "}\n",
    "\n",
    "prompts, generations = model.generate(x,generation_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5289a0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = tokenizer.decode(prompts)\n",
    "generations = tokenizer.decode(generations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a855070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = [{'prompt': prompt, 'generation': generation} for prompt, generation in zip(prompts,generations)]\n",
    "result\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "keys = result[0].keys()\n",
    "\n",
    "with open('people.csv', 'w', newline='') as output_file:\n",
    "    dict_writer = csv.DictWriter(output_file, keys)\n",
    "    dict_writer.writeheader()\n",
    "    dict_writer.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a2e6f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello, little dog. I am a dog. I am a boy. I like to play with you. You are my friend. You are loyal. You are fun. You are fun. You are fun.\"\\n\\nThe boy and the dog run and play. They chase a ball and a stick. They laugh and bark. They are happy. They are friends.\\n\\nBut then, the boy and the dog see a big cat. The cat is black and white. The cat is mean. The cat hisses and scratches. The cat says, \"This is my park. You are not my friends. You are not fun. You are mean. You are mean. You are not loyal. You are bad.\"\\n\\nThe boy and the dog are scared. They do not like the cat. They do not like the cat. They do not like the cat. They want to go away.\\n\\nBut then, they hear a voice. The voice is loud and angry. The voice says, \"Hey, you! Leave the cat alone! He is my friend. He is my family. He is loyal. He is fun. He is fun. He is my friend.\"\\n\\nThe boy and the dog are surprised. They look at the cat. The cat is not mean. The cat is not bad. The cat is not mean. The cat is not loyal. The cat is not mean. The cat is not mean. The cat is not bad.\\n\\nThe boy and the dog are happy. They are not scared. They are not hurt. They are not mean. They are not fun. They are loyal. They are not fun.\\n\\nThey say, \"Thank you, cat. You are right. You are our friend. You are our friend. You are loyal. You are fun. You are fun. You are fun. You are fun. You are fun. You are our friend.\"\\n\\nThe cat smiles. The cat says, \"You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome, boy. You are welcome,',\n",
       " \"Once there was a girl named Lilly. Lilly was three years old and loved to play. One day, Lilly was playing in the garden when she saw a big, round, yellow squash. She was so excited and ran to pick it up.\\n\\nLilly held the squash in her hands and looked at it closely. She noticed that it was very flexible. She could bend it and twist it in all sorts of ways. Lilly was so happy and started to play with the squash.\\n\\nShe rolled it around and made it bounce. She even tried to make it do tricks. Lilly was having so much fun. She was laughing and smiling as she played with the squash.\\n\\nLilly played with the squash until it was time to go inside. She put the squash in her pocket and went inside. Lilly was so happy that she had found the squash and she couldn't wait to play with it again tomorrow.\",\n",
       " 'Rowdy baby was a little girl. She was three years old and loved to play. One day, Rowdy was playing in the garden when she saw a big, white cauliflower. She was so excited and ran to it.\\n\\nRowdy said, \"Hello, cauliflower!\"\\n\\nThe cauliflower replied, \"Hello, Rowdy!\"\\n\\nRowdy was so surprised that the cauliflower could talk. She asked, \"What are you doing here?\"\\n\\nThe cauliflower said, \"I\\'m here to help you. I\\'m here to make you a delicious dinner!\"\\n\\nRowdy was so happy and said, \"Yay! That sounds great!\"\\n\\nThe cauliflower smiled and said, \"Let\\'s get started!\"\\n\\nSo, Rowdy and the cauliflower worked together to make a delicious dinner. When it was finished, Rowdy said, \"Thank you for helping me, cauliflower!\"\\n\\nThe cauliflower replied, \"You\\'re welcome, Rowdy. It was my pleasure!\"']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ddae1d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import torch\n",
    "import sentencepiece as spm\n",
    "import tqdm\n",
    "from automodel import AutoModel\n",
    "\n",
    "\n",
    "\n",
    "model = Model(config)\n",
    "model = AutoModel(model, \"saved_artifacts/models/model240221\", config)\n",
    "checkpoint = torch.load(\"saved_artifacts/models/model240221/checkpoint.pt\")\n",
    "## load model state dict\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "## Tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file='saved_artifacts/tokenizers/tok_4096.model')        \n",
    "\n",
    "\n",
    "\n",
    "def generate(x, max_new_tokens=500, temperature=0.0):\n",
    "\n",
    "    idx = x\n",
    "    t = 0\n",
    "    #gen_loop = tqdm.tqdm(total=max_new_tokens, desc=\"gen_progress\")\n",
    "    while t <= max_new_tokens and int(idx[:,-1].item()) != int(tokenizer.bos_id()):\n",
    "        prompt = idx[:,-config.seq_len:]\n",
    "        pred = model(prompt)\n",
    "        logits = pred[:,-1,:]\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            # \"sample\" the single most likely index\n",
    "            _, idx_next = torch.topk(logits, k=1, dim=-1)\n",
    "        else:\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "        #logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)\n",
    "\n",
    "\n",
    "        logits = torch.softmax(logits, axis=-1)\n",
    "        next_idx = torch.multinomial(logits, num_samples=1)        \n",
    "        idx = torch.cat([idx, next_idx], axis=1)\n",
    "        t += 1\n",
    "        #gen_loop.update(1)\n",
    "    return idx\n",
    "    generated_text = tokenizer.decode(idx.to(\"cpu\").numpy().tolist())\n",
    "    gen_loop.write(\"\\n###############################\")\n",
    "    gen_loop.write(generated_text[0])\n",
    "    gen_loop.write(\"\\n###############################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "935c245d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b08ac598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day a little girl called Shivaed a little girl named Lily and her mommy were playing in the park. Lily was very excited because she was going to the park with her mom.\n",
      "\n",
      "When they arrived, Lily saw a big, red balloon. She was so excited and asked her mom, \"Can I have it?\"\n",
      "\n",
      "Her mom said, \"Yes, you can have it. But first, let's go to the store and get some food for dinner.\"\n",
      "\n",
      "Lily was so happy. She ran to the store and saw a big, red balloon. She asked her mom, \"Can I have it?\"\n",
      "\n",
      "Her mom said, \"Yes, you can have it. But first, let's get some food for dinner.\"\n",
      "\n",
      "So they went to the food aisle and Lily picked out some yummy food. She was so excited to eat it.\n",
      "\n",
      "When they got home, Lily and her mom had a big dinner. They ate the red food and it was so delicious. Lily was so happy and said, \"Thank you, Mommy!\"\n"
     ]
    }
   ],
   "source": [
    "max_prompt_len = 50\n",
    "prompt = \"One day a little girl called Shiva \"\n",
    "prompt_tokens = tokenizer.encode([prompt], add_bos=True)[0]\n",
    "padded_prompt_tokens = [tokenizer.bos_id()]*max_prompt_len\n",
    "padded_prompt_tokens[-len(prompt_tokens[:max_prompt_len]):] = prompt_tokens[:max_prompt_len]\n",
    "padded_prompt_tokens = torch.tensor([padded_prompt_tokens],dtype=torch.long).to(\"cuda\")\n",
    "idx = generate(padded_prompt_tokens, temperature=0.1)\n",
    "idx = idx.squeeze(axis=0).tolist()\n",
    "print(tokenizer.decode(idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7b0b0f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 422, 361]]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer.decode(reponse_ids)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5fc03f2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [1,2,3,4,5]\n",
    "l[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1c8c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gen_progress:   0%|                                                                            | 0/500 [39:22<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "prompts = [\"One day\",\"Siva and bear both are friends\"]\n",
    "reponse_ids = []\n",
    "for prompt in prompts:\n",
    "    x = torch.tensor(tokenizer.encode([prompt]), dtype=torch.long).to(\"cuda\")\n",
    "    idx = generate(x, temperature=0.1)\n",
    "    reponse_ids.append(idx.squeeze(axis=0).tolist())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0d425240",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siva and bear both are friends. They like to play in the park. They have a ball, a bike and a kite. They have fun.\n",
      "\n",
      "One day, they see a big dog in the park. The dog is brown and furry. It has a collar and a leash. The dog barks and runs to them.\n",
      "\n",
      "\"Hello, dog!\" Anna says. She wants to pet the dog. She likes dogs.\n",
      "\n",
      "\"Be careful, Anna!\" Ben says. He is afraid of dogs. He thinks the dog might bite. He holds Anna's hand.\n",
      "\n",
      "The dog stops barking. It wags its tail. It wants to play. It sees the ball and the bike. It likes them. It runs to them.\n",
      "\n",
      "\"Wow, a dog!\" Anna says. She is happy. She likes dogs. She pets the dog.\n",
      "\n",
      "\"Be careful, Anna!\" Ben says. He is worried. He does not like dogs. He thinks dogs are mean. He pulls Anna's hand.\n",
      "\n",
      "The dog does not understand. It thinks Anna and Ben are playing. It jumps and bites the ball. It makes a mess.\n",
      "\n",
      "\"Ow, ow, ow!\" Anna and Ben say. They are sad. They are angry. They do not like the dog.\n",
      "\n",
      "The dog's owner comes. He is a man. He has a hat and a coat. He sees Anna and Ben. He is sorry.\n",
      "\n",
      "\"I'm sorry, kids!\" he says. He is not angry. He is kind. He smiles and pets the dog.\n",
      "\n",
      "\"It's okay, mister!\" Anna says. She is not mad. She likes the dog. She pets the dog back.\n",
      "\n",
      "\"Can we play with your dog?\" Ben asks. He is not scared. He likes dogs. He thinks dogs are fun.\n",
      "\n",
      "\"Sure, kids!\" the man says. He is friendly. He likes kids. He likes dogs. He likes kids.\n",
      "\n",
      "Anna and Ben are happy. They play with the dog. They throw the ball and the kite. They run and laugh.\n",
      "\n",
      "They forget about the dog. They forget about the mess. They have fun.\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(reponse_ids)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76e54920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One day, they saw a big truck with a picture of a cone on it. The truck was selling cones.\n",
      "\n",
      "\"Look, Ben, cones!\" Lily said. \"Let's go and see them.\"\n",
      "\n",
      "They ran to the truck and saw a man with a hat. He smiled and said, \"Hello, kids. Do you want a cone?\"\n",
      "\n",
      "\"Yes, please!\" Ben and Lily said.\n",
      "\n",
      "The man gave them each a cone with a scoop of ice cream on top. Lily chose chocolate and Ben chose vanilla.\n",
      "\n",
      "\"Thank you, mister!\" they said.\n",
      "\n",
      "\"You're welcome, kids. Enjoy your cones,\" the man said.\n",
      "\n",
      "Lily and Ben licked their cones happily. They liked the ice cream. It was cold and sweet and yummy.\n",
      "\n",
      "But then, a big dog came running. It saw the cones and wanted to play. It barked and jumped and tried to catch the cones.\n",
      "\n",
      "\"Hey, stop!\" Lily shouted. \"That's our cones!\"\n",
      "\n",
      "\"Go away, dog!\" Ben yelled. \"You're bad!\"\n",
      "\n",
      "The dog did not listen. It kept barking and jumping and trying to get the cones.\n",
      "\n",
      "Lily and Ben were scared. They did not like the dog. They wanted to go away.\n",
      "\n",
      "But then, a man came. He was the dog's owner. He saw the dog and the kids and the cones.\n",
      "\n",
      "\"Rex, come here!\" he called. \"Bad dog! Leave the kids alone!\"\n",
      "\n",
      "The dog heard the man and stopped barking. It ran to the man and licked his face. The man put a leash on the dog and said sorry to Lily and Ben.\n",
      "\n",
      "\"I'm sorry, kids. Rex is a friendly dog, but he likes to play. He didn't mean to scare you or your cones. He just wanted to have fun.\"\n",
      "\n",
      "Lily and Ben looked at the man and the dog. They saw that the dog was not mean. He was just playful.\n",
      "\n",
      "\"It's okay, mister,\" Lily said. \"But next time, please keep him on a leash.\"\n",
      "\n",
      "\"OK, kids,\" the man said. \"I will. Have fun with your cones. Bye-bye.\"\n",
      "\n",
      "He waved and\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(reponse_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "466c9c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 422,  361, 3968,  349, 1543,  261,  296, 1856, 2091,  780,  589,  285,\n",
       "         390, 2831, 3960,  317,  331,  282,  462,  857, 1828,  640, 2123,  320,\n",
       "         265,  538, 3946, 3960,   13,   13, 1993,  659, 3974, 3948, 1772,  436,\n",
       "        3968,  411,  265,  296,  340,  780,  595,  393,  298,  690,  312,  268,\n",
       "         261, 1663, 3960,  289,  896,  595, 2024, 1950,  430,  269, 2408,  267,\n",
       "         719,  285,  841, 3968,  411,  349, 1568, 3974, 3948,  949,  268,  335,\n",
       "         297,  436, 3960,  315,  936, 1341,  267, 1341, 3968,  411,  594,  341,\n",
       "        1219, 2071,  267, 1194, 3960,   13,   13, 1786, 2831,  595, 1357,  595,\n",
       "         302, 3951,  372,  595,  393,  298,  690,  312,  267,  619,  306,  285,\n",
       "         398,  371,  561, 1690,  267, 3283, 3960,  315,  285, 1851,  268, 1663,\n",
       "        3968,  411,  875, 2011, 1826,  574,  421, 1684,  856,  261, 1118,  690,\n",
       "        3957,  351,  595, 3947, 1600, 3960,  315,  816,  362, 1509,  306, 3974,\n",
       "        3954,  387,  772,  664, 1708,  885, 3968,  411,  979,  664, 1018, 1720,\n",
       "         267, 1235,  493, 3960,    1], device='cuda:0')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f7bc3400",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = torch.rand(size=(4,8))\n",
    "row = torch.arange(0,4).unsqueeze(axis=-1)\n",
    "col = torch.tensor([[0,2,2,4]]).permute(1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1856f76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4889, 0.3619, 0.2616, 0.3882, 0.1020, 0.4791, 0.0825, 0.2193],\n",
       "        [0.6540, 0.8920, 0.8110, 0.4591, 0.1197, 0.1988, 0.8841, 0.2273],\n",
       "        [0.9916, 0.4762, 0.6308, 0.6293, 0.4023, 0.5723, 0.6750, 0.9671],\n",
       "        [0.4034, 0.6587, 0.9476, 0.7676, 0.5436, 0.4817, 0.8547, 0.2095]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "697b5a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4889],\n",
       "        [0.8110],\n",
       "        [0.6308],\n",
       "        [0.5436]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[row,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "65872b98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [2],\n",
       "        [2],\n",
       "        [4]])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4b15933f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [5]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e9185a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "138534f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "prompts = [\n",
    "    \"Hello\",\n",
    "    \"Once there was a girl named Lilly\",\n",
    "    \"Rowdy baby\"\n",
    "]\n",
    "tokenized_prompts = []\n",
    "max_seq_len = 0\n",
    "for prompt in prompts:\n",
    "    tokenized_prompt = tokenizer.tokenize(prompt, add_bos=True, add_eos=True)\n",
    "    max_seq_len = max(len(tokenized_prompt) ,max_seq_len)\n",
    "    tokenized_prompts.append(tokenized_prompt)\n",
    "\n",
    "tokenized_prompts = [tokenized_prompt + [tokenizer.eos_id()]*(max_seq_len-len(tokenized_prompt))for tokenized_prompt in tokenized_prompts]\n",
    "    \n",
    "x = torch.tensor(tokenized_prompts, dtype=torch.long).to(\"cuda\")\n",
    "c_indi = torch.argmax(torch.where(x == tokenizer.eos_id(),1,0), axis=-1).unsqueeze(axis=1)\n",
    "r_indi = torch.arange(0,x.shape[0]).unsqueeze(axis=-1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97190fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  317, 1945,    2,    2,    2,    2,    2,    2],\n",
       "        [   1,  432,  401,  285,  261,  447,  502, 3820,    2],\n",
       "        [   1, 1044,  323,  749, 1450,    2,    2,    2,    2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47665a",
   "metadata": {},
   "source": [
    "### Batch Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d3b1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "logits = pred[r_indi, c_indi, :]\n",
    "logits = torch.softmax(logits, axis=-1)\n",
    "next_idxs = torch.multinomial(logits.squeeze(axis=1), num_samples=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d353af98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f27d06bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 9, 4096])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model(x)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5f29be4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4096])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[r_indi, c_indi, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ea9c145",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "prob_dist must be 1 or 2 dim",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m next_idx \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m        \n\u001b[0;32m     13\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([idx, next_idx], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: prob_dist must be 1 or 2 dim"
     ]
    }
   ],
   "source": [
    "temperature = 0.1\n",
    "pred = model(x)\n",
    "logits = pred[r_indi, c_indi, :]\n",
    "\n",
    "# pluck the logits at the final step and scale by desired temperature\n",
    "logits = logits / temperature\n",
    "\n",
    "#logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)\n",
    "\n",
    "\n",
    "logits = torch.softmax(logits, axis=-1)\n",
    "next_idx = torch.multinomial(logits, num_samples=1)        \n",
    "idx = torch.cat([idx, next_idx], axis=1)\n",
    "t += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4e239e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)\n",
    "logits = pred[r_indi, c_indi, :]\n",
    "logits = torch.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb8f3178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1608f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_idxs = torch.multinomial(logits.squeeze(axis=1), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1649781c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x[r_indi, c_indi] = next_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66a2f6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1,  317, 1945,  281,    2,    2,    2,    2,    2],\n",
       "        [   1,  432,  401,  285,  261,  447,  502, 3820, 3960],\n",
       "        [   1, 1044,  323,  749, 1450, 3952,    2,    2,    2]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_indi += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "70452057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]], device='cuda:0')"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_id_index = torch.zeros(size=(x.shape[0],1)).to(\"cuda\")\n",
    "eos_id_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "785232ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_id_index[next_idxs == tokenizer.eos_id()] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9071c3",
   "metadata": {},
   "source": [
    "## Dataset for Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a89097c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec00353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_model_path = os.path.join(\"saved_artifacts/tokenizers\",\"tok_4096.model\")\n",
    "tokenizer = spm.SentencePieceProcessor(model_file=tokenizer_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6f99bc1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd9d7995fa5461095c3be9f1d663017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/21990 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def process_rows_func(examples, tokenizer):   \n",
    "    from random import randint    \n",
    "    idx = 0 \n",
    "    p_len = 30\n",
    "    inputs, targets = [],[]\n",
    "    \n",
    "    \n",
    "    tokenized_examples = tokenizer.encode(examples[\"text\"], add_bos=True)\n",
    "    for example in tokenized_examples:        \n",
    "        inputs.append(example[:p_len])\n",
    "        targets.append(example)\n",
    "        \n",
    "    return {\"inputs\":inputs}\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "                    process_rows_func,\n",
    "                    fn_kwargs={\"tokenizer\":tokenizer},\n",
    "                    batched=True,\n",
    "                    num_proc=4,\n",
    "                    remove_columns=val_dataset.column_names    \n",
    "                )\n",
    "val_dataset.set_format(type='torch', columns=val_dataset.column_names)             \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c6a6c3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e2815422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 30])\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f317dcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 32 \n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = torch.cuda.device_count()\n",
    "        self.grad_accumulation_steps = int(self.global_batch_size/(self.seq_len * self.batch_size * self.world_size))\n",
    "        self.grad_clip = 1.0\n",
    "        self.learning_rate=1e-3\n",
    "        self.min_lr = self.learning_rate / 10\n",
    "        self.warmup_steps = 1000\n",
    "        self.total_params = 0\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"        \n",
    "        self.rank = 0 \n",
    "        \n",
    "config = Config()\n",
    "\n",
    "model = Model(config)\n",
    "model = model.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8784017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.3763, -0.0708, -0.0694,  ...,  0.4578, -0.8039, -0.3586],\n",
      "         [-0.1570, -1.1218,  0.0217,  ..., -0.2329, -0.6073,  0.7035],\n",
      "         ...,\n",
      "         [ 0.9770,  0.6675,  0.3975,  ..., -0.5922, -1.4464, -0.0071],\n",
      "         [ 0.0185,  0.5078,  0.4781,  ...,  0.0769, -0.2572,  0.4616],\n",
      "         [-0.4522,  0.7630,  0.6867,  ...,  0.1911,  0.0363, -0.2500]],\n",
      "\n",
      "        [[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.7603, -0.1441, -0.4651,  ...,  0.6543, -0.1545,  0.4792],\n",
      "         [ 0.1261, -0.0431, -0.3837,  ...,  0.2846,  0.3171, -0.0431],\n",
      "         ...,\n",
      "         [ 0.6645, -0.2119, -0.3644,  ...,  0.2470, -0.0699, -0.2320],\n",
      "         [-0.4947,  0.0696, -0.3579,  ...,  0.3642, -0.1448,  0.4825],\n",
      "         [ 0.3476, -0.2430,  0.8257,  ...,  0.0297,  0.3578, -1.4128]],\n",
      "\n",
      "        [[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.3763, -0.0708, -0.0694,  ...,  0.4578, -0.8039, -0.3586],\n",
      "         [-0.1570, -1.1218,  0.0217,  ..., -0.2329, -0.6073,  0.7035],\n",
      "         ...,\n",
      "         [-0.9382,  0.0410, -0.6118,  ...,  0.0528, -0.3660,  0.6529],\n",
      "         [-1.0608,  1.0386, -0.2930,  ...,  0.1452, -0.2975,  0.4646],\n",
      "         [-0.0230,  0.0724, -0.9661,  ..., -0.9350, -0.0559,  0.2500]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.3264,  0.4131,  0.0641,  ..., -0.1749, -0.1936,  0.4120],\n",
      "         [ 0.2139, -0.0829, -0.6031,  ...,  0.1054,  0.2795, -0.1387],\n",
      "         ...,\n",
      "         [ 0.9763,  0.5580,  0.5196,  ..., -0.4177, -1.2121, -0.2724],\n",
      "         [ 0.1347,  0.5424,  0.6012,  ...,  0.1168,  0.1111,  0.1962],\n",
      "         [ 0.4043, -0.1634, -0.9290,  ..., -0.9319, -0.6528, -0.3692]],\n",
      "\n",
      "        [[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.3763, -0.0708, -0.0694,  ...,  0.4578, -0.8039, -0.3586],\n",
      "         [-0.1570, -1.1218,  0.0217,  ..., -0.2329, -0.6073,  0.7035],\n",
      "         ...,\n",
      "         [-0.1106,  0.7375,  0.1305,  ...,  0.5531, -0.1172,  0.8854],\n",
      "         [-0.5632, -0.1009, -0.3688,  ...,  0.2251, -0.4516,  0.8224],\n",
      "         [ 1.1959,  0.3819, -0.1046,  ...,  0.7053,  0.3125,  0.6234]],\n",
      "\n",
      "        [[-0.1738,  1.0943, -0.2407,  ..., -0.2396, -0.7592, -0.0543],\n",
      "         [ 0.3763, -0.0708, -0.0694,  ...,  0.4578, -0.8039, -0.3586],\n",
      "         [-0.1570, -1.1218,  0.0217,  ..., -0.2329, -0.6073,  0.7035],\n",
      "         ...,\n",
      "         [-0.5086,  0.0853, -0.3164,  ...,  0.2206, -0.3720,  0.9484],\n",
      "         [ 0.2826, -0.2575,  0.9556,  ..., -0.0087,  0.1301, -0.9404],\n",
      "         [-0.0409,  1.2173, -0.8030,  ..., -0.1789,  0.9992, -0.4719]]],\n",
      "       device='cuda:0', grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for data in val_loader:\n",
    "    x = data['inputs'].to(0)\n",
    "    print(model(x))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07305767",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "\n",
    "\n",
    "class TinyStories:\n",
    "    def __init__(self, config):\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.context_length = config.seq_len\n",
    "        self.batch_size = config.batch_size\n",
    "        self.tokenizer_model_path = os.path.join(config.tokenizer_path,f\"tok_{config.vocab_size}.model\")\n",
    "        self.tokenizer = spm.SentencePieceProcessor(model_file=self.tokenizer_model_path)\n",
    "\n",
    "        #build dataset\n",
    "        self.dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "        self.train_dataset = self.dataset[\"train\"]\n",
    "        self.val_dataset = self.dataset[\"validation\"]\n",
    "\n",
    "        if config.n_val_examples != -1:\n",
    "            self.val_dataset = self.val_dataset.select(range(config.n_val_examples))\n",
    "\n",
    "        self.val_dataset = self.val_dataset.map(\n",
    "                    self.process_rows_func,                \n",
    "                    batched=True,\n",
    "                    num_proc=4,\n",
    "                    remove_columns=self.val_dataset.column_names    \n",
    "                )\n",
    "        self.val_dataset.set_format(type='torch', columns=self.val_dataset.column_names)             \n",
    "\n",
    "    \n",
    "    def process_rows_func(self,examples):     \n",
    "        idx = 0 \n",
    "        inputs, targets = [],[]\n",
    "        \n",
    "        \n",
    "        tokenized_examples = self.tokenizer.encode(examples[\"text\"], add_bos=True)\n",
    "        merged_text = [token for example in tokenized_examples for token in example]\n",
    "        \n",
    "        while idx < len(merged_text)-self.context_length:\n",
    "            chunk = merged_text[idx:idx+self.context_length+1]\n",
    "            inputs.append(chunk[:-1])\n",
    "            targets.append(chunk[1:])\n",
    "            idx += self.context_length\n",
    "        return {\"inputs\":inputs, \"targets\":targets}\n",
    "\n",
    "\n",
    "\n",
    "    def getTrainDataLoader(self, ddp):\n",
    "        sampler = DistributedSampler(self.train_dataset) if ddp else None\n",
    "        shuffle = True if not ddp else None\n",
    "        train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=shuffle, sampler=sampler)\n",
    "        return train_loader  \n",
    "    \n",
    "    \n",
    "\n",
    "    def getValDataLoader(self): \n",
    "        val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=True)    \n",
    "        return val_loader\n",
    "        \n",
    "    def getVocabSize(self):\n",
    "        return int(self.tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "beb07f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(size=(32,10))\n",
    "x = x.to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3958179c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9430, -0.5700, -0.1078,  0.7640,  0.7182, -1.1693, -0.1812, -0.3964,\n",
       "         -0.0596,  0.9160],\n",
       "        [-0.4061,  0.2077,  1.0531,  0.2025, -0.6469,  0.1366,  0.5040, -0.2615,\n",
       "          0.5389, -0.4528],\n",
       "        [ 1.3481, -1.2552, -1.0317,  1.0318,  0.6127, -1.7430, -1.1002, -3.1245,\n",
       "         -0.1625,  0.3771],\n",
       "        [ 0.9627,  1.3246, -0.8523, -0.3934, -3.2231, -1.4649, -0.1972,  0.8223,\n",
       "         -0.2575,  1.5554],\n",
       "        [-0.3162,  0.6525,  0.6975, -0.1848,  2.0393, -0.4173, -0.9319, -0.5348,\n",
       "          0.8592,  0.3856],\n",
       "        [ 0.4587, -1.2753,  0.9256, -0.6391, -2.0951, -0.2169, -0.9822, -0.4987,\n",
       "          0.7941, -0.7787],\n",
       "        [-0.5549,  0.7321,  0.5068,  0.4650, -2.2132, -2.3530,  0.2150,  1.0672,\n",
       "         -0.9140,  0.6902],\n",
       "        [-1.1208, -1.6164,  0.3974,  0.7971, -0.8254,  1.9163, -0.2140, -1.3782,\n",
       "          1.5481, -0.2181],\n",
       "        [-0.2725, -0.1696,  0.7520,  0.2743,  0.3651,  0.1679, -0.2391, -0.3577,\n",
       "          0.1537,  0.5679],\n",
       "        [ 0.5426,  1.0723, -1.7073,  1.2271,  0.6948,  0.5217,  0.4324, -0.0883,\n",
       "          0.2463, -1.3059],\n",
       "        [ 0.1511, -0.4268, -0.7214, -2.5065, -0.0037, -1.3799, -0.8354, -1.6838,\n",
       "          0.7967, -0.3534],\n",
       "        [-0.7404, -1.9885, -0.8198, -1.1847, -0.1431, -1.2735, -0.2084, -1.8994,\n",
       "          0.7461,  1.4042],\n",
       "        [-0.6256,  0.3552,  1.6393,  0.4431,  1.0639, -0.0158,  0.1809,  1.5464,\n",
       "          0.9218, -0.0641],\n",
       "        [ 1.3759,  0.6047, -0.0175, -0.2665,  2.3680, -1.6195, -0.3162,  0.3377,\n",
       "         -1.1983, -0.9054],\n",
       "        [ 0.5637, -0.0261,  0.1121,  0.4037,  0.5356,  0.7553, -2.0029, -0.8867,\n",
       "          0.7804,  0.2620],\n",
       "        [-1.0101,  2.0462,  1.3974,  1.1596, -0.2607,  0.8451,  1.0300,  1.5365,\n",
       "          0.1327,  0.6488],\n",
       "        [-0.4423, -0.4084,  0.4642,  0.1745,  0.0096, -1.1608, -1.3527,  0.9682,\n",
       "         -1.9746,  0.0177],\n",
       "        [-0.3445,  0.6310, -1.7961,  0.8309, -0.0175,  0.1438, -0.8682,  1.1337,\n",
       "         -1.3851,  0.4186],\n",
       "        [ 1.2258,  0.7651,  0.8324,  0.4624,  1.1376,  1.6961,  1.8018, -1.5681,\n",
       "         -0.7195,  0.6728],\n",
       "        [ 1.3383,  0.3093, -2.0562,  0.6325, -2.5553, -0.9167,  0.3516, -1.1748,\n",
       "          1.2218, -0.3756],\n",
       "        [-0.2680,  0.6721, -0.3627,  0.2091,  0.9934,  0.7471,  0.8978,  1.4594,\n",
       "         -0.3797, -0.0679],\n",
       "        [ 0.3176,  0.9658, -1.5466,  1.9252, -0.7686,  0.0089,  0.7560,  0.2042,\n",
       "          0.3900,  1.0561],\n",
       "        [ 0.4140,  0.9653,  0.1524,  0.7477, -0.3971,  0.3297,  1.0986, -0.8336,\n",
       "          0.1450,  0.3036],\n",
       "        [ 0.1695, -0.9573,  1.4629, -1.7683,  1.6254, -1.2772,  0.3855,  1.1796,\n",
       "         -1.0762, -1.9672],\n",
       "        [-1.5870, -1.2645,  0.1388,  2.1669, -0.8868, -0.5476, -2.2781,  0.3606,\n",
       "          0.7581, -1.1077],\n",
       "        [ 0.7850,  1.6328,  0.1743,  0.8249, -0.3565,  0.3257, -1.8311, -0.0722,\n",
       "          0.4866, -1.1540],\n",
       "        [-0.8374,  0.2695,  0.2494,  0.7182, -0.1205,  0.1179,  0.5294,  1.5974,\n",
       "         -0.2461, -1.1149],\n",
       "        [-0.4091, -0.7041,  1.0027, -0.8623,  0.1199, -0.8945,  0.0916,  0.4744,\n",
       "          0.5784,  0.2846],\n",
       "        [-0.7137, -1.3695,  0.3255, -1.5981,  0.2430, -2.2111, -1.1168,  1.1056,\n",
       "          1.0981, -0.3724],\n",
       "        [ 0.9050,  1.0590,  0.8191,  0.3884, -0.6646, -1.1318,  2.0015,  0.6828,\n",
       "          0.2953, -0.6010],\n",
       "        [ 0.3544,  0.4765,  0.0795, -1.1391,  0.7020,  0.0205, -0.7668, -0.5262,\n",
       "          1.0802, -0.3058],\n",
       "        [ 0.1338, -0.6990,  1.5409,  0.8160,  1.0410,  0.3007, -0.2509, -0.8781,\n",
       "          1.9315,  1.4436]], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c06eb257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1346d61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.eos_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "66b661ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9eeeff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1104\n"
     ]
    }
   ],
   "source": [
    "later = datetime.now()\n",
    "difference = int((now - later).total_seconds())\n",
    "print(int(20000000 /difference))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdaf1fd",
   "metadata": {},
   "source": [
    "# KV Cache Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "160b6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 16\n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = 1\n",
    "        self.total_params = 0\n",
    "        self.saved_checkpoint_path = \"saved_artifacts/models/model240221\"\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"\n",
    "        self.n_test_examples = 10000        \n",
    "        self.steps_to_serialize = 6\n",
    "        self.rank = 0\n",
    "        self.enable_kv_cache = True\n",
    "        \n",
    "config = Config()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37804029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import Model, AttentionLayer\n",
    "import os\n",
    "from torch import nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e7935f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (embedding_layer): Embedding(4096, 768)\n",
       "  (layers): Sequential(\n",
       "    (0): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (1): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (2): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (3): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (4): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (5): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (6): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "    (7): AttentionLayer(\n",
       "      (mha): MultiHeadAttention(\n",
       "        (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (rope): RoPE()\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "      )\n",
       "      (ffn): FeedForwordNetwork(\n",
       "        (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "        (silu): SiLU()\n",
       "      )\n",
       "      (anorm): RMSNorm()\n",
       "      (fnorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (hnorm): RMSNorm()\n",
       "  (clf_head): Linear(in_features=768, out_features=4096, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81d90ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0, high= 500, size=(1,25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd838843",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (25) must match the size of tensor b (768) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m al \u001b[38;5;241m=\u001b[39m AttentionLayer(config,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\A_DATA\\E\\AI\\usecases\\TinyStories\\model.py:206\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, x, enable_kv_cache, kv_cache)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x, enable_kv_cache, kv_cache):\n\u001b[1;32m--> 206\u001b[0m     x, kv_cache \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmha(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m, enable_kv_cache, kv_cache)\n\u001b[0;32m    207\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfnorm(x))\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x, enable_kv_cache, kv_cache\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\A_DATA\\E\\AI\\usecases\\TinyStories\\model.py:185\u001b[0m, in \u001b[0;36mRMSNorm.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    184\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_norm(x\u001b[38;5;241m.\u001b[39mfloat())\u001b[38;5;241m.\u001b[39mtype_as(x)\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moutput\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (25) must match the size of tensor b (768) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "al = AttentionLayer(config,1)\n",
    "al(x,True,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2438c135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 25])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3daacf7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x,kv_cache \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mC:\\A_DATA\\E\\AI\\usecases\\TinyStories\\model.py:225\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, y, kv_cache)\u001b[0m\n\u001b[0;32m    222\u001b[0m     kv_cache \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mn_layers)]\n\u001b[0;32m    224\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_layer(x)\n\u001b[1;32m--> 225\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menable_kv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkv_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhnorm(x)\n\u001b[0;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclf_head(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "\n",
    "x,kv_cache = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaee090a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f898d324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977d0be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f31a36bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0001, -0.0009,  0.0131,  ...,  0.0310, -0.0227, -0.0289],\n",
       "        [ 0.0129,  0.0352, -0.0253,  ..., -0.0252,  0.0011, -0.0096],\n",
       "        [-0.0016, -0.0095,  0.0183,  ..., -0.0121,  0.0074, -0.0008],\n",
       "        ...,\n",
       "        [-0.0179, -0.0028,  0.0111,  ..., -0.0097,  0.0229,  0.0282],\n",
       "        [-0.0304,  0.0022,  0.0034,  ..., -0.0250, -0.0042, -0.0303],\n",
       "        [ 0.0030, -0.0063,  0.0224,  ...,  0.0111,  0.0294,  0.0276]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].mha.query.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b5d01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(os.path.join(\"saved_artifacts/models/model240221\",\"checkpoint.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "237e7d93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('embedding_layer.weight',\n",
       "              tensor([[ 3.1257e-01, -6.4405e-04,  7.3055e-02,  ...,  3.5967e-01,\n",
       "                       -2.3772e-01,  1.9041e-01],\n",
       "                      [ 5.0298e-01, -1.8811e-01,  3.4336e-01,  ...,  4.2124e-02,\n",
       "                       -8.6812e-01, -5.6497e-01],\n",
       "                      [ 4.3298e-01,  8.2220e-01, -2.3425e-01,  ...,  4.7401e-02,\n",
       "                        3.2848e-02,  4.5675e-01],\n",
       "                      ...,\n",
       "                      [ 2.2998e-01, -3.3743e-01,  3.2875e-01,  ..., -1.4212e-01,\n",
       "                       -4.0337e-03,  7.0427e-02],\n",
       "                      [-4.5742e-01,  2.6336e-01, -5.7023e-02,  ..., -5.1284e-01,\n",
       "                       -5.9907e-02, -5.1216e-01],\n",
       "                      [-5.2214e-01, -1.5507e-01,  6.5681e-02,  ...,  2.5634e-01,\n",
       "                       -2.5154e-01, -3.2544e-01]], device='cuda:0')),\n",
       "             ('layers.0.mha.query.weight',\n",
       "              tensor([[-0.0549,  0.0011,  0.0336,  ..., -0.0550,  0.0424, -0.0435],\n",
       "                      [ 0.0011,  0.0446, -0.0323,  ...,  0.0947,  0.0315, -0.0504],\n",
       "                      [ 0.0682, -0.0285,  0.0473,  ...,  0.0287, -0.0028,  0.0275],\n",
       "                      ...,\n",
       "                      [-0.1056, -0.0556,  0.0495,  ...,  0.0237, -0.0599, -0.0140],\n",
       "                      [ 0.0028,  0.0481,  0.0005,  ..., -0.0011,  0.0257,  0.0798],\n",
       "                      [ 0.0103,  0.0329,  0.0139,  ..., -0.0539,  0.0451, -0.0475]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.mha.key.weight',\n",
       "              tensor([[ 0.1047,  0.0150, -0.0617,  ..., -0.0694, -0.0494, -0.0085],\n",
       "                      [ 0.0043,  0.0423,  0.0004,  ...,  0.0988, -0.0344,  0.0594],\n",
       "                      [ 0.0144, -0.0029, -0.0082,  ..., -0.0539,  0.0064, -0.0595],\n",
       "                      ...,\n",
       "                      [-0.0762, -0.0905,  0.0576,  ..., -0.0107,  0.0127,  0.0225],\n",
       "                      [-0.0348, -0.1296,  0.0185,  ...,  0.0122,  0.0026, -0.0535],\n",
       "                      [-0.0589, -0.0489, -0.0717,  ...,  0.0687,  0.0326,  0.0017]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.mha.value.weight',\n",
       "              tensor([[-0.0096, -0.0083,  0.0383,  ...,  0.0244, -0.0048,  0.0129],\n",
       "                      [-0.0488, -0.0060,  0.0533,  ...,  0.0229, -0.0195, -0.0213],\n",
       "                      [ 0.0372, -0.0328, -0.0178,  ..., -0.0062, -0.0236,  0.0116],\n",
       "                      ...,\n",
       "                      [ 0.0236, -0.0296,  0.0118,  ..., -0.0681, -0.0334,  0.0563],\n",
       "                      [-0.0375, -0.0168, -0.0275,  ...,  0.0716, -0.0058,  0.0421],\n",
       "                      [-0.0231, -0.0011, -0.0703,  ..., -0.0317,  0.0287,  0.0119]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.mha.proj.weight',\n",
       "              tensor([[ 0.0105, -0.0351,  0.0341,  ..., -0.0633,  0.0050, -0.0341],\n",
       "                      [ 0.0105, -0.0200, -0.0302,  ...,  0.0329, -0.0017, -0.0154],\n",
       "                      [-0.0172, -0.0073,  0.0127,  ...,  0.0327, -0.0014,  0.0348],\n",
       "                      ...,\n",
       "                      [-0.0328,  0.0201, -0.0050,  ...,  0.0051, -0.0137, -0.0099],\n",
       "                      [ 0.0133, -0.0228, -0.0030,  ...,  0.0188, -0.0045,  0.0511],\n",
       "                      [ 0.0071, -0.0206,  0.0079,  ...,  0.0280, -0.0239, -0.0081]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.ffn.w.weight',\n",
       "              tensor([[ 9.2766e-04, -1.2031e-03,  6.2762e-03,  ...,  1.9263e-02,\n",
       "                        3.7041e-02,  7.8835e-05],\n",
       "                      [-4.7647e-02, -4.3014e-02,  2.1597e-02,  ..., -6.0067e-02,\n",
       "                       -6.1820e-02,  2.9184e-02],\n",
       "                      [ 7.3381e-03, -6.9278e-02,  5.8770e-02,  ...,  5.1525e-02,\n",
       "                       -8.9932e-02, -1.9854e-02],\n",
       "                      ...,\n",
       "                      [ 1.1203e-02,  3.5142e-02, -3.2890e-02,  ...,  1.4145e-02,\n",
       "                       -1.5805e-02,  9.0907e-03],\n",
       "                      [-4.3460e-02,  4.0299e-02, -1.6631e-02,  ...,  3.3477e-02,\n",
       "                        4.2117e-02,  1.7314e-02],\n",
       "                      [ 8.6064e-02, -2.1152e-02,  3.8979e-02,  ...,  2.6432e-02,\n",
       "                       -8.5979e-02, -2.7556e-02]], device='cuda:0')),\n",
       "             ('layers.0.ffn.w.bias',\n",
       "              tensor([-0.0083, -0.0178, -0.0378,  ..., -0.0303, -0.0422, -0.0259],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.ffn.v.weight',\n",
       "              tensor([[-0.0145, -0.0111,  0.0259,  ...,  0.0226, -0.0443, -0.0865],\n",
       "                      [-0.0439,  0.0413, -0.0105,  ..., -0.0199,  0.0295, -0.0141],\n",
       "                      [ 0.0778,  0.0427,  0.0575,  ...,  0.0385, -0.0392, -0.0479],\n",
       "                      ...,\n",
       "                      [ 0.0613,  0.0415,  0.0110,  ..., -0.0087, -0.0040,  0.0456],\n",
       "                      [-0.0951,  0.0349,  0.0119,  ...,  0.0068, -0.0106,  0.0528],\n",
       "                      [-0.0651, -0.0248,  0.0595,  ...,  0.0457,  0.0171,  0.0463]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.ffn.v.bias',\n",
       "              tensor([ 0.0055, -0.0120, -0.0166,  ..., -0.0067, -0.0229, -0.0117],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.ffn.w2.weight',\n",
       "              tensor([[ 0.0003, -0.0311,  0.0407,  ..., -0.0168,  0.0668,  0.0096],\n",
       "                      [ 0.0008, -0.1125, -0.0002,  ..., -0.0249, -0.0083, -0.0206],\n",
       "                      [ 0.0021, -0.0327, -0.0279,  ..., -0.0624, -0.0595,  0.0739],\n",
       "                      ...,\n",
       "                      [-0.0688, -0.0073,  0.0400,  ..., -0.0435, -0.0319, -0.0194],\n",
       "                      [ 0.0532, -0.0212, -0.0660,  ..., -0.0285, -0.0719,  0.0890],\n",
       "                      [-0.0238, -0.0578,  0.0273,  ...,  0.0690,  0.0373, -0.0208]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.0.ffn.w2.bias',\n",
       "              tensor([-1.6564e-03, -6.0818e-03,  7.4043e-03,  2.7625e-03,  2.8631e-02,\n",
       "                       4.4127e-02, -1.7301e-02,  1.3034e-02, -1.4215e-02,  7.9530e-03,\n",
       "                       1.3247e-02,  2.2052e-03, -7.7880e-03,  7.1174e-03, -2.7649e-02,\n",
       "                      -4.5521e-02, -3.7098e-02,  6.0030e-03, -4.8566e-02, -1.9898e-02,\n",
       "                      -2.7120e-02, -2.5685e-02, -4.0824e-02,  3.3326e-02,  1.5556e-02,\n",
       "                      -1.6724e-02, -4.2772e-02,  7.4154e-03, -3.8361e-03,  1.5309e-02,\n",
       "                       1.5012e-02, -2.6340e-03, -1.5133e-02, -2.7597e-02,  5.6137e-02,\n",
       "                       3.0834e-02,  4.5605e-02,  1.6883e-02,  3.5768e-02,  3.4188e-04,\n",
       "                      -1.8599e-02,  4.1657e-02,  3.0144e-02, -3.8825e-03, -4.0264e-02,\n",
       "                       4.4400e-03,  5.3297e-02, -1.9223e-02, -9.3277e-03, -5.1730e-02,\n",
       "                       2.0232e-02, -5.5428e-03, -3.8755e-02, -8.4307e-03, -4.8335e-03,\n",
       "                      -1.8621e-02,  2.1594e-02,  3.3280e-03,  5.2962e-02,  4.9677e-03,\n",
       "                      -1.5948e-02, -3.0180e-02, -2.1074e-02,  1.0288e-02,  2.8898e-02,\n",
       "                       3.9724e-02, -1.2340e-02,  3.8653e-02, -7.0609e-02,  1.1097e-02,\n",
       "                      -3.3352e-02, -5.1412e-02, -2.6619e-02, -2.4485e-02, -2.9579e-02,\n",
       "                       5.1923e-03, -4.2240e-02,  1.3356e-02, -1.8096e-02,  1.9339e-03,\n",
       "                       1.9964e-02,  2.4469e-02, -1.0734e-01,  2.7837e-02,  5.1049e-02,\n",
       "                      -2.3598e-02,  4.0939e-02, -8.2929e-03, -3.0941e-02,  3.6848e-02,\n",
       "                      -3.2891e-02,  2.8677e-03,  6.8053e-03,  9.5105e-03,  4.7665e-03,\n",
       "                      -2.5252e-03, -3.9823e-02, -2.7299e-02, -1.6949e-02,  1.0184e-02,\n",
       "                      -2.7937e-03,  4.7843e-02, -2.6179e-02,  2.4243e-02,  1.4308e-02,\n",
       "                      -5.7809e-03, -1.3342e-02, -3.2140e-03, -1.6853e-02, -5.5515e-03,\n",
       "                       3.8905e-02, -7.5080e-03,  2.5689e-02, -3.6847e-03,  4.7020e-03,\n",
       "                      -2.3889e-03,  5.2397e-02,  5.0876e-02,  1.4939e-02, -5.0152e-02,\n",
       "                       3.9416e-02,  1.2240e-02,  4.0298e-02, -1.9256e-02, -2.6502e-02,\n",
       "                      -2.4089e-02, -3.0079e-02,  1.7225e-02, -4.6278e-02,  1.4095e-02,\n",
       "                      -3.3238e-02, -3.0730e-02,  5.6474e-02, -2.0802e-03, -2.4380e-02,\n",
       "                      -1.6207e-02,  1.8464e-02,  4.3909e-02, -4.1485e-02, -3.1076e-02,\n",
       "                       3.1648e-02, -4.0621e-03, -3.6562e-02,  1.5278e-02,  4.2561e-03,\n",
       "                      -7.4163e-03,  9.1298e-03,  1.8497e-02,  2.5173e-02, -2.6895e-02,\n",
       "                       3.6093e-02, -3.5772e-02,  1.5936e-02,  1.2817e-02, -5.5970e-02,\n",
       "                      -5.4629e-02, -5.5532e-03, -2.6457e-02, -2.5378e-02,  2.2318e-02,\n",
       "                      -4.2990e-02, -2.5720e-02, -2.7710e-03,  1.6348e-03,  2.2509e-02,\n",
       "                       2.9434e-02, -5.2733e-02, -7.2816e-03, -3.3039e-02, -5.1072e-03,\n",
       "                      -7.7483e-02, -2.1625e-02,  6.9200e-03,  6.4126e-02,  5.0558e-02,\n",
       "                      -1.5806e-02, -1.6636e-02, -2.3148e-02, -2.4357e-02, -4.7324e-03,\n",
       "                      -8.2843e-03, -2.1934e-02, -4.5759e-02, -1.4286e-02, -1.5431e-02,\n",
       "                       2.9325e-02,  1.4232e-02,  9.4136e-03,  1.6801e-03,  3.5628e-02,\n",
       "                       8.8738e-03, -1.9512e-02, -4.2929e-02,  3.3729e-02, -2.4442e-02,\n",
       "                      -2.1822e-02, -4.4049e-02, -1.6560e-02, -1.0121e-03,  7.9792e-03,\n",
       "                       3.8388e-05, -1.5589e-02, -1.1735e-02,  2.0113e-02, -1.7254e-02,\n",
       "                       5.8606e-03,  4.9695e-03, -5.0119e-02, -3.5622e-02, -7.0434e-02,\n",
       "                       1.7890e-02, -5.8722e-02, -2.4411e-03, -3.9260e-02, -7.8605e-03,\n",
       "                       1.8152e-02,  4.3064e-02, -2.6421e-02,  2.7754e-02, -1.5184e-02,\n",
       "                      -3.5690e-02,  1.2305e-02,  1.1977e-03,  3.7581e-02, -1.4046e-02,\n",
       "                      -9.6197e-03, -4.3945e-01,  1.7428e-02,  1.6404e-02, -3.0290e-02,\n",
       "                       3.3586e-02, -2.5523e-02,  3.4229e-02, -8.1574e-03, -2.0300e-02,\n",
       "                       2.6481e-03, -4.1844e-02, -1.1966e-02, -1.0654e-02,  1.0884e-02,\n",
       "                       5.3699e-03, -1.6158e-02,  4.4723e-03, -2.5638e-02,  3.3934e-02,\n",
       "                      -4.2850e-03, -2.5750e-02,  9.5203e-03, -9.0123e-03, -1.3658e-02,\n",
       "                       6.3307e-02,  1.1310e-02, -6.0946e-03, -5.0027e-02, -4.1378e-02,\n",
       "                       7.0648e-02, -2.4641e-02, -2.0620e-03, -4.0213e-02,  3.3018e-04,\n",
       "                      -5.0519e-03, -8.3891e-03,  5.6589e-02,  1.5056e-02, -1.3559e-02,\n",
       "                       3.0498e-02, -1.9743e-02,  3.2250e-02,  1.0614e-02, -3.0902e-02,\n",
       "                      -2.9275e-02, -3.1014e-03, -1.8212e-02, -7.6414e-03, -2.1406e-02,\n",
       "                       1.7383e-03, -2.6704e-03, -8.2964e-04, -3.4452e-02, -1.7413e-04,\n",
       "                       1.4898e-02, -1.7648e-02,  1.7299e-02, -4.2657e-02, -1.7323e-02,\n",
       "                      -7.5014e-03,  4.6825e-03,  9.2039e-03,  1.7722e-02, -2.9593e-02,\n",
       "                       1.3416e-02, -3.2992e-02, -2.1437e-02,  9.7617e-03,  3.4256e-02,\n",
       "                       1.4779e-02,  1.2940e-02,  4.4559e-02, -1.3990e-02,  2.2409e-02,\n",
       "                       3.6075e-02,  7.5093e-02,  2.4775e-02, -4.9329e-02, -2.6497e-02,\n",
       "                      -9.8070e-03, -5.4295e-02,  2.1989e-02,  6.6878e-03, -4.1519e-03,\n",
       "                       5.9209e-02,  3.3180e-02, -2.2159e-02, -3.1838e-02, -1.7193e-02,\n",
       "                      -1.5351e-02,  1.6081e-02, -9.5175e-03, -2.1966e-02,  1.2957e-02,\n",
       "                       5.7947e-02,  4.3510e-02,  4.7035e-02,  2.2011e-02,  5.3779e-03,\n",
       "                      -1.6454e-02,  7.6756e-03,  6.6513e-03, -3.7965e-02, -3.7443e-03,\n",
       "                       2.5285e-02,  1.9167e-02, -4.4209e-03,  8.8581e-03, -1.9381e-02,\n",
       "                       3.4958e-02, -1.4064e-02,  5.6552e-02, -2.6183e-03,  1.5036e-02,\n",
       "                       2.3707e-02, -7.4742e-03, -1.8452e-02,  7.8953e-04,  3.7094e-03,\n",
       "                       8.2622e-03, -5.7891e-02, -4.9068e-02,  7.8242e-03,  1.0289e-02,\n",
       "                      -3.6655e-04,  1.7689e-02,  1.7654e-02, -4.8448e-03,  6.0253e-02,\n",
       "                       2.8226e-02, -1.5607e-02, -2.1771e-02, -9.6135e-03, -4.0802e-02,\n",
       "                      -2.6239e-02,  2.9847e-02,  3.4030e-02,  2.5549e-02, -2.4004e-02,\n",
       "                      -1.1699e-02,  1.2032e-02, -8.9696e-03, -3.5926e-02, -1.4246e-03,\n",
       "                      -4.2605e-02,  1.1579e-04,  1.3190e-02, -2.7612e-02,  4.6011e-03,\n",
       "                       2.4302e-02, -2.3852e-02,  1.9924e-02,  8.6055e-03, -1.8790e-02,\n",
       "                      -4.5022e-02, -4.3056e-03, -2.9575e-03,  3.3510e-02,  5.3006e-02,\n",
       "                       1.6615e-02, -2.5262e-02, -3.1789e-02, -2.5630e-02,  5.7075e-02,\n",
       "                      -4.8875e-02,  2.8440e-02, -2.5352e-02,  2.4802e-03,  1.4923e-02,\n",
       "                       7.0345e-04,  1.1893e-02,  2.3517e-02, -2.4309e-04,  4.8992e-02,\n",
       "                      -3.1201e-02,  2.8399e-02,  3.2228e-03,  1.8118e-02, -1.8078e-02,\n",
       "                       6.7932e-03,  2.3527e-02,  1.6790e-02, -1.2613e+00, -1.4608e-02,\n",
       "                       2.1197e-02,  2.2535e-02,  9.7011e-03,  2.1110e-02,  1.3745e-02,\n",
       "                      -3.5330e-02, -3.5296e-03, -5.4679e-02, -4.4506e-02, -6.4524e-03,\n",
       "                       2.5792e-02,  5.2256e-02,  1.5744e-02, -3.5521e-02,  3.4621e-02,\n",
       "                      -2.7038e-02, -2.8074e-02, -3.5523e-03, -9.4099e-03, -3.8361e-02,\n",
       "                      -9.9118e-04,  4.5216e-02,  3.6104e-02, -2.8880e-02,  7.5368e-02,\n",
       "                      -3.2221e-02, -1.2754e-02,  3.8662e-02,  1.4363e-02, -4.1080e-02,\n",
       "                       3.4424e-02, -1.4880e-02,  3.1889e-02, -2.2922e-02, -2.4829e-02,\n",
       "                      -3.6355e-02,  8.2561e-04,  2.3518e-02,  8.9092e-04,  3.9569e-02,\n",
       "                      -2.1002e-02, -1.7510e-02,  3.7535e-02,  4.4745e-02, -2.2377e-02,\n",
       "                       1.8410e-03, -5.5602e-03, -3.5937e-02,  3.2385e-02, -1.4768e-02,\n",
       "                       2.7682e-02,  2.6467e-02, -1.1557e-02, -1.1269e-02,  1.0426e-02,\n",
       "                       1.5409e-02,  6.4490e-02,  4.3151e-02, -1.3999e-02,  9.0040e-03,\n",
       "                       6.1348e-03, -1.5918e-03, -7.8724e-03,  2.1688e-03, -1.2192e-02,\n",
       "                       3.0197e-02, -1.2124e-02,  1.3044e-02, -2.6252e-03,  3.8619e-03,\n",
       "                      -2.1748e-03,  2.4854e-02, -1.4414e-02,  9.9576e-03, -7.4398e-03,\n",
       "                       7.0797e-04, -3.8907e-02,  7.0740e-03, -4.6459e-04, -5.9291e-03,\n",
       "                      -2.2493e-03, -1.2814e-02,  1.4889e-02,  1.2703e-02,  6.4222e-02,\n",
       "                      -2.5855e-02,  2.0836e-02, -6.5588e-05,  2.1274e-01, -1.7475e-02,\n",
       "                       4.5147e-02, -1.8587e-02, -3.8500e-02,  5.1550e-02, -5.7145e-02,\n",
       "                      -2.8767e-02, -2.2261e-02,  1.5100e-02,  6.1702e-02, -3.4336e-02,\n",
       "                      -1.7536e-02, -1.5905e-02, -4.8045e-03, -2.0240e-02, -1.7800e-02,\n",
       "                       4.8466e-03,  1.9954e-02,  9.6938e-03,  1.4889e-02, -1.3615e-02,\n",
       "                      -2.6561e-02, -7.6484e-03, -3.0847e-02, -1.6173e-02, -3.4473e-02,\n",
       "                       6.2201e-03,  1.8201e-02,  1.3830e-03, -2.8331e-02, -1.3367e-03,\n",
       "                       4.8360e-03, -2.4540e-02,  2.5706e-02,  9.6267e-03,  4.3016e-02,\n",
       "                       3.2626e-02,  4.8273e-02,  7.7523e-03,  4.8578e-03, -3.2045e-02,\n",
       "                      -3.8207e-02,  9.1259e-03,  4.2734e-03,  1.9891e-03, -2.6151e-02,\n",
       "                       1.9287e-02, -1.0776e-02,  2.3878e-02,  9.2189e-04, -4.1991e-02,\n",
       "                      -1.0245e-02, -1.0637e-02,  3.1816e-03, -1.9252e-02, -1.2754e-02,\n",
       "                       2.5877e-02,  7.1347e-03,  3.5014e-02,  3.9283e-02,  6.6984e-03,\n",
       "                       2.0968e-02, -2.9567e-03,  2.0530e-03,  2.8386e-02, -1.2468e-02,\n",
       "                      -2.7210e-02, -3.1698e-03, -1.8469e-02,  1.3004e-02, -6.4480e-03,\n",
       "                      -2.8033e-03, -1.8788e-02,  6.4205e-03,  4.6406e-02, -2.0303e-02,\n",
       "                       5.0751e-03, -1.1164e-03, -8.7773e-03, -2.7854e-02, -4.1057e-02,\n",
       "                      -1.6284e-02, -3.9072e-02,  2.1551e-02, -6.7594e-03, -6.5069e-03,\n",
       "                      -3.0189e-02, -2.7566e-02, -1.0713e-02,  3.7547e-02, -2.3399e-02,\n",
       "                       3.9334e-03, -3.5724e-03,  7.6018e-03, -4.2186e-03, -4.9886e-02,\n",
       "                      -1.5044e-02, -2.2578e-02,  9.1955e-03,  3.3574e-02,  3.2603e-02,\n",
       "                      -1.4665e-02,  1.7773e-02, -4.1015e-02, -4.6076e-03, -2.2243e-02,\n",
       "                       6.0253e-03, -1.1757e-03, -3.5249e-02,  2.3443e-03, -4.7081e-02,\n",
       "                      -4.2205e-05, -6.8597e-02, -2.2283e-02,  2.3223e-03, -1.7610e-02,\n",
       "                      -6.3474e-02, -6.9449e-04,  2.1100e-02, -6.8758e-03,  2.1310e-02,\n",
       "                       1.0262e-02,  1.7906e-02,  2.3711e-02,  2.9387e-02,  4.7565e-02,\n",
       "                      -2.6301e-02, -1.2330e-02, -1.9088e-02,  2.0018e-02, -1.0772e-02,\n",
       "                       3.9797e-02,  6.2730e-02, -2.2664e-02, -6.7754e-04, -4.1695e-02,\n",
       "                       1.2351e-02,  1.6211e-02,  1.0875e-02,  2.3206e-02, -2.5104e-02,\n",
       "                       2.7987e-02,  4.0272e-02, -3.3917e-02, -2.4601e-02,  1.1000e-02,\n",
       "                      -1.0244e-03,  5.9045e-02,  1.0139e-02, -4.5004e-03,  4.4733e-02,\n",
       "                       6.7916e-03,  2.7132e-02, -8.2781e-02,  1.8378e-03,  1.5068e-02,\n",
       "                       2.3207e-03, -2.4541e-02,  1.8422e-02,  1.6108e-03,  1.5697e-02,\n",
       "                      -1.4752e-02, -4.1018e-02,  1.4051e-02,  2.2660e-02,  2.1343e-02,\n",
       "                       1.4976e-02,  1.1573e-02, -2.3084e-02,  3.8753e-02,  3.1503e-02,\n",
       "                       6.5328e-03,  1.2225e-02, -1.7913e-02, -2.4384e-02, -1.0713e-02,\n",
       "                       1.6644e-02, -1.4123e-02, -1.5119e-02,  4.0342e-02, -8.8506e-03,\n",
       "                      -4.5804e-02, -1.1338e-02,  5.9795e-03,  5.9467e-02, -4.9633e-02,\n",
       "                       1.6640e-03, -2.9966e-02, -5.7389e-02, -4.0567e-03, -9.2933e-03,\n",
       "                      -1.0788e-03,  2.3678e-03, -3.9658e-04,  4.9519e-02,  1.2237e-02,\n",
       "                      -7.9948e-03, -1.5435e-03,  2.9651e-03,  3.2232e-02,  5.3149e-03,\n",
       "                       2.0513e-02,  1.8150e-03,  4.7334e-02,  1.2212e-02, -3.4130e-02,\n",
       "                       1.3201e-02,  3.5879e-02, -3.8646e-02, -9.9188e-03,  2.3027e-03,\n",
       "                      -3.2878e-03,  5.4200e-02,  1.3471e-02,  1.0555e-02,  3.4594e-02,\n",
       "                      -1.8318e-02,  3.6384e-02,  5.3466e-03,  7.3359e-03,  1.2232e-02,\n",
       "                      -1.9345e-03, -7.8361e-03, -2.6950e-02,  4.9725e-02, -2.9327e-02,\n",
       "                       2.1251e-02, -2.0641e-02, -1.7502e-02,  1.0960e-03,  3.6896e-02,\n",
       "                      -4.8817e-02,  4.2501e-02,  8.4078e-03, -2.0099e-04,  2.9245e-02,\n",
       "                       2.9372e-02,  5.4621e-02,  1.2574e-02,  9.2361e-03,  1.5445e-02,\n",
       "                      -1.7725e-02,  2.9537e-03, -7.4612e-03,  8.5239e-03,  1.6325e-02,\n",
       "                      -1.4316e-02,  2.3556e-02,  5.6409e-03, -2.2882e-02,  4.4118e-02,\n",
       "                      -4.3669e-02,  2.8426e-02, -2.0259e-02, -1.2426e-03,  1.3117e-02,\n",
       "                       3.5147e-02,  3.3595e-02, -1.4089e-02, -1.1460e-02, -2.3145e-02,\n",
       "                       8.8399e-03,  4.4735e-02, -1.1084e-02, -2.2923e-02, -5.7915e-02,\n",
       "                      -2.0574e-02,  1.0662e-02, -5.7848e-02], device='cuda:0')),\n",
       "             ('layers.0.anorm.weight',\n",
       "              tensor([0.3159, 0.2982, 0.2491, 0.2453, 0.3371, 0.4281, 0.2753, 0.4319, 0.2758,\n",
       "                      0.1720, 0.2678, 0.4787, 0.1817, 0.2905, 0.2285, 0.5248, 0.3483, 0.3135,\n",
       "                      0.4840, 0.2743, 0.2182, 0.3113, 0.2135, 0.3161, 0.2308, 0.4177, 0.5136,\n",
       "                      0.1788, 0.2209, 0.2354, 0.2597, 0.1716, 0.1291, 0.3178, 0.2941, 0.1631,\n",
       "                      0.2581, 0.3556, 0.6001, 0.2231, 0.2608, 0.2960, 0.1989, 0.3523, 0.1160,\n",
       "                      0.1524, 0.3779, 0.2795, 0.3190, 0.4532, 0.2162, 0.1912, 0.4297, 0.1494,\n",
       "                      0.5420, 0.2562, 0.4171, 0.4092, 0.4395, 0.2619, 0.2102, 0.3868, 0.4023,\n",
       "                      0.1758, 0.1793, 0.4963, 0.3231, 0.2660, 0.3040, 0.2516, 0.2388, 0.3366,\n",
       "                      0.1855, 0.2390, 0.2121, 0.2516, 0.1469, 0.1988, 0.2123, 0.2661, 0.1705,\n",
       "                      0.2005, 0.5263, 0.3953, 0.2577, 0.2723, 0.2489, 0.4717, 0.3344, 0.2244,\n",
       "                      0.2198, 0.1439, 0.2903, 0.1622, 0.1897, 0.1852, 0.2304, 0.2573, 0.2581,\n",
       "                      0.3688, 0.1685, 0.4403, 0.2911, 0.5174, 0.1668, 0.4488, 0.2435, 0.2405,\n",
       "                      0.1897, 0.1753, 0.1664, 0.2648, 0.2433, 0.1620, 0.4068, 0.3607, 0.2305,\n",
       "                      0.1852, 0.2555, 0.1955, 0.3600, 0.3189, 0.3267, 0.2260, 0.2472, 0.1311,\n",
       "                      0.1996, 0.2120, 0.3239, 0.2007, 0.1542, 0.5184, 0.2704, 0.2613, 0.3690,\n",
       "                      0.2058, 0.2664, 0.2516, 0.2775, 0.2500, 0.1726, 0.1736, 0.2439, 0.1762,\n",
       "                      0.2062, 0.2190, 0.2894, 0.3752, 0.3006, 0.1868, 0.1673, 0.1281, 0.4372,\n",
       "                      0.5627, 0.2245, 0.3992, 0.3433, 0.2175, 0.2610, 0.2506, 0.2422, 0.2490,\n",
       "                      0.2922, 0.2757, 0.3220, 0.2175, 0.2051, 0.1964, 0.2428, 0.3534, 0.4457,\n",
       "                      0.3126, 0.2675, 0.1919, 0.2646, 0.2038, 0.1747, 0.2110, 0.2333, 0.2465,\n",
       "                      0.1442, 0.1708, 0.4320, 0.2422, 0.3042, 0.4400, 0.2659, 0.2475, 0.2315,\n",
       "                      0.3246, 0.2538, 0.2964, 0.2336, 0.3178, 0.2239, 0.1812, 0.2668, 0.2204,\n",
       "                      0.1868, 0.2141, 0.3071, 0.4630, 0.5151, 0.3379, 0.4764, 0.2382, 0.3179,\n",
       "                      0.4018, 0.2389, 0.4236, 0.3636, 0.2531, 0.1894, 0.3104, 0.2095, 0.4274,\n",
       "                      0.5624, 0.2893, 0.2498, 0.2831, 0.4036, 0.2714, 0.2353, 0.2180, 0.2450,\n",
       "                      0.3765, 0.5720, 0.2469, 0.1826, 0.4055, 0.3184, 0.5261, 0.5412, 0.1733,\n",
       "                      0.1822, 0.3481, 0.2327, 0.1504, 0.4432, 0.1446, 0.2568, 0.3477, 0.3194,\n",
       "                      0.3220, 0.3429, 0.3102, 0.3092, 0.5316, 0.2165, 0.1767, 0.2745, 0.2619,\n",
       "                      0.4532, 0.2594, 0.4529, 0.4258, 0.3332, 0.3139, 0.2050, 0.3035, 0.2632,\n",
       "                      0.2094, 0.2099, 0.2580, 0.2747, 0.1329, 0.2668, 0.3051, 0.1738, 0.1412,\n",
       "                      0.2769, 0.4669, 0.2672, 0.4833, 0.2381, 0.3588, 0.3710, 0.2821, 0.5014,\n",
       "                      0.3931, 0.4595, 0.3353, 0.4680, 0.4036, 0.2656, 0.5834, 0.3682, 0.2718,\n",
       "                      0.1561, 0.2817, 0.3999, 0.4459, 0.2944, 0.3782, 0.3335, 0.2639, 0.1057,\n",
       "                      0.2292, 0.4704, 0.2357, 0.1797, 0.3944, 0.2538, 0.5343, 0.2313, 0.2949,\n",
       "                      0.4562, 0.2417, 0.2946, 0.2174, 0.3835, 0.3354, 0.3014, 0.1840, 0.4882,\n",
       "                      0.2813, 0.2309, 0.3381, 0.2835, 0.3514, 0.2303, 0.2881, 0.3483, 0.2098,\n",
       "                      0.1358, 0.4974, 0.2335, 0.2294, 0.1636, 0.2315, 0.2115, 0.2993, 0.3425,\n",
       "                      0.3464, 0.2690, 0.4433, 0.2523, 0.1579, 0.2493, 0.1960, 0.1617, 0.2008,\n",
       "                      0.1024, 0.3765, 0.1401, 0.1811, 0.1536, 0.5053, 0.1969, 0.2176, 0.2666,\n",
       "                      0.2016, 0.2728, 0.2831, 0.3878, 0.2297, 0.2286, 0.2814, 0.4186, 0.3152,\n",
       "                      0.3349, 0.1166, 0.3856, 0.3085, 0.3116, 0.1426, 0.2258, 0.2657, 0.1762,\n",
       "                      0.2465, 0.2952, 0.3107, 0.2287, 0.2006, 0.1109, 0.1837, 0.3889, 0.1674,\n",
       "                      0.3068, 0.2347, 0.4274, 0.4566, 0.1965, 0.2752, 0.5647, 0.3163, 0.1746,\n",
       "                      0.2689, 0.2080, 0.3526, 0.1856, 0.4170, 0.2564, 0.1706, 0.2233, 0.2050,\n",
       "                      0.4620, 0.2008, 0.2091, 0.2190, 0.2222, 0.1564, 0.2381, 0.3321, 0.2917,\n",
       "                      0.2849, 0.3831, 0.2294, 0.4747, 0.4557, 0.1812, 0.2666, 0.2265, 0.3422,\n",
       "                      0.3225, 0.2649, 0.1936, 0.5212, 0.3029, 0.2933, 0.2540, 0.1595, 0.2995,\n",
       "                      0.2590, 0.2035, 0.2898, 0.2886, 0.3028, 0.2444, 0.2403, 0.2625, 0.2978,\n",
       "                      0.2284, 0.2190, 0.3903, 0.4009, 0.2443, 0.5204, 0.2256, 0.3234, 0.2605,\n",
       "                      0.3186, 0.3289, 0.4411, 0.3181, 0.2811, 0.3303, 0.4848, 0.3014, 0.1821,\n",
       "                      0.3825, 0.1847, 0.3178, 0.2978, 0.2340, 0.3156, 0.2535, 0.3422, 0.2480,\n",
       "                      0.2950, 0.1694, 0.1195, 0.2646, 0.5132, 0.3067, 0.2092, 0.2222, 0.2717,\n",
       "                      0.2246, 0.1702, 0.2053, 0.1309, 0.2648, 0.1939, 0.1597, 0.4059, 0.4117,\n",
       "                      0.2875, 0.1067, 0.2397, 0.2847, 0.3889, 0.3037, 0.2824, 0.1765, 0.3562,\n",
       "                      0.2406, 0.3236, 0.2164, 0.1892, 0.3657, 0.2554, 0.2282, 0.4420, 0.4359,\n",
       "                      0.2792, 0.3324, 0.3218, 0.1773, 0.1844, 0.2791, 0.3729, 0.3083, 0.0717,\n",
       "                      0.2661, 0.1931, 0.1344, 0.2390, 0.4333, 0.4053, 0.2019, 0.2447, 0.2703,\n",
       "                      0.1791, 0.1830, 0.1696, 0.2706, 0.2326, 0.3990, 0.1429, 0.1864, 0.2796,\n",
       "                      0.3184, 0.2289, 0.6599, 0.1997, 0.3637, 0.2855, 0.2911, 0.3584, 0.3774,\n",
       "                      0.2923, 0.4462, 0.2802, 0.3460, 0.1508, 0.3708, 0.2666, 0.4513, 0.2293,\n",
       "                      0.1365, 0.1539, 0.1796, 0.3360, 0.3332, 0.4979, 0.3061, 0.3067, 0.2279,\n",
       "                      0.2666, 0.6426, 0.3755, 0.1640, 0.3664, 0.2625, 0.4744, 0.6263, 0.2617,\n",
       "                      0.2020, 0.4376, 0.2792, 0.4535, 0.3068, 0.2325, 0.4008, 0.3955, 0.2874,\n",
       "                      0.2366, 0.1734, 0.2811, 0.3084, 0.2653, 0.3589, 0.1278, 0.3011, 0.3039,\n",
       "                      0.3374, 0.0929, 0.2555, 0.5562, 0.2233, 0.2191, 0.2094, 0.3040, 0.2420,\n",
       "                      0.3273, 0.2428, 0.3555, 0.1823, 0.3795, 0.3889, 0.2280, 0.3445, 0.4164,\n",
       "                      0.2222, 0.2485, 0.1698, 0.2053, 0.3959, 0.3499, 0.3053, 0.3268, 0.2616,\n",
       "                      0.3602, 0.2168, 0.3492, 0.3689, 0.2380, 0.2326, 0.2058, 0.2389, 0.4176,\n",
       "                      0.2822, 0.2987, 0.4114, 0.3109, 0.1908, 0.3310, 0.2181, 0.4545, 0.2501,\n",
       "                      0.3182, 0.3030, 0.3127, 0.2549, 0.3135, 0.3116, 0.2493, 0.2143, 0.5681,\n",
       "                      0.5077, 0.2666, 0.2107, 0.2678, 0.3220, 0.2608, 0.1943, 0.3241, 0.2078,\n",
       "                      0.3307, 0.5709, 0.3846, 0.3639, 0.1542, 0.2620, 0.2914, 0.2324, 0.5107,\n",
       "                      0.2539, 0.1247, 0.3087, 0.2926, 0.2382, 0.1780, 0.3390, 0.3573, 0.2990,\n",
       "                      0.3355, 0.4347, 0.2341, 0.4044, 0.1935, 0.2133, 0.3366, 0.3130, 0.3379,\n",
       "                      0.2539, 0.5619, 0.2123, 0.1903, 0.1417, 0.3095, 0.2195, 0.1713, 0.3672,\n",
       "                      0.3520, 0.2102, 0.3549, 0.1032, 0.2872, 0.3882, 0.1191, 0.3806, 0.2299,\n",
       "                      0.1597, 0.2802, 0.2076, 0.3391, 0.1214, 0.3915, 0.1796, 0.3181, 0.4296,\n",
       "                      0.2420, 0.3115, 0.3594, 0.2190, 0.3056, 0.2236, 0.1951, 0.5382, 0.3421,\n",
       "                      0.1824, 0.3992, 0.3333, 0.4423, 0.2484, 0.2925, 0.5040, 0.3081, 0.5017,\n",
       "                      0.2909, 0.3082, 0.3151, 0.1342, 0.4124, 0.2435, 0.2764, 0.2871, 0.3170,\n",
       "                      0.2487, 0.2316, 0.2904, 0.4722, 0.3551, 0.2542, 0.2711, 0.1744, 0.2854,\n",
       "                      0.1799, 0.2417, 0.3210, 0.1311, 0.3113, 0.4729, 0.3232, 0.2423, 0.1501,\n",
       "                      0.4269, 0.2897, 0.4281, 0.2759, 0.2658, 0.3965, 0.3887, 0.2705, 0.1982,\n",
       "                      0.2541, 0.2542, 0.5297, 0.3004, 0.1711, 0.3204, 0.5813, 0.5520, 0.1936,\n",
       "                      0.4102, 0.2157, 0.2623, 0.2515, 0.3730, 0.3363, 0.2751, 0.3350, 0.3420,\n",
       "                      0.3166, 0.1948, 0.4718], device='cuda:0')),\n",
       "             ('layers.0.fnorm.weight',\n",
       "              tensor([0.2523, 0.2304, 0.2797, 0.2978, 0.3145, 0.2310, 0.3042, 0.4021, 0.2958,\n",
       "                      0.2129, 0.2698, 0.2971, 0.2746, 0.2547, 0.2818, 0.3049, 0.2951, 0.2821,\n",
       "                      0.2650, 0.3190, 0.3607, 0.3092, 0.2847, 0.2188, 0.2871, 0.2589, 0.3081,\n",
       "                      0.2574, 0.3193, 0.2406, 0.2810, 0.2589, 0.2174, 0.2820, 0.2911, 0.2718,\n",
       "                      0.2525, 0.2839, 0.2488, 0.2897, 0.2150, 0.3200, 0.2634, 0.3073, 0.2713,\n",
       "                      0.2548, 0.2651, 0.2853, 0.2404, 0.2895, 0.3038, 0.2234, 0.3308, 0.2709,\n",
       "                      0.2926, 0.2975, 0.2556, 0.2792, 0.3287, 0.3038, 0.2251, 0.3367, 0.2784,\n",
       "                      0.2421, 0.2347, 0.2629, 0.2567, 0.2765, 0.2868, 0.2759, 0.2366, 0.2983,\n",
       "                      0.2692, 0.2822, 0.2835, 0.2579, 0.2553, 0.2620, 0.2902, 0.2342, 0.2478,\n",
       "                      0.2862, 0.2815, 0.2516, 0.2919, 0.2697, 0.2319, 0.3068, 0.2791, 0.2894,\n",
       "                      0.2724, 0.2528, 0.2482, 0.2962, 0.2893, 0.2425, 0.3864, 0.2543, 0.2726,\n",
       "                      0.3175, 0.2722, 0.2816, 0.2975, 0.3281, 0.2761, 0.3143, 0.2708, 0.2609,\n",
       "                      0.2656, 0.2770, 0.2841, 0.2578, 0.2761, 0.2242, 0.3001, 0.2684, 0.3066,\n",
       "                      0.2412, 0.2447, 0.2695, 0.2878, 0.3309, 0.3274, 0.2916, 0.2426, 0.2289,\n",
       "                      0.2770, 0.2963, 0.2903, 0.2533, 0.2831, 0.3098, 0.2571, 0.2987, 0.2242,\n",
       "                      0.2852, 0.2459, 0.3035, 0.2831, 0.2670, 0.2640, 0.2333, 0.2549, 0.2562,\n",
       "                      0.2703, 0.2397, 0.2515, 0.2711, 0.2999, 0.2684, 0.2700, 0.2570, 0.2666,\n",
       "                      0.2608, 0.3397, 0.2439, 0.3017, 0.2962, 0.3000, 0.3067, 0.2838, 0.2522,\n",
       "                      0.2331, 0.2500, 0.2957, 0.2686, 0.2516, 0.2592, 0.2955, 0.2810, 0.2781,\n",
       "                      0.2796, 0.2682, 0.2946, 0.3127, 0.2829, 0.2332, 0.2888, 0.2451, 0.2379,\n",
       "                      0.2478, 0.2621, 0.3285, 0.2877, 0.2838, 0.3181, 0.2737, 0.2449, 0.2516,\n",
       "                      0.2731, 0.2771, 0.2945, 0.2476, 0.2492, 0.2573, 0.2292, 0.2694, 0.2980,\n",
       "                      0.2291, 0.2391, 0.2934, 0.2637, 0.3535, 0.2556, 0.2678, 0.2483, 0.2986,\n",
       "                      0.2820, 0.3032, 0.3162, 0.3050, 0.3308, 0.2578, 0.3046, 0.2514, 0.3030,\n",
       "                      0.2599, 0.3004, 0.2434, 0.2695, 0.2877, 0.2765, 0.2589, 0.2635, 0.2592,\n",
       "                      0.2806, 0.0025, 0.3203, 0.2659, 0.2806, 0.2847, 0.2766, 0.2730, 0.2465,\n",
       "                      0.2677, 0.2474, 0.2261, 0.2629, 0.3256, 0.3225, 0.3263, 0.3102, 0.2948,\n",
       "                      0.2774, 0.2833, 0.2559, 0.2382, 0.2873, 0.2980, 0.2619, 0.2929, 0.2540,\n",
       "                      0.3106, 0.2600, 0.2778, 0.2729, 0.2675, 0.2881, 0.2596, 0.2455, 0.2779,\n",
       "                      0.2360, 0.2748, 0.2616, 0.2098, 0.2759, 0.2532, 0.2865, 0.2552, 0.2893,\n",
       "                      0.2521, 0.3186, 0.2787, 0.2953, 0.3045, 0.2403, 0.2914, 0.2479, 0.2981,\n",
       "                      0.2386, 0.2537, 0.2815, 0.3223, 0.2894, 0.3065, 0.2806, 0.2373, 0.2932,\n",
       "                      0.3175, 0.2809, 0.2922, 0.2940, 0.3183, 0.2593, 0.2715, 0.2639, 0.2170,\n",
       "                      0.2655, 0.3090, 0.2757, 0.2252, 0.3104, 0.2750, 0.2340, 0.2421, 0.3353,\n",
       "                      0.2873, 0.2511, 0.2980, 0.2878, 0.2784, 0.3278, 0.2759, 0.2918, 0.3073,\n",
       "                      0.2742, 0.3024, 0.2413, 0.2818, 0.2623, 0.3107, 0.3002, 0.2759, 0.2882,\n",
       "                      0.2439, 0.2872, 0.2819, 0.3026, 0.2569, 0.3046, 0.3292, 0.2698, 0.2746,\n",
       "                      0.2977, 0.2770, 0.3034, 0.3230, 0.2489, 0.2514, 0.2443, 0.2681, 0.2355,\n",
       "                      0.2716, 0.2741, 0.2286, 0.2577, 0.2345, 0.2623, 0.2938, 0.2968, 0.2914,\n",
       "                      0.2553, 0.2726, 0.2900, 0.2731, 0.2827, 0.2523, 0.2397, 0.2841, 0.3110,\n",
       "                      0.2834, 0.2486, 0.2666, 0.2631, 0.2887, 0.2385, 0.2869, 0.2919, 0.2801,\n",
       "                      0.2660, 0.2222, 0.3023, 0.2475, 0.2199, 0.2649, 0.2627, 0.2684, 0.2066,\n",
       "                      0.2677, 0.2774, 0.2946, 0.2914, 0.2618, 0.3285, 0.3330, 0.3408, 0.2885,\n",
       "                      0.2700, 0.2871, 0.3026, 0.2392, 0.2442, 0.2577, 0.2330, 0.2571, 0.2997,\n",
       "                      0.3170, 0.2618, 0.2349, 0.2572, 0.3010, 0.2728, 0.2538, 0.2378, 0.3013,\n",
       "                      0.3288, 0.3081, 0.2782, 0.0037, 0.2778, 0.2643, 0.2502, 0.2720, 0.2902,\n",
       "                      0.2328, 0.2883, 0.2739, 0.2289, 0.3229, 0.3203, 0.2737, 0.2878, 0.3155,\n",
       "                      0.2983, 0.2427, 0.2728, 0.2903, 0.2554, 0.3207, 0.2607, 0.2749, 0.3158,\n",
       "                      0.2792, 0.2908, 0.3053, 0.3110, 0.2825, 0.3149, 0.2640, 0.2297, 0.2281,\n",
       "                      0.2919, 0.2340, 0.2573, 0.3412, 0.2465, 0.2858, 0.2795, 0.3141, 0.2111,\n",
       "                      0.3642, 0.2763, 0.3452, 0.2993, 0.2436, 0.2809, 0.2865, 0.3265, 0.3258,\n",
       "                      0.3435, 0.2925, 0.2558, 0.2836, 0.3311, 0.2671, 0.2712, 0.2910, 0.2658,\n",
       "                      0.2694, 0.2684, 0.3041, 0.2500, 0.2430, 0.2806, 0.2735, 0.2621, 0.2563,\n",
       "                      0.3318, 0.2450, 0.2593, 0.3136, 0.3420, 0.2897, 0.2725, 0.3045, 0.3021,\n",
       "                      0.2571, 0.2558, 0.3028, 0.2231, 0.3236, 0.2993, 0.2761, 0.2754, 0.2735,\n",
       "                      0.2439, 0.2933, 0.2697, 0.0087, 0.2410, 0.3009, 0.2546, 0.3532, 0.2175,\n",
       "                      0.2768, 0.2605, 0.2392, 0.2880, 0.2941, 0.3045, 0.2197, 0.2780, 0.2975,\n",
       "                      0.2833, 0.2582, 0.2713, 0.3083, 0.2566, 0.2851, 0.2723, 0.2645, 0.2653,\n",
       "                      0.2902, 0.3366, 0.3277, 0.2520, 0.3005, 0.2705, 0.3038, 0.2914, 0.2887,\n",
       "                      0.2963, 0.2741, 0.2648, 0.3056, 0.2766, 0.2868, 0.2980, 0.2519, 0.2809,\n",
       "                      0.2405, 0.2884, 0.2662, 0.2934, 0.3029, 0.2472, 0.3027, 0.2830, 0.2670,\n",
       "                      0.2686, 0.2996, 0.2460, 0.2751, 0.2839, 0.3446, 0.2913, 0.3825, 0.3001,\n",
       "                      0.2443, 0.3284, 0.3061, 0.2746, 0.2295, 0.2281, 0.3225, 0.2864, 0.3164,\n",
       "                      0.2940, 0.2513, 0.2691, 0.2488, 0.3020, 0.3541, 0.2527, 0.2633, 0.2758,\n",
       "                      0.2858, 0.2940, 0.3104, 0.2753, 0.2747, 0.2365, 0.2676, 0.2552, 0.2508,\n",
       "                      0.2993, 0.2668, 0.2990, 0.2473, 0.2720, 0.2771, 0.2772, 0.2544, 0.2823,\n",
       "                      0.2306, 0.2627, 0.2728, 0.2449, 0.2939, 0.2612, 0.2810, 0.2514, 0.2756,\n",
       "                      0.2661, 0.3150, 0.2781, 0.2957, 0.2871, 0.2683, 0.3116, 0.2898, 0.3051,\n",
       "                      0.2532, 0.2980, 0.3041, 0.2575, 0.2916, 0.3041, 0.2647, 0.2927, 0.2567,\n",
       "                      0.3559, 0.3044, 0.2577, 0.2609, 0.2791, 0.3325, 0.3137, 0.2864, 0.2600,\n",
       "                      0.2893, 0.2713, 0.2515, 0.2911, 0.2962, 0.2582, 0.2421, 0.3180, 0.2767,\n",
       "                      0.2795, 0.3268, 0.2805, 0.2892, 0.2775, 0.2713, 0.2639, 0.3113, 0.3000,\n",
       "                      0.2965, 0.2274, 0.3044, 0.2929, 0.3025, 0.2722, 0.3421, 0.2966, 0.2921,\n",
       "                      0.2698, 0.2824, 0.2740, 0.3200, 0.2379, 0.2587, 0.3024, 0.2921, 0.3127,\n",
       "                      0.3279, 0.3197, 0.2519, 0.2850, 0.2413, 0.3322, 0.2980, 0.2779, 0.3057,\n",
       "                      0.3051, 0.2683, 0.3189, 0.2136, 0.3226, 0.2287, 0.2394, 0.2802, 0.3122,\n",
       "                      0.3040, 0.2754, 0.2599, 0.3474, 0.2510, 0.3226, 0.2764, 0.2260, 0.2873,\n",
       "                      0.2283, 0.2514, 0.2820, 0.3264, 0.2922, 0.2686, 0.2508, 0.3063, 0.2417,\n",
       "                      0.2634, 0.2999, 0.2819, 0.3267, 0.2600, 0.3260, 0.2938, 0.2973, 0.3016,\n",
       "                      0.2821, 0.2541, 0.2674, 0.2770, 0.2539, 0.2691, 0.2661, 0.2397, 0.3070,\n",
       "                      0.3247, 0.2423, 0.2980, 0.2963, 0.2156, 0.2550, 0.2605, 0.2996, 0.3142,\n",
       "                      0.2511, 0.2756, 0.3191, 0.2674, 0.3297, 0.2810, 0.2980, 0.3214, 0.2578,\n",
       "                      0.2407, 0.3286, 0.2991, 0.2531, 0.2825, 0.3130, 0.3044, 0.2748, 0.2998,\n",
       "                      0.2766, 0.2683, 0.3172, 0.2908, 0.2744, 0.2289, 0.3170, 0.2908, 0.2666,\n",
       "                      0.2849, 0.2560, 0.3131, 0.2884, 0.3051, 0.2781, 0.3105, 0.2936, 0.2182,\n",
       "                      0.3029, 0.2839, 0.3216], device='cuda:0')),\n",
       "             ('layers.1.mha.query.weight',\n",
       "              tensor([[-0.0292,  0.0695, -0.0348,  ...,  0.0335,  0.0276, -0.0155],\n",
       "                      [ 0.1246, -0.0401, -0.0278,  ..., -0.0359,  0.0414, -0.0029],\n",
       "                      [-0.0216,  0.0188, -0.0077,  ...,  0.0043, -0.0263,  0.0022],\n",
       "                      ...,\n",
       "                      [ 0.0057, -0.0059, -0.0134,  ...,  0.0475,  0.0180, -0.0810],\n",
       "                      [-0.0017, -0.0095, -0.0044,  ...,  0.0475, -0.0245, -0.0206],\n",
       "                      [ 0.0419, -0.1011,  0.0391,  ...,  0.0158,  0.0018, -0.0502]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.mha.key.weight',\n",
       "              tensor([[ 0.0908, -0.0420, -0.0285,  ..., -0.0621,  0.0240, -0.0628],\n",
       "                      [ 0.0202,  0.0266, -0.0944,  ..., -0.0502, -0.0356,  0.0714],\n",
       "                      [ 0.0255,  0.0300, -0.0887,  ...,  0.1090, -0.0758, -0.0209],\n",
       "                      ...,\n",
       "                      [-0.0427,  0.0410, -0.0614,  ...,  0.0723, -0.0119, -0.0440],\n",
       "                      [ 0.0313,  0.0192,  0.0138,  ...,  0.0496, -0.1550, -0.0334],\n",
       "                      [ 0.0353,  0.0603,  0.0293,  ...,  0.0556, -0.0332, -0.0481]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.mha.value.weight',\n",
       "              tensor([[-0.0373,  0.0192,  0.0016,  ..., -0.0577,  0.0068,  0.0211],\n",
       "                      [-0.0117, -0.0289, -0.0302,  ..., -0.0182, -0.0291,  0.0160],\n",
       "                      [-0.0656, -0.0140, -0.0489,  ...,  0.0148,  0.0237, -0.0038],\n",
       "                      ...,\n",
       "                      [ 0.0001, -0.0144, -0.0307,  ...,  0.0402, -0.0701,  0.0563],\n",
       "                      [-0.0110,  0.0034, -0.0371,  ..., -0.0368, -0.0032,  0.0071],\n",
       "                      [ 0.0237,  0.0394,  0.0638,  ...,  0.0043,  0.0071, -0.0209]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.mha.proj.weight',\n",
       "              tensor([[-0.0074, -0.0017,  0.0563,  ...,  0.0023,  0.0743,  0.0113],\n",
       "                      [ 0.0110, -0.0462, -0.0024,  ..., -0.0094,  0.0161, -0.0136],\n",
       "                      [ 0.0583,  0.0204,  0.0108,  ..., -0.0093,  0.0303, -0.0482],\n",
       "                      ...,\n",
       "                      [ 0.0336, -0.0049,  0.0657,  ...,  0.0235,  0.0497, -0.0093],\n",
       "                      [ 0.0314,  0.0190,  0.0218,  ..., -0.0149,  0.0052,  0.0143],\n",
       "                      [-0.0496,  0.0155, -0.0474,  ...,  0.0347, -0.0600,  0.0244]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.w.weight',\n",
       "              tensor([[ 0.0438,  0.0243, -0.0102,  ..., -0.0535, -0.0395,  0.0147],\n",
       "                      [ 0.0462,  0.0105, -0.0459,  ...,  0.0147,  0.0432, -0.0540],\n",
       "                      [ 0.0109, -0.0225,  0.0468,  ...,  0.0245,  0.0384, -0.0394],\n",
       "                      ...,\n",
       "                      [ 0.0233, -0.0405,  0.0081,  ..., -0.0136, -0.0010,  0.0161],\n",
       "                      [ 0.0289, -0.0391, -0.0844,  ..., -0.0069,  0.0193, -0.0034],\n",
       "                      [-0.0032, -0.0262, -0.0328,  ..., -0.0391,  0.0326, -0.0167]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.w.bias',\n",
       "              tensor([-0.0654, -0.0787, -0.0947,  ..., -0.0440, -0.0432, -0.0137],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.v.weight',\n",
       "              tensor([[-0.0448, -0.0170,  0.0207,  ..., -0.0337,  0.0409, -0.1022],\n",
       "                      [-0.0554, -0.0088, -0.0379,  ...,  0.0070, -0.0215, -0.0239],\n",
       "                      [-0.0619,  0.0320, -0.0559,  ...,  0.1249, -0.0046,  0.0298],\n",
       "                      ...,\n",
       "                      [-0.0532,  0.0767, -0.0402,  ..., -0.0208,  0.0357,  0.0061],\n",
       "                      [-0.0077,  0.0596,  0.0057,  ...,  0.0001,  0.0407, -0.0194],\n",
       "                      [-0.0051,  0.0034, -0.0042,  ..., -0.0974,  0.0954, -0.0308]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.v.bias',\n",
       "              tensor([ 0.0179,  0.0003, -0.0194,  ...,  0.0087, -0.0116,  0.0135],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.w2.weight',\n",
       "              tensor([[ 0.0218,  0.0399, -0.0140,  ...,  0.0073, -0.0269, -0.0365],\n",
       "                      [-0.0126,  0.0256, -0.0732,  ..., -0.0504, -0.0483, -0.0033],\n",
       "                      [ 0.0309, -0.0267, -0.0623,  ..., -0.0027, -0.0199,  0.0462],\n",
       "                      ...,\n",
       "                      [ 0.0180, -0.0088, -0.0023,  ...,  0.1064,  0.0362, -0.0401],\n",
       "                      [ 0.0162, -0.0044,  0.0284,  ...,  0.0141, -0.0152, -0.0321],\n",
       "                      [-0.0124,  0.0072,  0.0375,  ...,  0.0180,  0.0072,  0.0699]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.1.ffn.w2.bias',\n",
       "              tensor([-4.1366e-02, -1.2589e-02, -5.8375e-03, -1.4961e-03,  6.2785e-02,\n",
       "                       2.2374e-02, -9.8448e-03,  4.1165e-02,  2.4676e-02,  1.5415e-02,\n",
       "                      -1.1986e-02, -1.4718e-02,  1.7590e-03, -2.3811e-03, -8.7144e-03,\n",
       "                      -5.4097e-02, -3.6430e-02,  1.3137e-02, -1.4973e-02,  2.2931e-02,\n",
       "                      -5.8392e-03,  7.0161e-03, -2.2980e-02, -1.0609e-02,  1.3362e-02,\n",
       "                      -1.2239e-02, -4.7078e-02, -8.0894e-03,  3.1924e-02, -1.5902e-02,\n",
       "                       1.4213e-03,  2.0533e-02,  1.7664e-02,  8.7219e-03,  3.2728e-02,\n",
       "                      -1.8252e-02,  3.8933e-02,  3.6946e-03,  5.5832e-02, -1.4289e-02,\n",
       "                      -2.6398e-02,  6.1106e-02,  2.4597e-02,  7.0350e-03, -6.3363e-03,\n",
       "                       8.0165e-03,  1.1521e-03, -1.5974e-02, -1.4966e-02, -5.7299e-02,\n",
       "                       3.3889e-03, -2.8279e-02, -4.6169e-02, -6.7061e-03, -4.5563e-03,\n",
       "                       3.1212e-02,  1.8231e-02,  1.1822e-02,  3.0958e-02,  2.6114e-03,\n",
       "                       2.6138e-02, -4.9852e-02, -1.3431e-02,  1.7801e-02,  2.9217e-02,\n",
       "                       2.8427e-02, -3.4042e-02,  1.2124e-02, -3.3014e-02,  1.1825e-02,\n",
       "                      -2.2353e-02, -2.7449e-02,  2.0730e-03, -2.1064e-02, -2.0597e-02,\n",
       "                      -1.1182e-02, -5.3709e-02,  6.8791e-03,  2.6853e-02, -5.7430e-03,\n",
       "                       3.7359e-02,  3.9256e-02, -1.0394e-01,  3.4802e-02,  2.6565e-02,\n",
       "                      -4.9749e-02, -1.6498e-04,  1.0380e-02,  8.1342e-03,  3.1188e-02,\n",
       "                      -2.5242e-02,  6.9883e-03,  2.2599e-02, -1.7977e-04,  1.9993e-02,\n",
       "                      -1.0408e-02, -3.8689e-02,  4.6806e-03, -2.3980e-03, -1.1657e-02,\n",
       "                       1.7295e-04,  2.9585e-02, -1.2829e-02,  5.5887e-02, -2.7656e-02,\n",
       "                      -5.9522e-03,  1.3619e-03,  3.3024e-02, -3.6262e-02, -8.8488e-03,\n",
       "                       3.3865e-02, -5.0348e-03,  6.1867e-04, -3.7769e-03, -3.1505e-02,\n",
       "                      -1.9970e-02,  6.5438e-02,  2.5266e-03, -2.8164e-02, -4.4565e-02,\n",
       "                       4.4411e-03, -1.8728e-03,  2.3285e-02, -7.5664e-03, -7.9822e-02,\n",
       "                      -3.4461e-02, -1.8204e-02,  1.0026e-04, -7.1346e-02,  1.2452e-02,\n",
       "                      -2.4322e-02, -5.5071e-02,  5.8493e-02, -3.0187e-02, -3.4836e-02,\n",
       "                      -2.9435e-02,  2.4707e-02,  5.0869e-02, -2.9394e-02,  1.5685e-02,\n",
       "                       3.5945e-02, -1.2546e-02,  1.3584e-02,  1.5228e-02,  7.4057e-03,\n",
       "                      -2.5892e-02, -9.0550e-03,  8.5481e-03, -9.0517e-03, -3.0732e-02,\n",
       "                       4.7808e-02, -7.2283e-03,  2.0112e-02,  1.5651e-02, -2.4337e-02,\n",
       "                      -3.0010e-02, -2.5066e-02, -1.3983e-02,  1.2386e-02,  4.1922e-02,\n",
       "                      -3.7832e-02, -2.5235e-02, -3.7226e-02,  9.4138e-04,  3.0575e-02,\n",
       "                       4.1404e-02, -8.1128e-03,  5.4501e-03, -8.1739e-03, -1.0630e-02,\n",
       "                      -4.3690e-02, -4.6161e-03, -1.6961e-02,  7.4220e-02,  5.8399e-02,\n",
       "                       3.3251e-04, -2.0741e-02,  1.4217e-02,  1.6862e-02,  1.2123e-02,\n",
       "                      -2.3617e-02, -3.2750e-02, -4.4177e-02, -1.2745e-02, -3.3443e-03,\n",
       "                       7.9220e-04,  1.7141e-03,  4.1532e-02,  1.3279e-03,  1.2514e-02,\n",
       "                       5.4980e-03, -1.3352e-02, -4.8614e-03,  3.8115e-02, -9.9522e-03,\n",
       "                      -3.2683e-02, -1.2347e-02, -3.5081e-03, -1.4034e-02, -8.9598e-03,\n",
       "                      -2.5133e-02, -3.4446e-02, -4.3537e-02,  7.2011e-02, -3.7324e-02,\n",
       "                       1.9638e-02, -4.2236e-02, -1.2086e-02, -5.4092e-02, -6.3554e-02,\n",
       "                      -8.6330e-03, -7.8393e-02, -3.7231e-02, -1.3618e-02, -6.2587e-03,\n",
       "                      -7.5758e-03,  3.3374e-02,  3.0395e-02, -8.8439e-03, -4.5505e-02,\n",
       "                      -2.2520e-03, -2.2674e-02, -4.1276e-03,  7.0887e-04, -9.9003e-05,\n",
       "                       5.5763e-03, -4.8917e-01,  4.7042e-03, -3.6218e-03, -3.3016e-02,\n",
       "                       5.9477e-02, -6.8263e-02, -2.5640e-02,  2.6931e-02,  4.8057e-04,\n",
       "                       7.4294e-03, -8.8290e-03, -2.8366e-02,  1.0567e-02,  2.9183e-02,\n",
       "                       3.8686e-02, -2.9136e-02,  1.0596e-02, -4.8718e-03,  3.2671e-02,\n",
       "                      -1.6322e-02, -1.6918e-02,  1.9478e-02, -3.2862e-03, -3.3331e-02,\n",
       "                       4.1355e-02,  7.0189e-03,  2.4454e-02, -3.9392e-02,  1.7590e-03,\n",
       "                       4.4439e-02, -7.4134e-03, -1.4101e-02, -4.4696e-02, -6.8655e-03,\n",
       "                      -4.7414e-03,  4.7952e-03,  4.9122e-02,  7.4517e-04, -2.9340e-02,\n",
       "                       2.5415e-02, -5.5345e-04,  2.4202e-02, -1.1387e-03, -2.4600e-02,\n",
       "                       1.1001e-02, -2.5182e-02, -4.2512e-03, -5.8889e-03, -7.4688e-03,\n",
       "                      -7.9235e-03, -4.5140e-03,  2.6914e-03, -4.5908e-02, -9.0689e-03,\n",
       "                       1.6090e-02, -1.2609e-03,  3.2261e-02, -1.1501e-02, -1.3258e-02,\n",
       "                       4.0523e-03, -3.6487e-04,  1.2212e-02,  2.0318e-02, -3.0476e-03,\n",
       "                       2.0083e-02,  1.2229e-02, -6.2711e-03, -7.4140e-03, -9.8499e-03,\n",
       "                      -2.4070e-02,  3.8430e-02,  5.2931e-02, -2.1917e-02, -8.6924e-03,\n",
       "                      -8.1823e-03,  8.8311e-02,  7.2826e-02, -4.4635e-02, -2.4531e-02,\n",
       "                       3.3423e-02, -6.3480e-02, -9.5889e-03, -1.1511e-02,  1.7867e-02,\n",
       "                       3.8568e-02, -7.3598e-04, -4.0939e-03,  9.4022e-03, -2.1614e-02,\n",
       "                      -2.0578e-02,  9.5232e-03, -5.5183e-02, -2.0653e-02, -2.2637e-02,\n",
       "                       8.3343e-02,  3.4598e-03,  9.6886e-03, -2.0392e-02,  1.2740e-02,\n",
       "                       1.4785e-02, -8.5070e-03, -7.4573e-03, -7.8802e-03,  1.8502e-02,\n",
       "                       1.9099e-02,  1.0912e-02, -2.5821e-02,  1.5102e-02, -3.0801e-02,\n",
       "                       7.2381e-04, -1.4740e-02,  2.0432e-02,  4.6210e-02,  1.6146e-02,\n",
       "                       5.2037e-02,  1.0949e-02, -4.4877e-02, -1.4801e-03, -2.4286e-03,\n",
       "                      -2.7676e-03, -4.3391e-02, -7.1383e-02, -1.7867e-02,  3.0169e-02,\n",
       "                      -2.2682e-02, -7.5376e-03,  3.8304e-02, -1.0977e-02,  3.5990e-02,\n",
       "                       1.9736e-02, -2.8741e-02, -1.0780e-02,  1.9699e-02, -2.0494e-02,\n",
       "                       7.5720e-03, -6.1141e-04,  1.2939e-02,  2.6814e-02,  1.1034e-02,\n",
       "                      -2.0032e-03,  1.3887e-02,  1.3258e-02, -3.0343e-02, -8.5816e-03,\n",
       "                      -1.7223e-02, -3.7648e-03, -7.4759e-03, -9.5271e-03,  3.3337e-02,\n",
       "                      -4.7536e-03, -1.6174e-02,  4.1922e-02, -2.3251e-03, -2.4902e-02,\n",
       "                      -1.5630e-02, -7.0820e-03, -1.3077e-02,  2.6412e-02,  4.7170e-02,\n",
       "                      -2.0460e-03, -2.3271e-02, -2.3758e-02, -1.5643e-02,  3.9254e-02,\n",
       "                      -2.8843e-03,  2.9170e-02,  1.2226e-02,  6.5457e-03,  9.3355e-03,\n",
       "                      -1.7445e-02, -3.2961e-02,  4.3278e-02,  2.2069e-02,  5.3001e-02,\n",
       "                      -5.5632e-02, -8.1607e-03,  6.9023e-03, -1.7376e-02, -3.4638e-02,\n",
       "                       6.5931e-03,  1.0033e-03,  3.5097e-02, -9.6271e-01, -4.5128e-02,\n",
       "                       4.1847e-02,  6.3991e-02,  3.6296e-02, -7.8054e-04,  2.2078e-02,\n",
       "                      -4.3627e-02, -1.3311e-03, -4.3149e-02,  1.6837e-02, -4.2748e-03,\n",
       "                       6.4985e-02,  3.0727e-02, -2.0649e-02, -1.0901e-02,  2.6706e-02,\n",
       "                      -3.7738e-02, -4.7149e-02, -3.2137e-04, -2.1983e-02, -5.7379e-02,\n",
       "                      -2.2871e-03,  3.9275e-02,  2.3659e-02, -3.1849e-02,  8.5389e-02,\n",
       "                      -7.5253e-03,  4.8303e-03,  4.0567e-02, -2.5692e-02, -6.1101e-03,\n",
       "                      -2.3347e-03, -2.2118e-02,  4.2980e-02, -2.4969e-02, -1.9900e-02,\n",
       "                      -4.8841e-03, -3.3101e-02, -1.6921e-02,  5.3824e-02,  2.0672e-02,\n",
       "                       1.4266e-02, -1.7825e-03,  5.8736e-02,  5.1985e-02, -3.1293e-02,\n",
       "                       3.6459e-02, -1.2702e-02, -4.5916e-02,  1.2980e-02, -5.0035e-02,\n",
       "                       2.7238e-02, -1.1559e-02, -5.2570e-03, -1.1189e-02, -3.0221e-02,\n",
       "                      -1.9894e-02,  4.0492e-02, -8.9490e-03, -1.2933e-02,  2.9817e-02,\n",
       "                       1.4773e-02, -6.5461e-03,  8.8795e-03,  2.0592e-03, -2.5600e-03,\n",
       "                      -7.6519e-03, -2.1766e-02,  1.3389e-02,  2.7381e-02,  1.9503e-03,\n",
       "                       1.2519e-02,  2.0105e-02, -1.1690e-02,  1.2494e-02, -4.2025e-02,\n",
       "                       1.9291e-02, -1.4034e-02,  1.0285e-02,  1.0499e-02, -3.1295e-04,\n",
       "                      -7.3685e-03,  2.7985e-03,  2.0153e-02,  3.9559e-02,  5.1613e-02,\n",
       "                      -2.9022e-02, -3.2879e-03,  3.8012e-02,  2.5933e-01, -3.6039e-02,\n",
       "                       2.2811e-03, -2.0523e-02, -5.8209e-02,  4.8125e-02, -1.7771e-02,\n",
       "                      -2.8018e-02,  1.6143e-02, -3.0685e-02,  6.3492e-02,  2.4757e-04,\n",
       "                       2.1908e-03, -1.8260e-02,  7.5732e-03, -9.8662e-03, -2.8391e-02,\n",
       "                      -3.2564e-02,  1.2142e-02,  1.5122e-03, -2.5361e-02, -3.0454e-02,\n",
       "                      -1.5823e-02, -9.3260e-03,  5.0773e-03, -2.1459e-03, -1.2654e-02,\n",
       "                       2.4688e-02,  1.1318e-02, -3.0711e-02, -2.1983e-02, -2.1525e-02,\n",
       "                       2.0166e-03, -2.8395e-02,  2.7942e-02,  1.8491e-02,  2.2234e-02,\n",
       "                       2.4723e-02,  2.2435e-02, -1.7508e-02,  3.7666e-02, -1.3551e-02,\n",
       "                      -3.0829e-02,  1.0868e-02, -3.6355e-02, -2.6553e-02, -3.0688e-02,\n",
       "                       2.0826e-02,  3.5874e-02,  9.6714e-04, -4.7814e-03, -5.3927e-02,\n",
       "                      -4.0154e-02, -4.2792e-02,  1.6260e-02, -1.3149e-02, -8.7127e-03,\n",
       "                       1.1713e-02,  2.4079e-02,  5.1107e-02,  8.3932e-03, -1.1471e-02,\n",
       "                       1.8687e-03, -1.7305e-02,  5.5741e-02,  3.6696e-02, -3.2135e-02,\n",
       "                      -2.9930e-02,  1.6515e-02, -4.9896e-02,  7.9643e-03, -4.6642e-02,\n",
       "                       4.2191e-02, -2.6861e-02,  2.6209e-02,  4.4535e-02, -1.5612e-02,\n",
       "                       3.4582e-02,  3.5531e-03, -4.8371e-02,  1.5789e-03, -5.7842e-02,\n",
       "                      -2.7305e-02, -1.4136e-02,  7.1512e-03, -1.7773e-02, -5.3444e-02,\n",
       "                      -3.4526e-03, -1.6032e-02,  9.6460e-03,  1.1432e-03, -3.5316e-02,\n",
       "                       1.1139e-02,  3.4087e-03, -6.3413e-03, -8.2944e-03,  2.6221e-03,\n",
       "                       8.8786e-03, -7.3183e-03,  1.1897e-02,  3.8219e-02,  1.8737e-02,\n",
       "                      -2.9124e-02,  1.4920e-02, -2.0442e-02, -7.8300e-03, -1.7914e-02,\n",
       "                       2.3737e-02,  2.2198e-02, -2.3254e-02,  6.6476e-03, -3.3709e-02,\n",
       "                      -5.6432e-03, -6.1875e-02,  9.3240e-03,  2.6764e-02, -1.2516e-02,\n",
       "                      -4.0221e-02, -1.6139e-02,  1.3539e-02, -1.3355e-02, -1.3283e-02,\n",
       "                       3.4620e-02,  5.4830e-02,  3.8063e-03,  1.5906e-02,  4.5740e-02,\n",
       "                       1.4285e-02, -9.4679e-03,  1.4316e-02,  4.5577e-02,  2.2576e-02,\n",
       "                       4.2762e-02,  2.4339e-02, -2.5859e-02, -1.0237e-02,  1.0762e-02,\n",
       "                       3.7466e-02, -2.7870e-02,  9.0920e-03, -5.7566e-03, -3.6727e-02,\n",
       "                       4.2610e-02,  6.1440e-02, -7.6375e-03,  2.0211e-02, -6.4538e-04,\n",
       "                       1.7136e-02,  3.4582e-02, -9.0931e-03, -1.3779e-02,  3.5023e-02,\n",
       "                       3.1455e-04,  4.1901e-03, -9.0596e-02, -1.0076e-03,  4.1646e-02,\n",
       "                      -9.2237e-03, -5.1975e-02,  3.1674e-02, -2.1826e-02,  7.7601e-03,\n",
       "                      -9.3580e-03, -3.0744e-02, -2.2738e-02,  3.3604e-02, -2.4514e-03,\n",
       "                       2.7007e-02,  6.1418e-03,  2.5188e-02,  2.2191e-02,  2.2235e-02,\n",
       "                      -7.0385e-03,  2.1425e-02, -1.0509e-02,  4.4482e-03,  1.0858e-02,\n",
       "                       5.6865e-02, -4.7963e-02, -5.1386e-02,  2.1611e-02, -2.3526e-02,\n",
       "                      -2.0163e-02,  6.4853e-03, -2.2943e-03,  5.1706e-02, -7.3097e-03,\n",
       "                      -6.0985e-03, -1.9098e-02, -8.8696e-02,  1.6466e-02,  3.1923e-03,\n",
       "                      -2.7515e-02,  3.7069e-02,  4.8010e-03,  2.5347e-02,  1.5439e-02,\n",
       "                      -3.1772e-02,  2.1616e-03,  1.0085e-02,  5.0687e-02,  2.1258e-02,\n",
       "                       2.5124e-02, -1.9964e-02,  9.9144e-03,  5.5535e-03, -2.0320e-02,\n",
       "                       1.3251e-02,  5.5899e-02, -1.3592e-02, -3.9516e-02, -1.1074e-02,\n",
       "                       2.2616e-03,  2.1993e-02, -2.5368e-03, -1.6890e-02,  1.9207e-03,\n",
       "                      -2.2664e-02,  2.4045e-02, -3.3440e-02, -1.0498e-02,  1.9508e-02,\n",
       "                       2.6585e-02,  1.4263e-02, -2.8964e-02,  2.0984e-02, -1.0111e-02,\n",
       "                       5.5508e-03, -3.8566e-02, -1.5344e-02,  4.3639e-02, -3.5272e-03,\n",
       "                      -4.9630e-02,  1.1592e-02, -2.3127e-02, -6.4987e-03,  4.0213e-02,\n",
       "                       3.3773e-02,  4.4476e-02, -2.0842e-02, -3.4034e-02,  2.6281e-04,\n",
       "                      -1.5938e-02, -1.4205e-02, -1.4637e-04, -1.9844e-02,  3.3385e-02,\n",
       "                       2.3561e-02, -2.9061e-03,  2.3288e-02, -4.8073e-02,  1.9588e-02,\n",
       "                      -3.6425e-02,  1.8899e-02, -5.7665e-02,  5.1114e-03,  4.9077e-03,\n",
       "                      -3.0822e-03,  9.8361e-03, -2.4778e-02,  1.0319e-02,  2.8592e-02,\n",
       "                      -1.5273e-02,  5.0739e-02,  1.1067e-03, -2.0574e-02, -6.7398e-02,\n",
       "                      -3.0848e-02,  4.6611e-03, -2.4208e-02], device='cuda:0')),\n",
       "             ('layers.1.anorm.weight',\n",
       "              tensor([0.5837, 0.5330, 0.5189, 0.5374, 0.4823, 0.5340, 0.5106, 0.6019, 0.5309,\n",
       "                      0.4839, 0.5257, 0.5462, 0.5178, 0.5551, 0.6397, 0.5181, 0.5553, 0.5172,\n",
       "                      0.5933, 0.5958, 0.5430, 0.5890, 0.5817, 0.5289, 0.5375, 0.4537, 0.5562,\n",
       "                      0.5736, 0.5657, 0.5315, 0.4989, 0.5389, 0.4851, 0.6342, 0.6199, 0.5119,\n",
       "                      0.5636, 0.5113, 0.6222, 0.5169, 0.5101, 0.6020, 0.5570, 0.5010, 0.5114,\n",
       "                      0.4979, 0.4661, 0.5593, 0.5071, 0.6312, 0.5661, 0.5337, 0.6314, 0.5180,\n",
       "                      0.6927, 0.5397, 0.4897, 0.5045, 0.6084, 0.5688, 0.4809, 0.5602, 0.6618,\n",
       "                      0.5072, 0.5641, 0.6010, 0.5657, 0.5176, 0.5574, 0.5912, 0.4960, 0.5785,\n",
       "                      0.4951, 0.6088, 0.5528, 0.4856, 0.4970, 0.5613, 0.5185, 0.5837, 0.5381,\n",
       "                      0.4748, 0.6238, 0.5659, 0.5473, 0.5577, 0.5311, 0.5616, 0.6043, 0.5722,\n",
       "                      0.5284, 0.4728, 0.5782, 0.6009, 0.6031, 0.5575, 0.5473, 0.5954, 0.5338,\n",
       "                      0.6414, 0.5547, 0.5417, 0.5248, 0.5860, 0.5736, 0.5937, 0.5375, 0.5457,\n",
       "                      0.5047, 0.5437, 0.5143, 0.5442, 0.5698, 0.4761, 0.5744, 0.5721, 0.6422,\n",
       "                      0.5265, 0.4784, 0.5821, 0.5995, 0.5824, 0.6102, 0.6192, 0.5474, 0.5420,\n",
       "                      0.5701, 0.5193, 0.5964, 0.5052, 0.5952, 0.6106, 0.5658, 0.6218, 0.4952,\n",
       "                      0.5382, 0.5555, 0.5437, 0.6089, 0.5936, 0.5481, 0.5805, 0.4940, 0.5148,\n",
       "                      0.4862, 0.5078, 0.5418, 0.6547, 0.5184, 0.4997, 0.5348, 0.5300, 0.6049,\n",
       "                      0.5065, 0.5581, 0.5414, 0.5413, 0.6129, 0.5587, 0.5474, 0.5953, 0.5567,\n",
       "                      0.5348, 0.5908, 0.5344, 0.5653, 0.5122, 0.5285, 0.5448, 0.6537, 0.6284,\n",
       "                      0.5284, 0.5041, 0.5642, 0.5497, 0.5646, 0.5243, 0.5654, 0.5577, 0.5652,\n",
       "                      0.4441, 0.5600, 0.5860, 0.5844, 0.5905, 0.6491, 0.5648, 0.4401, 0.5058,\n",
       "                      0.5801, 0.5348, 0.5784, 0.5970, 0.5602, 0.5792, 0.5251, 0.6138, 0.5868,\n",
       "                      0.4909, 0.5015, 0.6033, 0.5311, 0.5498, 0.5169, 0.5444, 0.5648, 0.6129,\n",
       "                      0.5759, 0.5329, 0.6891, 0.5391, 0.5357, 0.6109, 0.5598, 0.5649, 0.5644,\n",
       "                      0.6516, 0.5283, 0.5558, 0.5275, 0.5824, 0.6130, 0.4671, 0.5268, 0.5067,\n",
       "                      0.5357, 0.5858, 0.6254, 0.5135, 0.6206, 0.5846, 0.5571, 0.6100, 0.4810,\n",
       "                      0.5075, 0.6062, 0.5656, 0.5364, 0.6132, 0.5272, 0.4981, 0.4989, 0.5512,\n",
       "                      0.5304, 0.5907, 0.5372, 0.5600, 0.5885, 0.6072, 0.5398, 0.5743, 0.6046,\n",
       "                      0.5767, 0.6319, 0.4539, 0.5796, 0.6036, 0.6125, 0.5604, 0.5839, 0.5049,\n",
       "                      0.4979, 0.5070, 0.6067, 0.5168, 0.5733, 0.5421, 0.6065, 0.5483, 0.5023,\n",
       "                      0.5314, 0.6237, 0.5588, 0.5926, 0.6027, 0.5961, 0.6266, 0.5587, 0.5534,\n",
       "                      0.5000, 0.5840, 0.6445, 0.5328, 0.5958, 0.5629, 0.5890, 0.5461, 0.5442,\n",
       "                      0.4941, 0.6010, 0.5399, 0.6208, 0.5508, 0.5077, 0.5184, 0.5911, 0.4250,\n",
       "                      0.5947, 0.5607, 0.5215, 0.4530, 0.6018, 0.5433, 0.5546, 0.5715, 0.5269,\n",
       "                      0.6027, 0.5458, 0.5565, 0.5604, 0.5829, 0.5602, 0.5207, 0.4676, 0.5840,\n",
       "                      0.5396, 0.5704, 0.5948, 0.6103, 0.6478, 0.5649, 0.5150, 0.5781, 0.5806,\n",
       "                      0.5079, 0.5211, 0.5857, 0.5306, 0.5502, 0.5362, 0.6022, 0.4647, 0.5661,\n",
       "                      0.5350, 0.5641, 0.6276, 0.5442, 0.4482, 0.6129, 0.5222, 0.5353, 0.5600,\n",
       "                      0.5243, 0.5934, 0.4758, 0.5782, 0.5224, 0.5860, 0.6306, 0.5014, 0.6581,\n",
       "                      0.5706, 0.6109, 0.6373, 0.6366, 0.5324, 0.5481, 0.6083, 0.5473, 0.5993,\n",
       "                      0.5513, 0.5676, 0.5349, 0.5398, 0.5540, 0.5592, 0.5533, 0.6269, 0.4856,\n",
       "                      0.5208, 0.5360, 0.5384, 0.6349, 0.4899, 0.5715, 0.5259, 0.6410, 0.5207,\n",
       "                      0.6163, 0.5882, 0.6428, 0.5720, 0.5295, 0.5907, 0.6314, 0.5269, 0.5927,\n",
       "                      0.6733, 0.5219, 0.6005, 0.5044, 0.5604, 0.6348, 0.4785, 0.4862, 0.5673,\n",
       "                      0.4977, 0.5733, 0.5543, 0.5683, 0.5100, 0.4783, 0.5940, 0.6820, 0.5771,\n",
       "                      0.5763, 0.5297, 0.6279, 0.3659, 0.6347, 0.5558, 0.5733, 0.5797, 0.5823,\n",
       "                      0.6147, 0.6071, 0.5406, 0.5823, 0.5665, 0.5944, 0.6185, 0.5743, 0.5335,\n",
       "                      0.5918, 0.5824, 0.5247, 0.5648, 0.5963, 0.4795, 0.5700, 0.5247, 0.6129,\n",
       "                      0.4985, 0.5136, 0.6194, 0.5217, 0.5198, 0.5037, 0.5239, 0.5256, 0.5191,\n",
       "                      0.6073, 0.4782, 0.5834, 0.5447, 0.5445, 0.5321, 0.4907, 0.6301, 0.4360,\n",
       "                      0.5189, 0.5676, 0.5611, 0.5670, 0.5736, 0.6434, 0.5826, 0.5724, 0.5630,\n",
       "                      0.5226, 0.5353, 0.5549, 0.5849, 0.5881, 0.5935, 0.5447, 0.5854, 0.5630,\n",
       "                      0.5163, 0.5122, 0.5354, 0.4963, 0.5924, 0.5020, 0.4878, 0.6059, 0.5294,\n",
       "                      0.5004, 0.4935, 0.5271, 0.6364, 0.5566, 0.5555, 0.6019, 0.5422, 0.6417,\n",
       "                      0.5045, 0.5747, 0.6055, 0.4540, 0.5833, 0.5396, 0.5085, 0.5767, 0.5941,\n",
       "                      0.5721, 0.5642, 0.5901, 0.4477, 0.5108, 0.6882, 0.4910, 0.5494, 0.5404,\n",
       "                      0.6003, 0.5335, 0.4938, 0.6285, 0.5651, 0.6189, 0.5500, 0.6005, 0.5737,\n",
       "                      0.5441, 0.4555, 0.5591, 0.6329, 0.5712, 0.5442, 0.5232, 0.6065, 0.5573,\n",
       "                      0.6012, 0.5835, 0.5829, 0.5287, 0.5804, 0.5099, 0.6302, 0.5152, 0.6247,\n",
       "                      0.5366, 0.4638, 0.5922, 0.6315, 0.5252, 0.6122, 0.5834, 0.5299, 0.6274,\n",
       "                      0.5249, 0.4772, 0.5139, 0.5383, 0.5699, 0.5538, 0.5829, 0.5504, 0.5896,\n",
       "                      0.5007, 0.5495, 0.5238, 0.5412, 0.5612, 0.5923, 0.5782, 0.5779, 0.6363,\n",
       "                      0.5527, 0.5195, 0.5622, 0.5965, 0.5734, 0.5749, 0.6354, 0.5873, 0.6264,\n",
       "                      0.6068, 0.5568, 0.4921, 0.5224, 0.5520, 0.5970, 0.4683, 0.6374, 0.5732,\n",
       "                      0.4615, 0.4693, 0.5375, 0.5404, 0.5667, 0.5361, 0.6466, 0.5962, 0.5884,\n",
       "                      0.5803, 0.5574, 0.5629, 0.5236, 0.7015, 0.5640, 0.5091, 0.6139, 0.6026,\n",
       "                      0.5577, 0.5641, 0.5734, 0.4259, 0.6000, 0.6600, 0.5244, 0.5175, 0.5457,\n",
       "                      0.5643, 0.5682, 0.6108, 0.5348, 0.6139, 0.5573, 0.5593, 0.5643, 0.6035,\n",
       "                      0.6487, 0.5690, 0.5719, 0.5809, 0.5289, 0.5101, 0.6076, 0.6087, 0.5752,\n",
       "                      0.5594, 0.5268, 0.5575, 0.4513, 0.4752, 0.5989, 0.6227, 0.5727, 0.5547,\n",
       "                      0.5828, 0.5372, 0.5173, 0.5366, 0.5811, 0.5569, 0.5394, 0.6275, 0.6251,\n",
       "                      0.5981, 0.6086, 0.5990, 0.5506, 0.5092, 0.5740, 0.5914, 0.6185, 0.5462,\n",
       "                      0.5521, 0.5232, 0.5484, 0.5848, 0.6168, 0.4421, 0.5897, 0.5736, 0.5842,\n",
       "                      0.5864, 0.6532, 0.4613, 0.4825, 0.4953, 0.5083, 0.5937, 0.4812, 0.5733,\n",
       "                      0.5667, 0.6600, 0.5056, 0.5165, 0.5505, 0.6447, 0.5374, 0.5080, 0.5151,\n",
       "                      0.5870, 0.5586, 0.6021, 0.4638, 0.4949, 0.5828, 0.4973, 0.5939, 0.5929,\n",
       "                      0.5824, 0.6447, 0.5193, 0.5616, 0.4655, 0.6003, 0.5899, 0.6168, 0.5219,\n",
       "                      0.5157, 0.5235, 0.5983, 0.5615, 0.4886, 0.5619, 0.5200, 0.4741, 0.5002,\n",
       "                      0.5324, 0.5335, 0.5358, 0.6261, 0.5558, 0.5548, 0.5439, 0.5647, 0.6104,\n",
       "                      0.4619, 0.5559, 0.5668, 0.5161, 0.6347, 0.4428, 0.4814, 0.5806, 0.5138,\n",
       "                      0.5488, 0.5942, 0.5658, 0.6567, 0.5797, 0.5814, 0.5279, 0.5880, 0.5879,\n",
       "                      0.5080, 0.5385, 0.6317, 0.5142, 0.5889, 0.6720, 0.5588, 0.5774, 0.5135,\n",
       "                      0.5017, 0.6576, 0.5489, 0.4625, 0.5215, 0.6329, 0.5529, 0.6548, 0.5592,\n",
       "                      0.5908, 0.5166, 0.6183, 0.5754, 0.5443, 0.5142, 0.6054, 0.6255, 0.5373,\n",
       "                      0.6003, 0.5175, 0.5082, 0.5518, 0.5541, 0.5468, 0.5794, 0.5645, 0.5483,\n",
       "                      0.5701, 0.5559, 0.5563], device='cuda:0')),\n",
       "             ('layers.1.fnorm.weight',\n",
       "              tensor([0.4998, 0.4076, 0.4577, 0.4589, 0.5116, 0.4684, 0.4628, 0.6021, 0.4631,\n",
       "                      0.3980, 0.4631, 0.4520, 0.4664, 0.4803, 0.5001, 0.5104, 0.4797, 0.4908,\n",
       "                      0.4347, 0.5068, 0.5945, 0.5003, 0.5663, 0.4359, 0.4907, 0.3753, 0.5185,\n",
       "                      0.4620, 0.5643, 0.4444, 0.4912, 0.4545, 0.4404, 0.5009, 0.4953, 0.4447,\n",
       "                      0.4615, 0.5277, 0.4465, 0.5182, 0.4234, 0.5900, 0.4543, 0.4949, 0.4093,\n",
       "                      0.3947, 0.5332, 0.4498, 0.3735, 0.4661, 0.5091, 0.4205, 0.5216, 0.4518,\n",
       "                      0.4734, 0.4903, 0.4622, 0.4908, 0.5588, 0.4709, 0.4118, 0.5693, 0.4987,\n",
       "                      0.3916, 0.4332, 0.4467, 0.5015, 0.4980, 0.5658, 0.4167, 0.4412, 0.4745,\n",
       "                      0.4610, 0.4644, 0.4680, 0.4515, 0.4105, 0.4447, 0.4550, 0.4874, 0.4838,\n",
       "                      0.4741, 0.4896, 0.4187, 0.5018, 0.4626, 0.4762, 0.4855, 0.4121, 0.4864,\n",
       "                      0.5064, 0.4144, 0.4226, 0.4812, 0.4242, 0.4111, 0.5034, 0.4046, 0.4790,\n",
       "                      0.5154, 0.4853, 0.5019, 0.4766, 0.3062, 0.4664, 0.4779, 0.4472, 0.4540,\n",
       "                      0.4725, 0.4459, 0.4348, 0.4612, 0.5013, 0.4819, 0.5231, 0.4868, 0.4714,\n",
       "                      0.4364, 0.4643, 0.4787, 0.4535, 0.5104, 0.5660, 0.4561, 0.4789, 0.4290,\n",
       "                      0.4849, 0.4655, 0.5021, 0.4084, 0.4701, 0.4918, 0.4917, 0.4597, 0.4202,\n",
       "                      0.5187, 0.4598, 0.4719, 0.5035, 0.4696, 0.5483, 0.4566, 0.4657, 0.5006,\n",
       "                      0.4670, 0.4114, 0.4384, 0.4447, 0.5069, 0.5051, 0.5010, 0.3836, 0.4647,\n",
       "                      0.4574, 0.4992, 0.4269, 0.4642, 0.4577, 0.4631, 0.5647, 0.5034, 0.4471,\n",
       "                      0.5185, 0.4335, 0.4795, 0.4726, 0.3940, 0.4353, 0.5023, 0.4803, 0.4554,\n",
       "                      0.4679, 0.4406, 0.4220, 0.5172, 0.5409, 0.4298, 0.4546, 0.4399, 0.5290,\n",
       "                      0.4608, 0.4862, 0.5056, 0.4666, 0.4852, 0.5274, 0.5019, 0.4481, 0.4221,\n",
       "                      0.4988, 0.4712, 0.5407, 0.4947, 0.5103, 0.5030, 0.4737, 0.4909, 0.4700,\n",
       "                      0.4472, 0.4432, 0.4805, 0.4261, 0.4970, 0.4564, 0.4491, 0.4093, 0.5175,\n",
       "                      0.5171, 0.5262, 0.4907, 0.4829, 0.5092, 0.4996, 0.5075, 0.4748, 0.4989,\n",
       "                      0.4969, 0.4691, 0.4332, 0.4244, 0.5470, 0.4553, 0.4664, 0.4633, 0.4621,\n",
       "                      0.5099, 0.0054, 0.5450, 0.5297, 0.5210, 0.5106, 0.4085, 0.4953, 0.4671,\n",
       "                      0.5166, 0.4500, 0.3988, 0.4706, 0.4751, 0.5086, 0.4686, 0.4385, 0.5015,\n",
       "                      0.4574, 0.5731, 0.4471, 0.4231, 0.4181, 0.4986, 0.4391, 0.5526, 0.4456,\n",
       "                      0.5320, 0.4937, 0.4027, 0.4599, 0.4450, 0.5210, 0.4249, 0.4892, 0.5155,\n",
       "                      0.4570, 0.4065, 0.4457, 0.4130, 0.4856, 0.4876, 0.4893, 0.4479, 0.4443,\n",
       "                      0.4397, 0.5119, 0.4724, 0.4826, 0.5354, 0.4553, 0.4498, 0.4890, 0.5091,\n",
       "                      0.4432, 0.4683, 0.5015, 0.5155, 0.4734, 0.5081, 0.4398, 0.4943, 0.4471,\n",
       "                      0.5105, 0.4803, 0.4789, 0.5257, 0.5221, 0.4288, 0.4769, 0.4971, 0.3915,\n",
       "                      0.5020, 0.5225, 0.4596, 0.4131, 0.4581, 0.4211, 0.4143, 0.4687, 0.5425,\n",
       "                      0.4405, 0.4785, 0.4875, 0.4955, 0.4834, 0.5658, 0.4878, 0.4291, 0.4994,\n",
       "                      0.4887, 0.4592, 0.4245, 0.4784, 0.5234, 0.5226, 0.5427, 0.5124, 0.5241,\n",
       "                      0.4639, 0.4854, 0.4489, 0.4895, 0.4428, 0.4549, 0.5444, 0.4863, 0.4301,\n",
       "                      0.4635, 0.4776, 0.4916, 0.4932, 0.4167, 0.4634, 0.4487, 0.5046, 0.3720,\n",
       "                      0.4330, 0.4699, 0.4343, 0.4697, 0.4323, 0.4489, 0.5100, 0.4909, 0.4847,\n",
       "                      0.4349, 0.5544, 0.5340, 0.4575, 0.4437, 0.4426, 0.4361, 0.4503, 0.5137,\n",
       "                      0.4390, 0.3597, 0.4611, 0.4416, 0.4977, 0.4651, 0.5324, 0.5298, 0.4734,\n",
       "                      0.4421, 0.4327, 0.4862, 0.4356, 0.4560, 0.4298, 0.4832, 0.5267, 0.4777,\n",
       "                      0.4723, 0.4798, 0.5042, 0.4641, 0.4614, 0.5380, 0.4883, 0.5571, 0.4746,\n",
       "                      0.4779, 0.5263, 0.5128, 0.4178, 0.3827, 0.4607, 0.3957, 0.4680, 0.4608,\n",
       "                      0.4683, 0.4966, 0.4694, 0.4475, 0.4421, 0.4190, 0.4403, 0.4372, 0.4983,\n",
       "                      0.5789, 0.4447, 0.5138, 0.0038, 0.4844, 0.4519, 0.4403, 0.4597, 0.5277,\n",
       "                      0.4422, 0.4922, 0.4825, 0.4915, 0.5055, 0.5575, 0.4672, 0.4820, 0.4722,\n",
       "                      0.5172, 0.4452, 0.4436, 0.5042, 0.5421, 0.5053, 0.4462, 0.4652, 0.5222,\n",
       "                      0.4141, 0.4673, 0.4987, 0.4893, 0.4396, 0.4395, 0.4583, 0.4339, 0.4657,\n",
       "                      0.4568, 0.4788, 0.4427, 0.5031, 0.4538, 0.4648, 0.4772, 0.5438, 0.4097,\n",
       "                      0.5345, 0.4377, 0.6557, 0.5381, 0.4159, 0.4230, 0.5266, 0.5009, 0.5139,\n",
       "                      0.5290, 0.4039, 0.4498, 0.5205, 0.5639, 0.5443, 0.4527, 0.4956, 0.4942,\n",
       "                      0.4871, 0.4098, 0.4724, 0.4249, 0.4139, 0.4908, 0.4822, 0.5457, 0.4364,\n",
       "                      0.4833, 0.4241, 0.4465, 0.5038, 0.5618, 0.4830, 0.4119, 0.4421, 0.5065,\n",
       "                      0.4051, 0.4628, 0.4821, 0.4403, 0.5041, 0.4835, 0.4783, 0.4446, 0.4612,\n",
       "                      0.4553, 0.4833, 0.4880, 0.0502, 0.4950, 0.5300, 0.4663, 0.4430, 0.4194,\n",
       "                      0.4902, 0.5282, 0.4116, 0.5283, 0.4894, 0.5524, 0.3711, 0.5231, 0.5767,\n",
       "                      0.5141, 0.4607, 0.4817, 0.5108, 0.4586, 0.4783, 0.4799, 0.4676, 0.4804,\n",
       "                      0.4872, 0.6020, 0.4631, 0.4681, 0.5864, 0.4923, 0.5286, 0.4919, 0.5033,\n",
       "                      0.4569, 0.4997, 0.4940, 0.4953, 0.4901, 0.4962, 0.5168, 0.4406, 0.4809,\n",
       "                      0.4103, 0.5505, 0.5054, 0.5226, 0.5047, 0.5122, 0.4873, 0.4852, 0.4935,\n",
       "                      0.4899, 0.4688, 0.3762, 0.4482, 0.4804, 0.5350, 0.4813, 0.4901, 0.5864,\n",
       "                      0.4051, 0.5685, 0.5320, 0.4756, 0.3962, 0.4282, 0.5937, 0.4026, 0.4897,\n",
       "                      0.5199, 0.4833, 0.4780, 0.4272, 0.5007, 0.5605, 0.4540, 0.4597, 0.4705,\n",
       "                      0.4347, 0.3945, 0.4687, 0.5298, 0.4615, 0.3777, 0.4661, 0.4710, 0.5218,\n",
       "                      0.4798, 0.4484, 0.5278, 0.4512, 0.5180, 0.4448, 0.4375, 0.4612, 0.4738,\n",
       "                      0.4677, 0.4550, 0.4084, 0.4445, 0.5263, 0.5175, 0.4806, 0.4755, 0.5410,\n",
       "                      0.4566, 0.4844, 0.5021, 0.4922, 0.5147, 0.4652, 0.5319, 0.5115, 0.4705,\n",
       "                      0.4870, 0.4961, 0.5275, 0.4616, 0.4547, 0.5385, 0.4919, 0.4832, 0.4111,\n",
       "                      0.5898, 0.4820, 0.5313, 0.4317, 0.4242, 0.5512, 0.5401, 0.4805, 0.4240,\n",
       "                      0.5152, 0.4531, 0.4781, 0.5236, 0.5084, 0.4375, 0.4307, 0.4845, 0.4699,\n",
       "                      0.4468, 0.5211, 0.5083, 0.5056, 0.4178, 0.4468, 0.4640, 0.5433, 0.4414,\n",
       "                      0.5107, 0.3843, 0.5025, 0.4857, 0.4599, 0.4744, 0.5555, 0.4671, 0.5056,\n",
       "                      0.4498, 0.4717, 0.4909, 0.5062, 0.3884, 0.4689, 0.5159, 0.4904, 0.5549,\n",
       "                      0.5347, 0.5252, 0.4657, 0.4489, 0.4953, 0.5665, 0.4840, 0.4298, 0.4938,\n",
       "                      0.4910, 0.4824, 0.4791, 0.4136, 0.5062, 0.4500, 0.4529, 0.5565, 0.4360,\n",
       "                      0.4621, 0.4390, 0.4720, 0.5833, 0.4468, 0.4389, 0.4879, 0.4530, 0.4568,\n",
       "                      0.4241, 0.4354, 0.4839, 0.4823, 0.4921, 0.4653, 0.4575, 0.4815, 0.4091,\n",
       "                      0.4405, 0.4800, 0.4557, 0.4919, 0.4369, 0.4981, 0.4577, 0.4475, 0.4551,\n",
       "                      0.3843, 0.4519, 0.4881, 0.4352, 0.4564, 0.4473, 0.4421, 0.4026, 0.4366,\n",
       "                      0.5392, 0.4264, 0.4973, 0.4526, 0.3946, 0.4588, 0.4261, 0.5853, 0.5876,\n",
       "                      0.4392, 0.4953, 0.5030, 0.5182, 0.5759, 0.4666, 0.5266, 0.5371, 0.4516,\n",
       "                      0.4650, 0.5127, 0.5291, 0.4692, 0.5042, 0.5327, 0.5372, 0.4632, 0.5216,\n",
       "                      0.4725, 0.4180, 0.4363, 0.4650, 0.4555, 0.4366, 0.4913, 0.4906, 0.4557,\n",
       "                      0.5443, 0.4025, 0.5330, 0.4660, 0.4636, 0.5217, 0.5008, 0.4753, 0.4288,\n",
       "                      0.4701, 0.4958, 0.5058], device='cuda:0')),\n",
       "             ('layers.2.mha.query.weight',\n",
       "              tensor([[ 0.0317, -0.0073, -0.0970,  ...,  0.0597, -0.0972, -0.0649],\n",
       "                      [ 0.0119,  0.0711, -0.0366,  ..., -0.0009, -0.0207,  0.0769],\n",
       "                      [-0.0428, -0.0204,  0.0272,  ..., -0.0270, -0.0084, -0.0245],\n",
       "                      ...,\n",
       "                      [-0.0866, -0.0944,  0.0378,  ...,  0.0665,  0.0327,  0.0070],\n",
       "                      [-0.0468,  0.0085,  0.0653,  ..., -0.0148,  0.0007, -0.0266],\n",
       "                      [ 0.0179,  0.0357, -0.0679,  ..., -0.0607,  0.0051, -0.0513]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.mha.key.weight',\n",
       "              tensor([[ 0.0373,  0.0137, -0.0384,  ..., -0.0066, -0.0246, -0.0513],\n",
       "                      [-0.0229,  0.0902, -0.0469,  ..., -0.0135, -0.0178,  0.0189],\n",
       "                      [ 0.0269, -0.0017, -0.0637,  ..., -0.0075,  0.0236, -0.0290],\n",
       "                      ...,\n",
       "                      [-0.0403,  0.1003, -0.0125,  ...,  0.0355,  0.0179,  0.0266],\n",
       "                      [ 0.0773, -0.0081, -0.1183,  ...,  0.0018,  0.0397, -0.0170],\n",
       "                      [-0.0193, -0.0310, -0.0731,  ...,  0.0126, -0.0654, -0.0304]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.mha.value.weight',\n",
       "              tensor([[-0.0183,  0.0080,  0.0091,  ..., -0.0283, -0.0584,  0.0310],\n",
       "                      [-0.0332,  0.0457,  0.0376,  ..., -0.0054,  0.0214,  0.0240],\n",
       "                      [-0.0235,  0.0377, -0.0546,  ..., -0.0141,  0.0324, -0.0478],\n",
       "                      ...,\n",
       "                      [ 0.0679, -0.0506, -0.0229,  ..., -0.0094,  0.0195, -0.0043],\n",
       "                      [-0.0061, -0.0105,  0.0001,  ..., -0.0025, -0.0367, -0.0107],\n",
       "                      [-0.0308,  0.0014,  0.0110,  ..., -0.0271,  0.0560, -0.0693]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.mha.proj.weight',\n",
       "              tensor([[-0.0347, -0.0053, -0.0210,  ..., -0.0382,  0.0514,  0.0106],\n",
       "                      [ 0.0333, -0.0149,  0.0527,  ...,  0.0445, -0.0083,  0.0272],\n",
       "                      [ 0.0397, -0.0286,  0.0410,  ..., -0.0386,  0.0279, -0.0387],\n",
       "                      ...,\n",
       "                      [-0.0220,  0.0357,  0.0174,  ...,  0.0408, -0.0572, -0.0278],\n",
       "                      [-0.0557, -0.0375, -0.0121,  ...,  0.0152, -0.0065, -0.0357],\n",
       "                      [ 0.0360, -0.0023,  0.0193,  ..., -0.0108,  0.0009, -0.0372]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.ffn.w.weight',\n",
       "              tensor([[ 0.0174, -0.0220, -0.0001,  ..., -0.0167, -0.0263,  0.0832],\n",
       "                      [-0.0150, -0.0215,  0.0193,  ...,  0.0116, -0.0458, -0.0039],\n",
       "                      [-0.0212,  0.0532, -0.0049,  ..., -0.0202,  0.0420, -0.0029],\n",
       "                      ...,\n",
       "                      [-0.0645,  0.0530,  0.0293,  ...,  0.0480,  0.0011,  0.0531],\n",
       "                      [-0.0421,  0.0696,  0.0726,  ...,  0.0280,  0.0139, -0.0189],\n",
       "                      [ 0.0809,  0.0120,  0.0056,  ..., -0.0553, -0.0182,  0.0122]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.ffn.w.bias',\n",
       "              tensor([-0.0405, -0.0506, -0.1126,  ..., -0.0734, -0.0651, -0.0787],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.ffn.v.weight',\n",
       "              tensor([[ 6.3476e-03,  1.5335e-02,  6.0520e-03,  ..., -4.8653e-02,\n",
       "                        3.5757e-02,  2.3251e-02],\n",
       "                      [-2.2484e-02, -1.8825e-04, -3.7464e-02,  ..., -8.6175e-02,\n",
       "                        1.3649e-01,  6.5650e-02],\n",
       "                      [-3.5144e-02,  8.1078e-02, -6.7678e-02,  ..., -9.8463e-03,\n",
       "                        1.8119e-02, -4.0766e-03],\n",
       "                      ...,\n",
       "                      [-3.2870e-02, -2.7695e-02,  2.7748e-02,  ..., -8.7500e-02,\n",
       "                        4.8850e-03, -1.5040e-02],\n",
       "                      [ 2.1431e-02,  6.6990e-03,  1.9483e-02,  ...,  8.8298e-05,\n",
       "                        5.1450e-03,  9.5737e-02],\n",
       "                      [ 7.1799e-02,  2.6834e-03,  5.4992e-02,  ...,  3.6423e-02,\n",
       "                        6.3025e-03,  2.5755e-02]], device='cuda:0')),\n",
       "             ('layers.2.ffn.v.bias',\n",
       "              tensor([ 0.0612, -0.0872,  0.0367,  ...,  0.0377,  0.0120,  0.0161],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.ffn.w2.weight',\n",
       "              tensor([[ 0.0669, -0.0072, -0.0283,  ...,  0.0040, -0.0594,  0.0104],\n",
       "                      [-0.0064,  0.0381,  0.0072,  ...,  0.0049,  0.0189,  0.0488],\n",
       "                      [ 0.0507,  0.0691, -0.0209,  ...,  0.0398, -0.0748, -0.0607],\n",
       "                      ...,\n",
       "                      [-0.0341, -0.0306, -0.0083,  ...,  0.0403,  0.0177,  0.0384],\n",
       "                      [-0.0446, -0.0033,  0.0026,  ..., -0.0201, -0.0174, -0.0469],\n",
       "                      [ 0.0030,  0.0501, -0.0032,  ..., -0.0599, -0.0547,  0.0479]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.2.ffn.w2.bias',\n",
       "              tensor([-4.2385e-03,  5.3652e-02, -5.5869e-03, -1.0034e-02,  8.9281e-02,\n",
       "                       3.0681e-02,  3.9234e-02, -4.0386e-03, -4.4488e-04,  3.5523e-02,\n",
       "                       7.2323e-03, -1.8913e-02, -1.2680e-02,  3.6309e-02, -4.2123e-02,\n",
       "                      -5.3976e-02, -3.0410e-02,  1.2118e-02, -4.4202e-03, -1.6817e-02,\n",
       "                      -6.6950e-03,  3.1869e-02,  4.7036e-02,  7.9703e-03,  4.6492e-02,\n",
       "                      -3.6803e-03, -6.3672e-02,  7.5914e-03,  1.6295e-02, -2.1998e-02,\n",
       "                      -6.5206e-03, -1.8818e-02,  2.2485e-02, -4.3623e-02,  5.2217e-02,\n",
       "                       1.5800e-02,  1.5440e-02,  1.5708e-02,  6.1392e-03, -4.7220e-02,\n",
       "                      -3.8650e-02,  6.4953e-02,  4.7316e-02,  1.9117e-02, -3.4849e-02,\n",
       "                      -3.6892e-02,  4.3205e-02, -3.2512e-02, -3.6759e-03, -7.5118e-02,\n",
       "                       4.1791e-02,  2.5029e-03, -5.9448e-02,  2.9534e-02,  5.6010e-03,\n",
       "                      -1.0843e-02,  4.4474e-02,  6.5402e-02, -6.0627e-04,  1.3399e-02,\n",
       "                       1.9088e-02, -4.8292e-02, -8.8327e-02,  4.7481e-02,  5.5653e-02,\n",
       "                       1.0385e-02,  9.8736e-03,  1.7586e-02, -8.7095e-02,  2.4948e-02,\n",
       "                      -3.4663e-02, -2.1934e-02, -1.1586e-02, -7.0395e-03, -2.7017e-02,\n",
       "                      -2.4779e-02, -5.8038e-03, -4.8535e-02,  2.8936e-02, -1.8616e-02,\n",
       "                       5.0814e-02,  6.3095e-02, -9.4697e-02,  4.4452e-02, -2.3881e-02,\n",
       "                      -8.4375e-02,  3.1465e-03, -1.5033e-02,  1.1077e-04, -9.7547e-03,\n",
       "                      -2.4282e-02, -5.0839e-03,  3.2702e-02,  1.2347e-02, -1.8715e-02,\n",
       "                      -1.7476e-02, -1.7503e-02, -2.0271e-02,  4.3107e-03,  1.4893e-02,\n",
       "                      -1.0473e-03,  5.9400e-02, -4.4498e-02,  3.3064e-02, -1.0424e-03,\n",
       "                      -1.1869e-02, -2.8684e-03, -1.4841e-03, -9.5562e-03, -2.2985e-03,\n",
       "                       2.7338e-02,  1.7299e-02, -1.6109e-02,  5.2882e-02, -6.6466e-02,\n",
       "                      -2.5057e-02,  4.8288e-02, -9.4658e-03,  2.8894e-02, -3.1778e-02,\n",
       "                      -2.9130e-02, -2.4794e-02,  4.5131e-03, -1.8844e-02, -7.3458e-02,\n",
       "                      -8.5505e-02, -1.5293e-02,  1.7822e-02, -6.3743e-02,  2.3968e-02,\n",
       "                      -1.3793e-02, -8.1606e-02,  3.4489e-02, -2.5096e-02, -7.5935e-03,\n",
       "                      -1.1837e-02,  8.9200e-03,  5.2958e-02,  2.7858e-02, -5.2110e-02,\n",
       "                       2.2929e-03,  5.3342e-03, -2.4437e-02,  1.3494e-02,  3.4786e-02,\n",
       "                      -1.8357e-02, -6.9919e-05, -4.5962e-03, -3.7328e-02, -1.1419e-01,\n",
       "                       4.4513e-02, -2.6310e-02,  2.6224e-02, -7.3995e-03, -2.6766e-02,\n",
       "                      -2.4462e-02,  1.5493e-02, -9.5132e-03,  7.0235e-03,  5.5139e-02,\n",
       "                      -4.8124e-02, -5.4230e-03, -2.8669e-02, -1.7534e-02, -1.3489e-02,\n",
       "                       3.2924e-02, -1.7840e-02,  5.4305e-03,  5.9234e-02, -3.9737e-02,\n",
       "                      -5.5364e-02,  2.2333e-02, -4.9981e-02,  1.0963e-01,  3.8019e-02,\n",
       "                       2.0796e-02,  1.8572e-03, -1.0434e-02, -2.3790e-02, -2.0373e-03,\n",
       "                      -5.8760e-02, -4.2244e-02, -6.2638e-02, -4.3877e-02,  4.2341e-03,\n",
       "                      -2.7513e-02,  6.5648e-02, -2.9689e-02,  2.7705e-02, -4.3126e-02,\n",
       "                       5.9950e-03, -4.9185e-02, -5.9919e-03,  1.1208e-03, -3.6590e-02,\n",
       "                      -1.8374e-02, -5.4765e-02, -1.8800e-02,  4.0653e-03,  1.0187e-02,\n",
       "                      -1.0961e-02, -2.1796e-02, -4.3292e-02,  6.6626e-02, -5.3983e-03,\n",
       "                       8.8870e-03, -3.1541e-02, -4.7020e-02, -5.5302e-02, -7.2857e-02,\n",
       "                      -1.7169e-02, -4.9342e-02, -1.3695e-03, -2.1954e-02, -1.7004e-02,\n",
       "                       3.6859e-02,  8.3598e-02,  9.1402e-03, -4.4628e-03, -4.3261e-02,\n",
       "                      -1.1842e-02,  9.8222e-03, -2.3609e-02, -8.5289e-03,  1.0876e-02,\n",
       "                      -1.7674e-03, -6.4221e-01,  6.4836e-02,  1.8475e-02, -3.1784e-02,\n",
       "                       1.9859e-02, -5.7339e-02, -2.9605e-02,  2.5703e-02, -2.4379e-02,\n",
       "                       5.3909e-02, -6.1933e-03, -2.1483e-02, -1.3682e-02,  6.8221e-03,\n",
       "                       3.2058e-02, -2.9404e-02,  2.0512e-02, -6.7641e-03,  1.9802e-02,\n",
       "                      -3.8237e-02,  9.4460e-03,  5.3818e-02, -2.7006e-02, -2.4586e-02,\n",
       "                       3.3766e-02, -2.2221e-02,  1.3510e-02, -2.3852e-02, -4.5382e-02,\n",
       "                       8.0413e-02, -4.6473e-02,  2.0370e-02, -4.3205e-02, -1.7771e-02,\n",
       "                       2.1356e-02, -1.6892e-02,  9.9364e-02, -1.6587e-02, -2.4404e-02,\n",
       "                       4.0850e-02, -8.5735e-03,  6.4054e-02,  1.5970e-02, -1.4680e-02,\n",
       "                      -6.1363e-03, -5.7189e-02, -7.4554e-03,  1.6945e-02, -2.9814e-02,\n",
       "                       1.7991e-02,  1.4298e-02, -2.0714e-02, -7.5474e-02,  4.3461e-03,\n",
       "                       6.8541e-03,  1.5022e-02,  6.4361e-02,  6.1167e-03, -3.9634e-02,\n",
       "                       6.9494e-03,  3.2437e-03, -2.3267e-02,  4.0137e-04,  1.1111e-02,\n",
       "                       2.3896e-02,  2.2253e-02, -1.2358e-02, -2.0133e-02, -4.2875e-02,\n",
       "                      -3.1843e-02, -8.4740e-04,  4.1034e-02, -5.5330e-02,  2.8723e-02,\n",
       "                       2.0382e-02,  1.3064e-01,  5.1277e-02, -1.8655e-02, -3.1057e-02,\n",
       "                      -4.8083e-03, -8.0548e-02,  3.5873e-03,  8.9222e-03,  2.6098e-02,\n",
       "                       7.6383e-02,  2.7531e-02,  1.9530e-02, -1.2353e-03, -5.9353e-02,\n",
       "                      -1.7494e-03,  2.7891e-02, -6.0516e-02,  4.9083e-02, -7.3539e-03,\n",
       "                       1.2384e-01,  1.6840e-02,  3.3343e-02, -3.5397e-02,  3.9371e-03,\n",
       "                       3.8158e-02, -1.7793e-02, -8.0307e-03, -3.2032e-02, -6.0695e-03,\n",
       "                       2.1811e-02,  4.5301e-02,  1.4412e-02,  1.3264e-02, -2.3454e-03,\n",
       "                       8.6593e-04, -5.9497e-02,  2.3617e-02,  2.1645e-03,  4.0899e-02,\n",
       "                       3.1745e-02, -1.2153e-03, -3.3964e-03,  2.9742e-02, -6.1718e-02,\n",
       "                       5.7513e-03, -6.8393e-02, -7.6338e-02, -7.6560e-03,  5.3971e-03,\n",
       "                      -3.3011e-02,  1.8058e-03,  4.0940e-02, -8.9911e-03,  3.4267e-02,\n",
       "                       1.8311e-02, -1.7212e-02, -1.6497e-02, -3.7200e-02, -4.0713e-02,\n",
       "                       3.8429e-02,  1.9531e-02,  2.2919e-02,  3.9109e-02,  2.9327e-02,\n",
       "                      -7.2528e-03,  1.6586e-02, -6.9805e-03, -4.0221e-02, -3.2755e-02,\n",
       "                      -4.2955e-02,  5.7396e-03, -3.0770e-03, -4.0429e-02,  4.1968e-03,\n",
       "                       3.5819e-02, -1.9192e-02,  5.7404e-02, -3.5504e-02, -3.7199e-02,\n",
       "                      -5.4948e-02,  5.2659e-02,  2.8839e-02,  2.5694e-02,  5.1875e-02,\n",
       "                       9.3406e-03, -8.1085e-02,  1.1919e-02,  6.8108e-03,  1.3980e-02,\n",
       "                      -1.2258e-02,  2.4535e-02,  9.5056e-03, -2.5015e-02, -3.6679e-02,\n",
       "                       4.6217e-02, -3.0895e-02,  6.9839e-02,  1.9568e-02,  4.3228e-02,\n",
       "                      -1.0504e-01,  7.1178e-03, -2.2839e-02,  6.1021e-03, -2.5908e-02,\n",
       "                       1.8011e-02, -6.8150e-03,  1.4388e-02, -8.4440e-01, -5.0008e-02,\n",
       "                       5.0547e-02,  2.1449e-02,  3.4951e-02,  2.9165e-02,  5.0482e-02,\n",
       "                      -3.4202e-02,  1.1743e-02, -2.5902e-02,  1.1465e-02, -1.8932e-02,\n",
       "                       4.6007e-02,  7.0674e-03, -1.2600e-02, -5.8306e-02,  4.0210e-02,\n",
       "                      -1.3422e-03, -6.4455e-02, -1.2165e-02,  1.6247e-02, -5.2815e-02,\n",
       "                       1.7456e-02,  2.2557e-02,  2.7907e-02,  4.2363e-03,  1.1448e-01,\n",
       "                      -3.9010e-02, -1.6384e-02,  2.6358e-02, -5.9402e-02,  2.2687e-02,\n",
       "                      -3.1634e-03,  3.5325e-03,  3.5099e-02, -4.7096e-03, -5.4569e-02,\n",
       "                       1.0903e-02, -3.4509e-02,  1.8695e-02,  7.5732e-02,  3.4053e-02,\n",
       "                       6.2831e-02, -2.6246e-02,  5.7984e-02,  4.7430e-02, -8.3590e-03,\n",
       "                       2.0850e-02, -2.1370e-02, -4.9586e-02,  3.7810e-02, -3.8803e-02,\n",
       "                       2.2560e-02, -3.5042e-02,  7.7196e-03,  6.0697e-03, -3.0465e-02,\n",
       "                       2.6143e-02,  9.4021e-03,  5.2079e-03, -2.3997e-02,  1.1641e-02,\n",
       "                       3.1250e-02,  3.5085e-02, -9.8611e-03, -1.8276e-02,  3.4717e-02,\n",
       "                      -4.7229e-02, -1.6258e-02, -2.4274e-02,  1.1921e-02, -5.4637e-05,\n",
       "                       2.6690e-03,  1.5262e-02, -9.1557e-03, -2.3120e-02, -3.7567e-02,\n",
       "                       1.7669e-02, -4.8386e-02,  2.1895e-02,  2.9513e-03,  2.1908e-03,\n",
       "                      -1.5556e-02,  2.2414e-02,  2.7394e-02,  6.3540e-02,  6.7425e-02,\n",
       "                      -7.4819e-02,  9.1686e-03,  1.7065e-02,  3.2281e-01, -4.3074e-02,\n",
       "                       4.9128e-02, -3.0006e-02, -1.2402e-02,  3.1371e-02, -2.2832e-02,\n",
       "                       2.4914e-03, -2.7435e-02, -3.5384e-02,  3.2040e-02,  9.0674e-03,\n",
       "                       8.8573e-03,  3.9077e-02,  2.3204e-02, -3.2984e-02,  4.5532e-03,\n",
       "                       1.8921e-02,  4.0077e-02, -3.4041e-02, -2.9052e-02, -4.6351e-02,\n",
       "                       1.4366e-03, -8.1437e-03,  3.6853e-02,  1.5669e-02,  1.3009e-04,\n",
       "                       5.7881e-03, -5.2228e-02, -3.9076e-02, -3.0339e-02,  4.0863e-02,\n",
       "                       1.5087e-02, -1.4674e-02, -2.9197e-02,  6.3924e-02,  3.2343e-02,\n",
       "                       3.2809e-02,  3.5171e-02, -6.4838e-03, -1.2553e-02, -5.9983e-02,\n",
       "                      -7.8702e-02,  1.8100e-02, -4.2591e-02, -2.6911e-02, -3.6026e-02,\n",
       "                       2.9356e-02,  1.1616e-02, -2.5623e-02,  5.5860e-02, -5.0318e-02,\n",
       "                      -2.8740e-02,  2.9167e-02,  2.1062e-02,  1.6689e-02, -2.0427e-03,\n",
       "                       2.0535e-02,  6.8429e-03,  5.4590e-02,  1.2542e-03, -1.8582e-02,\n",
       "                       3.5521e-02, -1.0943e-02,  3.5500e-02,  2.3151e-02, -6.4734e-02,\n",
       "                      -3.1958e-02,  3.3164e-02,  1.6091e-02,  4.1602e-02, -4.5743e-02,\n",
       "                       7.2834e-02, -3.8182e-02,  7.0143e-02,  4.2856e-02, -4.3029e-02,\n",
       "                       2.0817e-02,  1.3513e-02,  2.5156e-02, -4.1374e-02, -3.8445e-04,\n",
       "                      -2.9435e-02,  1.5504e-03,  3.8705e-02, -7.1709e-03, -5.5325e-02,\n",
       "                      -5.4461e-03, -1.0570e-02,  7.0284e-02,  3.9520e-02,  7.0441e-03,\n",
       "                       2.2975e-02, -6.3571e-02,  7.7882e-03,  3.6932e-02, -3.8044e-03,\n",
       "                       6.8917e-02, -2.4665e-02,  3.6534e-02,  6.3742e-03, -1.6719e-02,\n",
       "                      -4.2227e-02,  4.8664e-02, -2.1787e-02, -3.4080e-02, -2.8962e-02,\n",
       "                      -1.0194e-03,  1.4566e-02, -1.4498e-03,  1.7742e-02, -3.6946e-02,\n",
       "                      -1.4624e-02, -4.3064e-02,  1.3141e-02,  2.4379e-02, -2.2694e-02,\n",
       "                      -9.7470e-02, -2.1181e-03, -1.4412e-02,  7.8439e-03, -1.7518e-02,\n",
       "                       3.5459e-02,  2.4267e-02, -1.3918e-02, -1.9732e-02,  2.6244e-02,\n",
       "                      -3.1075e-02, -4.6510e-03,  1.2461e-02,  7.3980e-02,  2.3574e-02,\n",
       "                       2.3403e-02,  2.7576e-02, -7.3866e-02, -1.9724e-02,  1.1760e-02,\n",
       "                       4.5591e-02, -7.2049e-02,  2.4930e-02, -5.4612e-02, -8.0114e-02,\n",
       "                       4.0764e-02,  6.5198e-02, -1.1090e-02,  2.9002e-02,  6.1914e-03,\n",
       "                       1.6318e-02,  6.4885e-02, -1.8047e-02, -6.2276e-03,  4.5270e-02,\n",
       "                       2.2687e-02,  4.5106e-02, -1.0342e-01,  1.7238e-02,  1.1738e-02,\n",
       "                      -8.5033e-03, -6.3087e-02,  3.6181e-02, -5.5720e-02,  1.2150e-02,\n",
       "                      -8.7087e-03, -3.2720e-02, -7.5191e-02,  3.1646e-02,  6.7023e-02,\n",
       "                       3.0986e-02,  1.6188e-02,  2.2368e-02,  1.3615e-02,  6.7814e-02,\n",
       "                      -4.3577e-02,  1.8554e-02,  1.9333e-02, -6.5749e-02, -1.3314e-02,\n",
       "                       4.0656e-02, -6.4813e-02, -4.2461e-02,  3.6640e-02, -1.3187e-02,\n",
       "                       9.8254e-03, -2.0587e-02,  1.4881e-02,  6.9439e-02, -4.9059e-02,\n",
       "                      -5.1325e-03, -4.8156e-02, -1.1124e-01,  4.0990e-02,  1.8329e-02,\n",
       "                      -4.1940e-02,  4.6449e-02, -2.2130e-02,  4.0728e-02,  5.8848e-02,\n",
       "                      -3.7896e-03,  3.9498e-04,  7.8715e-03,  5.6334e-02,  2.4283e-02,\n",
       "                       3.3276e-02, -8.2299e-02,  2.1792e-02,  2.6872e-02, -3.5718e-02,\n",
       "                       5.4003e-02,  8.6675e-02, -3.8323e-02, -2.4516e-02,  9.8226e-03,\n",
       "                      -3.1265e-02,  4.6215e-02, -2.1464e-03,  2.6572e-03,  2.5906e-02,\n",
       "                      -5.6225e-03,  5.1613e-02, -2.0280e-02, -3.5191e-02,  7.5115e-03,\n",
       "                       6.5004e-03,  1.0195e-02, -4.3468e-02,  7.9091e-02, -2.8160e-03,\n",
       "                      -3.8418e-02, -5.3653e-02, -6.5033e-02,  5.1976e-02,  1.7687e-02,\n",
       "                      -6.1231e-02,  2.5685e-02, -4.4516e-03,  4.0666e-03,  7.3046e-02,\n",
       "                       3.5198e-02,  7.0603e-02,  1.1606e-02, -1.8361e-02,  8.5887e-03,\n",
       "                      -2.0172e-02,  6.3348e-03,  9.4422e-03, -5.2512e-03,  3.2840e-02,\n",
       "                      -1.5923e-02,  4.6013e-03, -5.0921e-03, -1.7876e-02,  5.1154e-02,\n",
       "                      -5.6972e-02,  1.9520e-02, -7.3828e-02, -1.4592e-02, -3.6736e-02,\n",
       "                      -5.9056e-03,  1.9152e-02, -4.4521e-03,  4.3943e-03,  1.0721e-02,\n",
       "                       2.3866e-02,  3.1144e-02, -4.9351e-02, -6.2700e-02, -5.7003e-02,\n",
       "                      -5.2836e-02, -2.2375e-02, -2.3366e-02], device='cuda:0')),\n",
       "             ('layers.2.anorm.weight',\n",
       "              tensor([0.7303, 0.6755, 0.7629, 0.7319, 0.7124, 0.7555, 0.7565, 0.7459, 0.7902,\n",
       "                      0.7293, 0.7810, 0.7185, 0.7621, 0.7160, 0.7886, 0.7256, 0.7908, 0.6706,\n",
       "                      0.7294, 0.7665, 0.7475, 0.8045, 0.6756, 0.7193, 0.7637, 0.6635, 0.7994,\n",
       "                      0.7385, 0.6634, 0.7043, 0.7288, 0.7211, 0.6856, 0.7661, 0.7996, 0.7093,\n",
       "                      0.8185, 0.7280, 0.7443, 0.7162, 0.7358, 0.7074, 0.7296, 0.7590, 0.7018,\n",
       "                      0.7752, 0.6982, 0.7444, 0.7433, 0.7770, 0.7666, 0.7360, 0.8504, 0.6647,\n",
       "                      0.7662, 0.6975, 0.7187, 0.8149, 0.7123, 0.7329, 0.6775, 0.7597, 0.7820,\n",
       "                      0.7257, 0.7242, 0.7353, 0.7676, 0.7059, 0.7614, 0.7296, 0.7292, 0.7263,\n",
       "                      0.6660, 0.7305, 0.7414, 0.7503, 0.6504, 0.8011, 0.7088, 0.7859, 0.7539,\n",
       "                      0.7335, 0.7138, 0.7363, 0.7974, 0.6571, 0.6931, 0.7156, 0.7081, 0.7896,\n",
       "                      0.7183, 0.7371, 0.7409, 0.7928, 0.7577, 0.7566, 0.7308, 0.6923, 0.6951,\n",
       "                      0.8026, 0.7370, 0.7173, 0.7834, 0.6176, 0.7757, 0.6861, 0.7537, 0.7112,\n",
       "                      0.7487, 0.7208, 0.8023, 0.7123, 0.8083, 0.6936, 0.7576, 0.7340, 0.7668,\n",
       "                      0.6911, 0.7009, 0.7699, 0.7817, 0.7787, 0.8158, 0.7421, 0.7708, 0.7386,\n",
       "                      0.7368, 0.7306, 0.7552, 0.7239, 0.8052, 0.8008, 0.7478, 0.7718, 0.7310,\n",
       "                      0.7669, 0.7724, 0.7475, 0.7361, 0.7641, 0.8086, 0.7517, 0.6866, 0.7173,\n",
       "                      0.7154, 0.7100, 0.7571, 0.7684, 0.7393, 0.7351, 0.7109, 0.7155, 0.7808,\n",
       "                      0.7652, 0.6437, 0.7197, 0.7814, 0.7341, 0.8003, 0.7270, 0.7843, 0.7482,\n",
       "                      0.7398, 0.7226, 0.7747, 0.8122, 0.7293, 0.7455, 0.6802, 0.7956, 0.7417,\n",
       "                      0.7549, 0.7136, 0.7119, 0.7251, 0.8063, 0.7330, 0.7849, 0.8054, 0.7467,\n",
       "                      0.7465, 0.7549, 0.7122, 0.7411, 0.8006, 0.7290, 0.7511, 0.7613, 0.6909,\n",
       "                      0.7219, 0.7541, 0.7798, 0.8261, 0.7914, 0.7166, 0.7280, 0.7627, 0.7622,\n",
       "                      0.7554, 0.7369, 0.7899, 0.7323, 0.7914, 0.7132, 0.7298, 0.7451, 0.7632,\n",
       "                      0.7839, 0.7882, 0.6871, 0.7372, 0.6379, 0.7887, 0.7685, 0.7094, 0.8090,\n",
       "                      0.8016, 0.6737, 0.7042, 0.7722, 0.8160, 0.7247, 0.7396, 0.6999, 0.7329,\n",
       "                      0.7746, 0.7390, 0.7483, 0.6964, 0.7913, 0.7173, 0.7065, 0.8235, 0.6981,\n",
       "                      0.7940, 0.7282, 0.7139, 0.7164, 0.7531, 0.6961, 0.6228, 0.6974, 0.7409,\n",
       "                      0.7641, 0.7862, 0.7549, 0.7690, 0.7504, 0.7429, 0.7416, 0.7384, 0.7135,\n",
       "                      0.7711, 0.7714, 0.7006, 0.7312, 0.6977, 0.7328, 0.7393, 0.7316, 0.7628,\n",
       "                      0.7103, 0.7703, 0.7416, 0.7112, 0.7113, 0.7289, 0.8151, 0.7698, 0.7404,\n",
       "                      0.7576, 0.7590, 0.7706, 0.7299, 0.6742, 0.8040, 0.7372, 0.7269, 0.7410,\n",
       "                      0.7288, 0.7379, 0.7291, 0.7131, 0.7562, 0.7602, 0.7632, 0.7145, 0.7417,\n",
       "                      0.7028, 0.7733, 0.7262, 0.7429, 0.7353, 0.7285, 0.6760, 0.7247, 0.7115,\n",
       "                      0.7662, 0.7473, 0.6878, 0.7331, 0.7563, 0.6891, 0.6936, 0.7044, 0.7444,\n",
       "                      0.7594, 0.7340, 0.8557, 0.7415, 0.6600, 0.7229, 0.7731, 0.6727, 0.7414,\n",
       "                      0.7474, 0.6816, 0.7873, 0.7853, 0.8278, 0.7316, 0.7886, 0.7788, 0.7559,\n",
       "                      0.7153, 0.6217, 0.6942, 0.7556, 0.7503, 0.7560, 0.8067, 0.7721, 0.7585,\n",
       "                      0.7469, 0.7471, 0.8094, 0.7542, 0.7052, 0.7456, 0.7364, 0.6974, 0.7547,\n",
       "                      0.7241, 0.7669, 0.7427, 0.7745, 0.7514, 0.7399, 0.7019, 0.7212, 0.8134,\n",
       "                      0.7188, 0.7379, 0.7894, 0.7772, 0.7611, 0.7703, 0.7775, 0.7377, 0.8393,\n",
       "                      0.6844, 0.7394, 0.7773, 0.7208, 0.8018, 0.7172, 0.7540, 0.8003, 0.7332,\n",
       "                      0.7358, 0.7596, 0.7868, 0.7567, 0.7148, 0.7564, 0.6928, 0.7854, 0.7345,\n",
       "                      0.7645, 0.7137, 0.7596, 0.7642, 0.8152, 0.7711, 0.7215, 0.7221, 0.7674,\n",
       "                      0.7872, 0.7703, 0.6983, 0.7561, 0.7329, 0.7793, 0.7349, 0.7254, 0.7564,\n",
       "                      0.8100, 0.7944, 0.7480, 0.7124, 0.6968, 0.7060, 0.7639, 0.7390, 0.7314,\n",
       "                      0.7887, 0.7675, 0.7627, 0.4062, 0.8380, 0.7254, 0.6659, 0.7193, 0.6405,\n",
       "                      0.7727, 0.7711, 0.6882, 0.6908, 0.7661, 0.7519, 0.8075, 0.7135, 0.7680,\n",
       "                      0.7963, 0.7557, 0.7103, 0.7960, 0.8038, 0.7068, 0.8382, 0.7418, 0.8000,\n",
       "                      0.7394, 0.7404, 0.6895, 0.7599, 0.7438, 0.7509, 0.7889, 0.6888, 0.7057,\n",
       "                      0.7277, 0.7651, 0.7381, 0.7282, 0.7505, 0.7178, 0.7553, 0.7425, 0.7332,\n",
       "                      0.6965, 0.7733, 0.7636, 0.7266, 0.7443, 0.7560, 0.7607, 0.7266, 0.7847,\n",
       "                      0.7059, 0.6909, 0.6918, 0.7811, 0.8452, 0.7405, 0.7510, 0.7555, 0.7327,\n",
       "                      0.7696, 0.7342, 0.7913, 0.7186, 0.7194, 0.7887, 0.7252, 0.7359, 0.6719,\n",
       "                      0.7325, 0.7424, 0.7478, 0.7730, 0.7564, 0.7209, 0.7632, 0.7130, 0.7906,\n",
       "                      0.7115, 0.7618, 0.7571, 0.7436, 0.7446, 0.7412, 0.7554, 0.7343, 0.8067,\n",
       "                      0.7751, 0.7417, 0.7158, 0.4257, 0.7636, 0.7966, 0.7873, 0.7420, 0.6883,\n",
       "                      0.7713, 0.7401, 0.7838, 0.7496, 0.7586, 0.7756, 0.7059, 0.7319, 0.6960,\n",
       "                      0.7560, 0.7195, 0.7659, 0.7837, 0.7065, 0.7845, 0.7502, 0.8033, 0.7572,\n",
       "                      0.7462, 0.7508, 0.7520, 0.7137, 0.7460, 0.7786, 0.7624, 0.7006, 0.8060,\n",
       "                      0.7230, 0.7398, 0.7563, 0.7557, 0.7682, 0.8061, 0.7623, 0.7497, 0.8062,\n",
       "                      0.7629, 0.7274, 0.7263, 0.7075, 0.7461, 0.7456, 0.7946, 0.7737, 0.7270,\n",
       "                      0.7298, 0.7163, 0.7023, 0.7253, 0.7200, 0.7608, 0.7575, 0.7145, 0.7857,\n",
       "                      0.7515, 0.6831, 0.7816, 0.7581, 0.6525, 0.7423, 0.7551, 0.7346, 0.7269,\n",
       "                      0.8071, 0.7400, 0.7375, 0.8221, 0.7525, 0.7398, 0.7104, 0.7701, 0.7777,\n",
       "                      0.7407, 0.7226, 0.7183, 0.6979, 0.7733, 0.7142, 0.7686, 0.7656, 0.8052,\n",
       "                      0.7315, 0.7841, 0.7830, 0.7657, 0.7480, 0.7740, 0.6958, 0.7894, 0.6908,\n",
       "                      0.7851, 0.7914, 0.7428, 0.7733, 0.6953, 0.8249, 0.7807, 0.7923, 0.7721,\n",
       "                      0.7836, 0.7093, 0.7849, 0.7540, 0.7114, 0.7280, 0.8126, 0.7451, 0.7670,\n",
       "                      0.7732, 0.7607, 0.7238, 0.7698, 0.6851, 0.7037, 0.7629, 0.7147, 0.7385,\n",
       "                      0.7356, 0.7388, 0.7482, 0.7090, 0.6616, 0.8072, 0.7947, 0.7375, 0.7273,\n",
       "                      0.7320, 0.7201, 0.7293, 0.7114, 0.6833, 0.7203, 0.8254, 0.7787, 0.7365,\n",
       "                      0.7510, 0.7525, 0.8107, 0.8123, 0.7465, 0.7139, 0.8212, 0.7883, 0.7679,\n",
       "                      0.7296, 0.6835, 0.7426, 0.7884, 0.7625, 0.7490, 0.7978, 0.8083, 0.7681,\n",
       "                      0.7603, 0.7866, 0.7590, 0.7217, 0.7779, 0.7275, 0.7672, 0.7049, 0.7681,\n",
       "                      0.7617, 0.7517, 0.6832, 0.7105, 0.6942, 0.7870, 0.7555, 0.7239, 0.8035,\n",
       "                      0.7776, 0.7336, 0.7942, 0.6980, 0.7909, 0.7521, 0.7494, 0.7720, 0.7829,\n",
       "                      0.8258, 0.7314, 0.6930, 0.7319, 0.7277, 0.7407, 0.8295, 0.7266, 0.7177,\n",
       "                      0.7634, 0.7287, 0.7797, 0.8012, 0.7635, 0.7767, 0.7471, 0.7433, 0.7344,\n",
       "                      0.7726, 0.7218, 0.7958, 0.7371, 0.7519, 0.7641, 0.7620, 0.7701, 0.7892,\n",
       "                      0.6215, 0.7019, 0.7587, 0.7114, 0.7901, 0.7430, 0.7378, 0.6962, 0.7239,\n",
       "                      0.7264, 0.7964, 0.7432, 0.7309, 0.7315, 0.7910, 0.7607, 0.7119, 0.7541,\n",
       "                      0.6842, 0.7744, 0.7344, 0.7674, 0.7793, 0.7399, 0.7405, 0.7680, 0.7227,\n",
       "                      0.7094, 0.7896, 0.7048, 0.7518, 0.7562, 0.6946, 0.7753, 0.7668, 0.7609,\n",
       "                      0.8225, 0.7868, 0.7086, 0.6787, 0.7721, 0.8254, 0.8034, 0.7476, 0.7700,\n",
       "                      0.7537, 0.6719, 0.7602, 0.7008, 0.7421, 0.6966, 0.7549, 0.6670, 0.6940,\n",
       "                      0.7579, 0.7594, 0.7020], device='cuda:0')),\n",
       "             ('layers.2.fnorm.weight',\n",
       "              tensor([0.7290, 0.6610, 0.7208, 0.6503, 0.7248, 0.6794, 0.7450, 0.7516, 0.6833,\n",
       "                      0.6202, 0.6212, 0.6483, 0.6561, 0.7133, 0.7301, 0.6969, 0.7106, 0.6782,\n",
       "                      0.6076, 0.7938, 0.7949, 0.7207, 0.7443, 0.7043, 0.6900, 0.6171, 0.6772,\n",
       "                      0.6630, 0.6885, 0.7189, 0.6728, 0.6273, 0.6440, 0.7087, 0.6651, 0.6203,\n",
       "                      0.6781, 0.6563, 0.6601, 0.7201, 0.5849, 0.7394, 0.6879, 0.7393, 0.6740,\n",
       "                      0.6319, 0.7645, 0.6686, 0.6012, 0.6653, 0.7247, 0.6000, 0.6452, 0.6655,\n",
       "                      0.6531, 0.6543, 0.6490, 0.6517, 0.6521, 0.6644, 0.6224, 0.7245, 0.7136,\n",
       "                      0.6036, 0.6630, 0.6907, 0.6965, 0.6942, 0.6652, 0.6062, 0.7033, 0.7032,\n",
       "                      0.7165, 0.7041, 0.6869, 0.7238, 0.6655, 0.7244, 0.6508, 0.6707, 0.7141,\n",
       "                      0.7186, 0.5183, 0.6445, 0.6922, 0.6807, 0.6878, 0.6447, 0.6877, 0.6879,\n",
       "                      0.7329, 0.6681, 0.5946, 0.6996, 0.6631, 0.6821, 0.6909, 0.6491, 0.6506,\n",
       "                      0.7494, 0.6572, 0.7086, 0.7119, 0.5420, 0.6910, 0.7243, 0.6679, 0.6313,\n",
       "                      0.6985, 0.6325, 0.7032, 0.6737, 0.7665, 0.6858, 0.7638, 0.6571, 0.6095,\n",
       "                      0.6073, 0.6658, 0.6550, 0.7344, 0.7435, 0.7134, 0.6804, 0.7178, 0.6652,\n",
       "                      0.6755, 0.6788, 0.7540, 0.6567, 0.6995, 0.6981, 0.6727, 0.7163, 0.6332,\n",
       "                      0.7416, 0.6706, 0.6696, 0.6865, 0.6563, 0.7245, 0.6604, 0.6605, 0.7213,\n",
       "                      0.7161, 0.6589, 0.6350, 0.6604, 0.7635, 0.7319, 0.6984, 0.5865, 0.7280,\n",
       "                      0.6425, 0.6361, 0.6341, 0.6499, 0.6620, 0.6600, 0.7307, 0.6188, 0.6120,\n",
       "                      0.7785, 0.6550, 0.7282, 0.6723, 0.6368, 0.5878, 0.6119, 0.7081, 0.6668,\n",
       "                      0.7107, 0.5877, 0.6569, 0.7211, 0.6179, 0.6823, 0.7476, 0.6298, 0.7096,\n",
       "                      0.7405, 0.6831, 0.6999, 0.7110, 0.6567, 0.7353, 0.6335, 0.7054, 0.6924,\n",
       "                      0.6883, 0.7236, 0.7225, 0.6629, 0.6982, 0.6842, 0.6304, 0.6503, 0.6817,\n",
       "                      0.6429, 0.6212, 0.6708, 0.6354, 0.6918, 0.6426, 0.6166, 0.6457, 0.6980,\n",
       "                      0.7111, 0.8085, 0.6714, 0.6993, 0.6831, 0.7004, 0.6479, 0.6956, 0.7287,\n",
       "                      0.6870, 0.7029, 0.6587, 0.6830, 0.7650, 0.5772, 0.6665, 0.6965, 0.7181,\n",
       "                      0.7743, 0.0170, 0.7087, 0.6947, 0.7041, 0.6825, 0.6206, 0.6091, 0.6431,\n",
       "                      0.7548, 0.7281, 0.6384, 0.7045, 0.6821, 0.7055, 0.6299, 0.6435, 0.6935,\n",
       "                      0.6484, 0.7448, 0.6683, 0.6170, 0.6270, 0.7397, 0.6049, 0.7444, 0.6676,\n",
       "                      0.7359, 0.6346, 0.5975, 0.6094, 0.7169, 0.6579, 0.6636, 0.6426, 0.7642,\n",
       "                      0.6368, 0.6918, 0.6791, 0.6229, 0.7098, 0.6036, 0.7010, 0.6812, 0.7161,\n",
       "                      0.6659, 0.7298, 0.6181, 0.7271, 0.7531, 0.6866, 0.6748, 0.6801, 0.6983,\n",
       "                      0.6340, 0.6351, 0.6469, 0.7274, 0.6642, 0.6997, 0.6839, 0.7038, 0.7110,\n",
       "                      0.7035, 0.6550, 0.7468, 0.7396, 0.7287, 0.6929, 0.6655, 0.6835, 0.6396,\n",
       "                      0.6859, 0.7613, 0.6734, 0.5836, 0.6391, 0.6473, 0.6119, 0.6572, 0.7181,\n",
       "                      0.7042, 0.6757, 0.7873, 0.7344, 0.6057, 0.7109, 0.6969, 0.6292, 0.6909,\n",
       "                      0.7096, 0.6768, 0.6512, 0.7120, 0.6537, 0.7337, 0.6796, 0.6846, 0.6784,\n",
       "                      0.6498, 0.6894, 0.6052, 0.6677, 0.6904, 0.6260, 0.7581, 0.7110, 0.6863,\n",
       "                      0.6293, 0.6680, 0.7362, 0.7295, 0.5838, 0.6803, 0.6527, 0.6905, 0.5988,\n",
       "                      0.6415, 0.6901, 0.6479, 0.6413, 0.6638, 0.6394, 0.7032, 0.6465, 0.6681,\n",
       "                      0.6494, 0.7690, 0.7422, 0.7439, 0.6967, 0.6413, 0.6472, 0.6613, 0.7116,\n",
       "                      0.7103, 0.5980, 0.6512, 0.6643, 0.6884, 0.6768, 0.7565, 0.7029, 0.6573,\n",
       "                      0.6965, 0.6373, 0.7509, 0.7309, 0.6739, 0.6592, 0.6815, 0.7376, 0.6609,\n",
       "                      0.6785, 0.6960, 0.7134, 0.6546, 0.6694, 0.7250, 0.6960, 0.7554, 0.6497,\n",
       "                      0.6953, 0.7073, 0.7448, 0.7102, 0.6364, 0.7526, 0.7003, 0.6763, 0.7174,\n",
       "                      0.6691, 0.6744, 0.6767, 0.6591, 0.7326, 0.6176, 0.7055, 0.6624, 0.6642,\n",
       "                      0.7662, 0.6825, 0.7293, 0.0694, 0.7470, 0.6547, 0.6262, 0.6969, 0.6594,\n",
       "                      0.6451, 0.6928, 0.6638, 0.6745, 0.6958, 0.8009, 0.6900, 0.6769, 0.6989,\n",
       "                      0.7100, 0.6294, 0.6786, 0.7283, 0.6955, 0.7448, 0.6853, 0.7369, 0.6636,\n",
       "                      0.6388, 0.6886, 0.6931, 0.7205, 0.7061, 0.6479, 0.6351, 0.6208, 0.6804,\n",
       "                      0.7199, 0.6740, 0.6500, 0.7432, 0.6755, 0.5503, 0.6863, 0.7413, 0.6155,\n",
       "                      0.6627, 0.7006, 0.8129, 0.7518, 0.7167, 0.6578, 0.6866, 0.7086, 0.7382,\n",
       "                      0.7424, 0.7080, 0.7303, 0.7529, 0.6946, 0.7418, 0.6717, 0.6857, 0.6856,\n",
       "                      0.6887, 0.6374, 0.6629, 0.6751, 0.5823, 0.7174, 0.7249, 0.7188, 0.6561,\n",
       "                      0.6707, 0.6496, 0.6886, 0.6585, 0.7521, 0.6569, 0.6859, 0.7278, 0.7154,\n",
       "                      0.6884, 0.6295, 0.7493, 0.7000, 0.6917, 0.7089, 0.7288, 0.6754, 0.6544,\n",
       "                      0.6809, 0.7018, 0.7076, 0.2379, 0.6234, 0.6790, 0.6936, 0.6946, 0.6707,\n",
       "                      0.6516, 0.6913, 0.6747, 0.7070, 0.7345, 0.7600, 0.5718, 0.7526, 0.7410,\n",
       "                      0.6710, 0.6870, 0.6627, 0.7362, 0.6781, 0.6820, 0.7404, 0.6763, 0.6778,\n",
       "                      0.6975, 0.7652, 0.7161, 0.6856, 0.7562, 0.6784, 0.7457, 0.7311, 0.6692,\n",
       "                      0.7156, 0.7442, 0.7142, 0.6832, 0.6557, 0.6230, 0.6799, 0.6910, 0.6299,\n",
       "                      0.6392, 0.7043, 0.7075, 0.7625, 0.7587, 0.6614, 0.7166, 0.7202, 0.7081,\n",
       "                      0.6188, 0.6656, 0.5866, 0.6627, 0.6887, 0.7107, 0.6375, 0.7168, 0.7821,\n",
       "                      0.6077, 0.7317, 0.7906, 0.6616, 0.6307, 0.6603, 0.6936, 0.5693, 0.6813,\n",
       "                      0.6561, 0.7667, 0.6745, 0.6913, 0.7694, 0.7479, 0.6715, 0.6831, 0.6971,\n",
       "                      0.6594, 0.6822, 0.7845, 0.6436, 0.7313, 0.6224, 0.6555, 0.7228, 0.6644,\n",
       "                      0.6923, 0.6641, 0.7740, 0.6806, 0.6715, 0.6721, 0.6898, 0.7454, 0.6761,\n",
       "                      0.6816, 0.6840, 0.6466, 0.6916, 0.7274, 0.7331, 0.7141, 0.6828, 0.6966,\n",
       "                      0.7411, 0.6689, 0.7194, 0.6708, 0.7579, 0.6623, 0.6774, 0.6734, 0.6934,\n",
       "                      0.6895, 0.7604, 0.7158, 0.7107, 0.6052, 0.7975, 0.6862, 0.7312, 0.6523,\n",
       "                      0.7372, 0.7657, 0.7018, 0.6269, 0.6767, 0.7442, 0.7651, 0.6686, 0.6174,\n",
       "                      0.7087, 0.7034, 0.7126, 0.6793, 0.7196, 0.6902, 0.7117, 0.7068, 0.7148,\n",
       "                      0.6262, 0.7248, 0.6882, 0.6990, 0.6249, 0.6671, 0.6849, 0.7476, 0.7068,\n",
       "                      0.6875, 0.5694, 0.7097, 0.7348, 0.6762, 0.5928, 0.7028, 0.6871, 0.6602,\n",
       "                      0.7445, 0.7178, 0.7068, 0.6861, 0.5828, 0.6554, 0.6684, 0.6489, 0.7397,\n",
       "                      0.7534, 0.6978, 0.6735, 0.6980, 0.7128, 0.7451, 0.6761, 0.6218, 0.7049,\n",
       "                      0.6938, 0.6777, 0.7113, 0.5950, 0.7128, 0.6669, 0.6415, 0.7888, 0.7167,\n",
       "                      0.6668, 0.6520, 0.7413, 0.7917, 0.6695, 0.6544, 0.7216, 0.7150, 0.6887,\n",
       "                      0.6242, 0.6695, 0.6537, 0.6323, 0.6710, 0.7496, 0.6661, 0.6795, 0.6858,\n",
       "                      0.7024, 0.7338, 0.6745, 0.7209, 0.6521, 0.6976, 0.6864, 0.6770, 0.7034,\n",
       "                      0.5815, 0.6232, 0.6923, 0.5839, 0.7096, 0.6751, 0.6337, 0.6010, 0.6711,\n",
       "                      0.7108, 0.6285, 0.6986, 0.6424, 0.6487, 0.6583, 0.7031, 0.7469, 0.7143,\n",
       "                      0.6990, 0.6622, 0.6796, 0.7116, 0.7268, 0.6461, 0.6715, 0.6993, 0.6803,\n",
       "                      0.6249, 0.7170, 0.6915, 0.6617, 0.6972, 0.6972, 0.7495, 0.6851, 0.7242,\n",
       "                      0.7262, 0.6722, 0.6424, 0.6186, 0.6655, 0.6145, 0.6548, 0.7181, 0.6835,\n",
       "                      0.7366, 0.6034, 0.7484, 0.7220, 0.6596, 0.7365, 0.6939, 0.6615, 0.6535,\n",
       "                      0.7323, 0.6966, 0.6890], device='cuda:0')),\n",
       "             ('layers.3.mha.query.weight',\n",
       "              tensor([[ 0.0091, -0.0596, -0.0087,  ..., -0.0371, -0.0554,  0.0182],\n",
       "                      [-0.0233, -0.0209,  0.0154,  ..., -0.0087,  0.0154,  0.0302],\n",
       "                      [-0.0147, -0.0651,  0.0369,  ..., -0.0411, -0.0019,  0.0153],\n",
       "                      ...,\n",
       "                      [ 0.0257, -0.0157, -0.0256,  ...,  0.0809,  0.0060, -0.0354],\n",
       "                      [ 0.0360, -0.0865, -0.0008,  ..., -0.0195,  0.0234,  0.0930],\n",
       "                      [ 0.0796, -0.0016, -0.0799,  ..., -0.1103,  0.0810, -0.0387]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.mha.key.weight',\n",
       "              tensor([[-0.0341,  0.0182, -0.0260,  ...,  0.0252, -0.0224, -0.0189],\n",
       "                      [-0.0055,  0.0273,  0.0526,  ..., -0.0201,  0.0442, -0.0194],\n",
       "                      [-0.0450, -0.0397,  0.0093,  ..., -0.0469,  0.0180,  0.0262],\n",
       "                      ...,\n",
       "                      [-0.0465,  0.0034, -0.0121,  ...,  0.0172, -0.0034,  0.0554],\n",
       "                      [ 0.0354, -0.0091, -0.0116,  ..., -0.0227,  0.1029, -0.0126],\n",
       "                      [ 0.0541,  0.0345,  0.0700,  ...,  0.1204,  0.0166,  0.0330]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.mha.value.weight',\n",
       "              tensor([[-7.2016e-02, -1.5524e-02,  6.7583e-02,  ..., -2.3790e-02,\n",
       "                       -2.3845e-02, -4.0171e-02],\n",
       "                      [ 9.8213e-02,  1.3328e-02,  7.1766e-03,  ..., -6.5781e-02,\n",
       "                       -1.4510e-01,  2.0939e-03],\n",
       "                      [ 5.6822e-02, -2.6052e-03, -6.8412e-03,  ...,  2.5688e-02,\n",
       "                        4.5767e-03,  2.0562e-02],\n",
       "                      ...,\n",
       "                      [ 5.2483e-02,  2.1546e-02, -4.8990e-02,  ..., -3.0268e-02,\n",
       "                       -2.0427e-02,  5.3006e-02],\n",
       "                      [ 1.3123e-02,  9.1329e-04, -8.9371e-02,  ...,  1.2957e-02,\n",
       "                       -1.2528e-02, -1.9907e-02],\n",
       "                      [-1.2095e-04,  1.0072e-02,  6.3030e-02,  ...,  1.8369e-02,\n",
       "                        4.0999e-03,  2.7676e-02]], device='cuda:0')),\n",
       "             ('layers.3.mha.proj.weight',\n",
       "              tensor([[-0.0330, -0.0290,  0.0023,  ...,  0.0245, -0.0404, -0.0800],\n",
       "                      [-0.0258, -0.0329, -0.0518,  ..., -0.0671,  0.0375,  0.0024],\n",
       "                      [-0.0191, -0.0135,  0.0177,  ..., -0.0628,  0.0545, -0.0109],\n",
       "                      ...,\n",
       "                      [ 0.0162,  0.1094,  0.0257,  ...,  0.0125,  0.0612,  0.0103],\n",
       "                      [ 0.0859, -0.0206, -0.0420,  ..., -0.0202,  0.0297,  0.0747],\n",
       "                      [-0.0384,  0.0250, -0.0137,  ...,  0.0186, -0.0097, -0.0640]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.w.weight',\n",
       "              tensor([[ 0.0310, -0.0629, -0.0370,  ...,  0.0327, -0.0143, -0.0408],\n",
       "                      [ 0.0453, -0.0569, -0.0326,  ..., -0.0102,  0.0615, -0.0112],\n",
       "                      [-0.0153,  0.0126, -0.0499,  ..., -0.0373, -0.0356, -0.0624],\n",
       "                      ...,\n",
       "                      [ 0.0181, -0.0692, -0.0155,  ..., -0.0156,  0.0357, -0.0361],\n",
       "                      [-0.0354,  0.0124,  0.0274,  ...,  0.0231,  0.0598, -0.0646],\n",
       "                      [ 0.0038,  0.0799, -0.0024,  ...,  0.0869, -0.0210, -0.0135]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.w.bias',\n",
       "              tensor([ 0.0378, -0.1110, -0.0430,  ..., -0.0438, -0.1028, -0.0422],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.v.weight',\n",
       "              tensor([[-0.0742, -0.0915, -0.0125,  ..., -0.0197,  0.0611, -0.0120],\n",
       "                      [ 0.0873, -0.0424,  0.0319,  ..., -0.0079,  0.0720,  0.0283],\n",
       "                      [-0.0149,  0.0278, -0.0182,  ...,  0.0300,  0.0109, -0.0128],\n",
       "                      ...,\n",
       "                      [ 0.0593, -0.0644, -0.0089,  ...,  0.0255,  0.0841, -0.0572],\n",
       "                      [-0.0059, -0.0043, -0.0644,  ..., -0.0467, -0.0357, -0.0018],\n",
       "                      [-0.0886,  0.0063, -0.0321,  ...,  0.0338,  0.0412,  0.0648]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.v.bias',\n",
       "              tensor([ 0.0389,  0.0527,  0.0322,  ...,  0.0192, -0.0142,  0.0155],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.w2.weight',\n",
       "              tensor([[-0.0305,  0.0785,  0.0292,  ...,  0.0075, -0.0188,  0.0275],\n",
       "                      [ 0.0491,  0.0795,  0.0020,  ...,  0.0549,  0.0668, -0.0146],\n",
       "                      [ 0.0220, -0.0110, -0.0111,  ...,  0.0242, -0.0089,  0.0185],\n",
       "                      ...,\n",
       "                      [ 0.0379,  0.0046, -0.0365,  ..., -0.0111, -0.0868,  0.0169],\n",
       "                      [-0.0307,  0.0236,  0.0890,  ..., -0.0612, -0.0241, -0.0697],\n",
       "                      [ 0.0234,  0.0125,  0.0037,  ..., -0.0418, -0.0181, -0.0020]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.3.ffn.w2.bias',\n",
       "              tensor([ 9.6719e-03,  2.2678e-02, -1.7867e-03, -6.4069e-03,  7.6425e-02,\n",
       "                       2.4348e-02,  1.7362e-02, -9.1940e-03,  1.6948e-03,  4.3012e-03,\n",
       "                      -4.5287e-02, -4.6174e-02, -1.2284e-02, -1.2551e-02, -8.2031e-02,\n",
       "                      -7.5398e-02, -6.7008e-03,  1.3656e-02, -3.5944e-04, -1.9919e-02,\n",
       "                      -4.8486e-02,  4.1019e-03,  1.0901e-02,  2.2422e-03,  1.6927e-02,\n",
       "                      -4.8115e-02, -9.4145e-03, -4.8703e-02,  3.1090e-02, -5.4006e-02,\n",
       "                      -2.8207e-02, -5.7031e-02,  8.5912e-03, -1.9096e-02,  7.5394e-02,\n",
       "                       2.3943e-02, -1.4422e-04, -2.4517e-02,  4.4836e-02, -6.7166e-02,\n",
       "                      -6.3030e-03,  7.3881e-02,  2.2324e-02,  6.4169e-02, -4.7638e-02,\n",
       "                       3.3174e-03,  4.0663e-02, -9.1628e-03, -7.0487e-02, -7.0498e-02,\n",
       "                       3.0438e-03, -3.7542e-02, -3.8380e-02,  2.9443e-02,  9.7699e-03,\n",
       "                      -3.5297e-02,  5.3413e-02,  9.1905e-02,  3.0976e-02,  1.5178e-02,\n",
       "                      -1.1450e-02, -5.3216e-02, -1.1194e-01,  5.6953e-02,  1.1162e-01,\n",
       "                       5.4705e-02, -2.2255e-02, -4.7690e-02, -1.1446e-01, -8.8633e-03,\n",
       "                      -1.0576e-02, -2.7671e-02, -2.3044e-02, -2.3670e-02, -2.2662e-02,\n",
       "                      -1.0103e-02, -1.6624e-02, -3.8624e-02,  2.0290e-02, -3.6183e-03,\n",
       "                       1.5145e-02,  7.5216e-02, -1.1648e-01,  4.9297e-02, -2.4684e-02,\n",
       "                      -1.1122e-01, -1.2703e-02, -2.1156e-02, -2.0286e-02,  3.3517e-02,\n",
       "                      -7.7375e-02,  3.2104e-02,  2.1280e-02,  6.2563e-02,  1.1423e-02,\n",
       "                      -5.3360e-02, -4.2687e-02, -2.4133e-02, -2.7961e-02, -1.6432e-03,\n",
       "                      -5.4664e-02,  2.0816e-02, -7.9696e-02,  2.9525e-02,  1.5606e-02,\n",
       "                       2.4915e-02,  2.1369e-03, -5.5096e-03, -4.6946e-02, -5.6286e-03,\n",
       "                       1.9143e-02, -2.9384e-02,  6.5959e-03,  2.7653e-02, -4.7860e-02,\n",
       "                       3.0627e-02,  1.3086e-02,  1.3192e-02,  4.8168e-03, -3.2530e-02,\n",
       "                      -3.3816e-02,  1.3847e-02,  3.1057e-02, -6.1029e-02, -5.8813e-02,\n",
       "                      -6.7894e-02, -2.0734e-02,  4.7277e-03, -8.0735e-02,  4.5936e-02,\n",
       "                      -5.2576e-02, -1.2338e-02,  6.8515e-02, -7.3516e-02, -2.4670e-02,\n",
       "                      -6.9145e-02, -2.9584e-02,  1.8935e-02,  4.1722e-02, -6.9999e-02,\n",
       "                       5.1825e-02,  5.1347e-03, -3.9862e-02,  2.5922e-02,  4.2940e-03,\n",
       "                       1.4722e-02, -1.5035e-02,  1.1511e-02, -3.5458e-02, -1.0678e-01,\n",
       "                       5.6687e-02, -3.9170e-02,  4.2451e-02,  1.5467e-04, -1.1458e-01,\n",
       "                      -4.3238e-02, -3.6999e-02, -2.5951e-02, -2.4184e-02,  5.2104e-02,\n",
       "                      -7.0443e-02, -4.9392e-03, -3.5928e-02, -4.6339e-02, -4.8546e-03,\n",
       "                       2.1154e-02, -1.9555e-02, -1.3611e-02,  9.7211e-02, -6.6751e-03,\n",
       "                      -1.0196e-01,  1.0647e-03, -3.4983e-02,  1.5828e-01, -5.2043e-03,\n",
       "                       1.7163e-02, -1.0634e-02, -2.6510e-03, -2.3634e-02,  8.9398e-03,\n",
       "                      -1.0530e-01, -5.9976e-02, -6.9795e-02, -3.0298e-02, -6.7180e-03,\n",
       "                      -8.8946e-03,  5.5042e-02, -4.7900e-02, -3.3237e-02, -7.4239e-02,\n",
       "                      -2.9160e-04, -7.8357e-02, -1.9573e-02,  3.8867e-02, -4.3923e-03,\n",
       "                      -3.3812e-03, -9.6234e-02, -6.9274e-03,  2.7707e-02,  1.1544e-02,\n",
       "                      -1.7656e-02, -3.0246e-02, -6.2553e-02,  7.7090e-02, -1.4248e-02,\n",
       "                       4.9483e-02, -6.3701e-02, -1.1173e-01, -7.9373e-02, -4.1162e-02,\n",
       "                      -2.4631e-02, -3.5467e-02,  3.3867e-02, -1.6205e-02, -7.5006e-02,\n",
       "                      -1.0732e-02,  1.1230e-01,  1.2984e-02,  2.1094e-02, -2.3028e-02,\n",
       "                       2.3621e-03, -4.0393e-02,  5.0358e-02, -2.8335e-02,  3.7458e-02,\n",
       "                      -4.1030e-02, -7.4632e-01,  2.7402e-02,  6.0423e-02, -3.4710e-02,\n",
       "                       2.5556e-02, -3.8351e-02, -7.1867e-02,  4.8745e-02,  1.0023e-02,\n",
       "                       5.6562e-02, -2.7573e-02, -2.0579e-02, -8.1766e-02,  1.3700e-02,\n",
       "                       4.2788e-02, -5.5353e-02, -9.0377e-03, -2.1162e-02,  3.9569e-02,\n",
       "                       1.1722e-02,  2.4997e-02,  4.7251e-02, -6.6346e-02, -1.3509e-02,\n",
       "                       1.8378e-02, -1.2049e-02, -6.9677e-03,  3.0587e-02, -4.1481e-02,\n",
       "                       9.6000e-02, -2.0097e-02,  4.6764e-03, -1.0215e-01, -1.0612e-02,\n",
       "                       1.4274e-02,  3.0897e-02,  1.4623e-01, -3.9955e-02, -5.0698e-02,\n",
       "                       5.7952e-02, -3.4046e-03,  9.1966e-02,  4.4962e-02, -1.1323e-02,\n",
       "                       5.5038e-02, -9.2380e-02,  2.1756e-03, -1.6197e-02,  1.8871e-03,\n",
       "                      -2.6688e-02, -1.6795e-02, -3.2074e-02, -3.9694e-02, -1.6537e-02,\n",
       "                       1.5355e-02, -5.8608e-03,  1.0637e-01, -9.8242e-03, -9.5324e-02,\n",
       "                      -1.5303e-02, -3.5208e-02,  2.1746e-02,  4.8994e-02, -4.7854e-02,\n",
       "                       5.6951e-02,  4.8812e-02,  4.7448e-03,  2.7496e-03, -6.1040e-02,\n",
       "                      -9.8133e-03, -3.7179e-02,  5.2764e-02, -6.5375e-02, -1.2643e-02,\n",
       "                       1.4612e-02,  1.7010e-01,  5.1642e-02, -5.0716e-02, -2.8550e-02,\n",
       "                      -1.2003e-02, -1.5544e-01, -4.0450e-02, -1.7162e-02,  4.2723e-03,\n",
       "                       1.3982e-01,  1.3728e-04,  4.7629e-02,  4.0720e-04, -9.7459e-02,\n",
       "                       1.7700e-02,  1.9587e-02, -8.1543e-02,  2.6039e-02, -2.3091e-02,\n",
       "                       1.5769e-01,  2.1718e-02, -2.0651e-02,  8.6238e-03, -5.1068e-02,\n",
       "                      -8.7179e-03,  1.7637e-02,  4.0074e-02, -5.6077e-02, -3.9650e-03,\n",
       "                       2.7474e-02,  6.3753e-02, -1.3878e-02,  2.0388e-02,  4.2753e-02,\n",
       "                       2.4258e-02, -7.3492e-02,  4.2375e-02, -1.4711e-02,  4.0604e-02,\n",
       "                       7.7312e-02,  1.4407e-02, -2.9118e-02,  2.6988e-02, -6.5396e-02,\n",
       "                       2.9364e-02, -9.4482e-02, -4.3715e-02, -2.2413e-02,  2.4687e-02,\n",
       "                      -2.2751e-02,  2.7430e-03,  7.4662e-02, -4.2786e-02,  9.6144e-02,\n",
       "                       2.5289e-02, -6.7288e-02,  1.2318e-02, -6.0282e-02, -2.7813e-02,\n",
       "                       7.1464e-02,  1.8045e-02,  5.4378e-02,  4.6410e-03, -4.7492e-03,\n",
       "                       1.8478e-02, -2.2488e-03, -1.6987e-02, -3.9330e-02, -1.7847e-02,\n",
       "                      -4.9802e-02,  6.5388e-02,  1.4980e-02, -4.9234e-02, -4.2729e-03,\n",
       "                       8.5772e-02, -2.6578e-02,  2.6175e-02, -1.5973e-02, -4.3330e-02,\n",
       "                      -9.5972e-02,  5.6866e-02, -1.1528e-02,  6.1270e-02,  3.9694e-02,\n",
       "                       2.7585e-02, -8.8068e-02,  9.9998e-04,  1.0949e-02,  2.0422e-02,\n",
       "                      -4.5655e-02,  8.6103e-02, -1.9831e-02, -4.5139e-02, -4.3466e-02,\n",
       "                       2.9453e-02, -9.7043e-02,  8.6884e-02,  3.5365e-02,  5.3952e-02,\n",
       "                      -1.3067e-01,  1.3871e-03, -2.4194e-02,  5.6123e-02, -7.4925e-02,\n",
       "                      -4.5862e-04, -1.7602e-02,  4.7559e-02, -8.4128e-01, -8.0481e-02,\n",
       "                       5.7020e-02,  4.3802e-02, -2.6334e-02,  5.1712e-02,  3.9874e-02,\n",
       "                      -8.5660e-02, -8.0257e-03, -4.5158e-02,  1.6341e-02, -4.7091e-02,\n",
       "                       9.5160e-02, -1.9167e-02,  7.6531e-03, -4.7056e-02,  1.0712e-02,\n",
       "                      -1.2348e-02, -8.5495e-02, -4.7435e-02, -9.0508e-03, -6.1359e-02,\n",
       "                       7.7008e-03,  2.9766e-02,  7.5929e-02, -3.6843e-02,  1.5392e-01,\n",
       "                      -4.9808e-02, -1.5837e-03,  8.4247e-03, -6.2643e-02, -9.2602e-03,\n",
       "                       2.2723e-02, -5.3416e-03,  6.0600e-03, -2.5985e-02, -6.9887e-02,\n",
       "                       1.8304e-02, -7.7581e-03, -2.3821e-02,  1.1766e-01,  1.2086e-02,\n",
       "                       5.4630e-02, -2.2792e-02,  5.2871e-02,  6.0259e-02, -6.7997e-02,\n",
       "                       3.6131e-02, -2.3269e-02, -5.7818e-02,  5.6788e-02, -5.5448e-02,\n",
       "                       1.5316e-02, -1.5302e-02,  3.4511e-02,  5.1052e-02, -5.0993e-03,\n",
       "                      -9.8757e-04,  5.8304e-02, -2.8835e-02,  6.3417e-03,  1.5480e-02,\n",
       "                       1.6680e-02,  1.1031e-02,  3.9202e-02, -2.5657e-02,  3.0740e-02,\n",
       "                      -5.6187e-02, -2.2299e-02,  5.2408e-03,  2.6203e-02,  4.4050e-03,\n",
       "                       3.5236e-02,  6.1037e-02, -4.7833e-02, -2.1258e-03, -7.3415e-02,\n",
       "                       1.0703e-02, -6.8501e-02, -2.0950e-03, -1.1127e-02,  5.9695e-04,\n",
       "                      -5.6521e-02, -1.5862e-03,  4.6633e-02,  2.7505e-02,  4.6884e-02,\n",
       "                      -5.1535e-02, -8.5244e-03,  1.4095e-02,  3.5807e-01, -7.1001e-02,\n",
       "                       2.0273e-02, -3.5357e-02, -2.0518e-02,  6.3827e-02, -5.9137e-02,\n",
       "                      -1.3735e-02, -2.2553e-02, -9.3409e-02,  7.2728e-02, -3.4215e-02,\n",
       "                      -1.3809e-02,  8.1857e-02,  6.1388e-02, -3.0255e-02, -2.3370e-02,\n",
       "                      -3.5821e-02,  6.7661e-02, -5.7603e-02, -4.4184e-02, -5.0535e-02,\n",
       "                      -4.0850e-02, -1.3783e-02,  2.5033e-02,  2.1911e-02, -3.4112e-02,\n",
       "                      -1.8851e-03, -7.8467e-02, -8.6739e-02, -4.0071e-02,  5.1230e-02,\n",
       "                      -2.7227e-02, -1.6417e-02, -2.0250e-02,  8.3954e-02,  5.7281e-02,\n",
       "                       1.5956e-03,  4.6488e-02, -1.6008e-02,  3.5711e-02, -6.0204e-02,\n",
       "                      -8.1774e-02, -2.4607e-02, -8.0307e-02, -3.1516e-03, -4.7263e-02,\n",
       "                       1.9269e-02,  3.3141e-02, -4.2194e-02,  3.7215e-02, -8.5876e-02,\n",
       "                      -4.8652e-02, -8.7956e-03,  3.2149e-02,  1.3712e-02, -8.4297e-04,\n",
       "                      -5.4970e-02,  2.3456e-02,  7.2215e-02, -4.4347e-02, -6.6235e-02,\n",
       "                       5.4067e-02, -4.5801e-03, -2.1344e-02,  1.8361e-02, -3.5758e-02,\n",
       "                      -7.3813e-03,  1.5748e-02, -6.0049e-02,  6.2884e-02, -8.1990e-02,\n",
       "                       1.5185e-01, -8.0485e-02,  8.9332e-02,  6.0336e-02, -9.1342e-02,\n",
       "                       6.1149e-03, -8.3512e-02, -1.8441e-02, -5.9177e-02, -6.5509e-02,\n",
       "                       4.2054e-03, -5.9312e-03,  1.8403e-02, -3.9050e-02,  1.7031e-02,\n",
       "                      -3.7043e-02, -3.2957e-02,  9.7812e-02,  3.2865e-02, -4.5263e-02,\n",
       "                      -1.2181e-02, -9.3605e-02, -2.9426e-02,  3.9189e-02, -2.3478e-02,\n",
       "                       3.9017e-02, -2.8189e-02,  3.5782e-03,  2.9980e-02, -1.2776e-02,\n",
       "                      -1.6261e-02,  1.8893e-02, -1.1757e-02,  3.0032e-02,  3.0277e-04,\n",
       "                       1.4052e-02,  1.0692e-03, -1.2992e-02,  1.7526e-02, -5.6811e-02,\n",
       "                      -1.2770e-03, -7.4674e-02, -3.8561e-03,  9.8633e-02, -5.1139e-02,\n",
       "                      -9.7238e-02, -1.8581e-02, -5.4230e-02,  1.3954e-02, -5.8076e-02,\n",
       "                      -7.1569e-03,  4.7306e-02, -7.5561e-03, -2.9224e-02,  3.9084e-02,\n",
       "                       1.3358e-02,  4.1475e-03, -2.2947e-02,  8.4633e-02,  4.3245e-02,\n",
       "                       3.3890e-02,  1.2194e-02, -4.6145e-02,  2.5341e-03,  3.2278e-02,\n",
       "                       9.2304e-02, -3.3685e-02,  2.7088e-02, -7.0371e-02, -8.5503e-02,\n",
       "                       3.3201e-02,  2.2504e-02,  1.6234e-02,  3.4824e-03,  5.4983e-02,\n",
       "                       7.1108e-02,  6.8811e-02, -8.5597e-03, -2.7094e-02,  1.5508e-02,\n",
       "                       1.2367e-02,  1.7590e-02, -9.6990e-02,  1.0612e-02,  7.4660e-02,\n",
       "                      -4.1773e-02, -6.6935e-02,  2.4254e-02, -2.6467e-02,  1.9375e-02,\n",
       "                      -1.2140e-02, -6.0627e-02, -7.4000e-02,  4.0308e-02,  5.6745e-02,\n",
       "                       3.3694e-02,  1.1541e-02,  4.4178e-03,  6.1200e-02,  1.6349e-01,\n",
       "                      -4.3252e-02,  4.0773e-02, -4.8867e-04, -4.3012e-02, -2.6886e-02,\n",
       "                       9.2340e-02, -9.0153e-02, -8.6947e-02,  1.4576e-02, -3.3112e-03,\n",
       "                       1.0428e-03, -9.5128e-03, -1.5975e-02,  1.1151e-01, -3.6182e-02,\n",
       "                       2.4038e-02, -7.2333e-02, -1.4791e-01,  4.2501e-02,  2.2236e-02,\n",
       "                      -1.0122e-01,  5.4490e-02, -2.5853e-02,  2.1776e-02,  9.1593e-02,\n",
       "                      -5.5692e-02, -2.3229e-02, -9.4868e-03,  1.1199e-01,  9.1398e-03,\n",
       "                       2.5226e-02, -1.0609e-01,  1.4103e-02,  9.4550e-02, -6.2557e-02,\n",
       "                       3.8153e-02,  5.0335e-02, -1.9118e-02,  2.9376e-03, -1.0451e-02,\n",
       "                      -3.6021e-02, -1.9843e-03, -9.4992e-03,  6.3819e-03,  7.1813e-02,\n",
       "                       6.6413e-03,  7.8054e-02, -5.4064e-02, -1.3865e-02, -8.1435e-03,\n",
       "                       4.8010e-02,  3.5941e-02, -6.1694e-02,  3.6260e-02, -5.4001e-02,\n",
       "                      -4.8210e-02, -7.8314e-02, -5.8534e-02,  6.3714e-02, -4.7164e-02,\n",
       "                      -3.6891e-02,  4.9810e-02, -3.0294e-02, -3.2920e-02,  1.2895e-01,\n",
       "                       3.7429e-02,  8.5329e-02,  6.5271e-02,  1.0115e-02, -1.1451e-02,\n",
       "                      -2.3320e-02, -4.9630e-02,  2.7817e-02,  4.2781e-02,  4.0975e-02,\n",
       "                      -1.2096e-02,  6.1739e-02,  1.4899e-02, -3.8077e-02,  1.2271e-03,\n",
       "                      -5.0250e-02,  1.8563e-02, -5.0935e-02,  4.3802e-03, -5.6040e-02,\n",
       "                       5.0264e-03,  3.5319e-02,  1.3189e-02,  7.9489e-03, -4.9780e-03,\n",
       "                       3.8550e-02,  5.3766e-02, -3.0944e-02, -5.3914e-02, -3.1223e-02,\n",
       "                      -8.8354e-02,  3.1940e-03, -2.8866e-02], device='cuda:0')),\n",
       "             ('layers.3.anorm.weight',\n",
       "              tensor([0.7810, 0.7958, 0.8388, 0.8518, 0.7904, 0.8337, 0.7856, 0.7549, 0.8047,\n",
       "                      0.7933, 0.8771, 0.7941, 0.8225, 0.8019, 0.7993, 0.7977, 0.8258, 0.7797,\n",
       "                      0.8172, 0.8471, 0.8487, 0.8477, 0.8133, 0.7482, 0.7250, 0.7346, 0.7813,\n",
       "                      0.8163, 0.7618, 0.8597, 0.8018, 0.8319, 0.7883, 0.8600, 0.8848, 0.7761,\n",
       "                      0.8377, 0.7669, 0.7641, 0.7744, 0.8066, 0.7721, 0.8067, 0.8227, 0.8353,\n",
       "                      0.7925, 0.8459, 0.7751, 0.8253, 0.8693, 0.8387, 0.8084, 0.8274, 0.7622,\n",
       "                      0.7697, 0.8010, 0.8213, 0.8639, 0.7613, 0.8055, 0.7491, 0.8301, 0.8982,\n",
       "                      0.7863, 0.8018, 0.8415, 0.7922, 0.8437, 0.8060, 0.7845, 0.8165, 0.8077,\n",
       "                      0.7851, 0.8203, 0.7944, 0.7638, 0.8030, 0.8468, 0.8157, 0.8533, 0.8796,\n",
       "                      0.8020, 0.8076, 0.8570, 0.8208, 0.8106, 0.8371, 0.8136, 0.8206, 0.7884,\n",
       "                      0.7674, 0.8106, 0.8834, 0.8584, 0.7833, 0.8210, 0.8121, 0.7983, 0.8041,\n",
       "                      0.8707, 0.7787, 0.8578, 0.8426, 0.7399, 0.8070, 0.8490, 0.8238, 0.8163,\n",
       "                      0.8104, 0.7673, 0.8470, 0.7611, 0.8795, 0.8149, 0.7420, 0.8067, 0.8099,\n",
       "                      0.7298, 0.8327, 0.8493, 0.8266, 0.8509, 0.8343, 0.8456, 0.8038, 0.8082,\n",
       "                      0.8049, 0.8322, 0.8042, 0.7783, 0.8105, 0.8382, 0.7805, 0.8238, 0.8051,\n",
       "                      0.7911, 0.7792, 0.7581, 0.8250, 0.8012, 0.8368, 0.8271, 0.8268, 0.8008,\n",
       "                      0.7833, 0.7633, 0.8696, 0.8454, 0.7805, 0.8155, 0.7575, 0.8430, 0.8708,\n",
       "                      0.7882, 0.7144, 0.8067, 0.7829, 0.7633, 0.8274, 0.7533, 0.8928, 0.8072,\n",
       "                      0.8194, 0.7854, 0.7646, 0.8185, 0.7961, 0.7621, 0.7395, 0.8306, 0.8036,\n",
       "                      0.7897, 0.8417, 0.8081, 0.7767, 0.8107, 0.8303, 0.8070, 0.7697, 0.8089,\n",
       "                      0.8539, 0.8576, 0.8004, 0.7878, 0.8173, 0.8175, 0.8182, 0.7870, 0.8440,\n",
       "                      0.8347, 0.7739, 0.8218, 0.8778, 0.8116, 0.8360, 0.7742, 0.7624, 0.8389,\n",
       "                      0.8198, 0.8049, 0.7797, 0.8807, 0.8357, 0.7531, 0.7557, 0.8293, 0.8008,\n",
       "                      0.8292, 0.8378, 0.7197, 0.8018, 0.7513, 0.8146, 0.8014, 0.7455, 0.8394,\n",
       "                      0.8106, 0.7782, 0.7355, 0.8450, 0.7908, 0.7936, 0.7777, 0.8179, 0.7751,\n",
       "                      0.7962, 0.9031, 0.8533, 0.7593, 0.8137, 0.8097, 0.7701, 0.7787, 0.7882,\n",
       "                      0.8160, 0.8801, 0.7868, 0.7622, 0.8491, 0.6896, 0.7392, 0.7731, 0.8033,\n",
       "                      0.8154, 0.8128, 0.7562, 0.8258, 0.7752, 0.8313, 0.7989, 0.7984, 0.7784,\n",
       "                      0.8272, 0.8368, 0.7609, 0.7815, 0.8763, 0.8075, 0.7733, 0.8573, 0.8494,\n",
       "                      0.7834, 0.8790, 0.7856, 0.7770, 0.7118, 0.7686, 0.8571, 0.8198, 0.8482,\n",
       "                      0.8287, 0.7692, 0.7813, 0.8123, 0.8201, 0.8138, 0.7847, 0.7908, 0.8228,\n",
       "                      0.7696, 0.8266, 0.8362, 0.8587, 0.7963, 0.8782, 0.7904, 0.7705, 0.7791,\n",
       "                      0.7729, 0.8127, 0.7937, 0.8144, 0.7819, 0.7489, 0.8167, 0.8196, 0.7630,\n",
       "                      0.8085, 0.8025, 0.8188, 0.8270, 0.7864, 0.7913, 0.8125, 0.7868, 0.8300,\n",
       "                      0.7976, 0.8058, 0.8182, 0.8405, 0.7785, 0.8288, 0.8417, 0.7070, 0.7644,\n",
       "                      0.7972, 0.7926, 0.8647, 0.8403, 0.8052, 0.8151, 0.8831, 0.8173, 0.8438,\n",
       "                      0.8010, 0.7908, 0.7599, 0.7969, 0.8274, 0.7568, 0.8054, 0.7932, 0.7942,\n",
       "                      0.7957, 0.8052, 0.8840, 0.8159, 0.7396, 0.8403, 0.7670, 0.7742, 0.7771,\n",
       "                      0.7414, 0.8161, 0.7084, 0.8115, 0.7739, 0.8452, 0.8449, 0.8056, 0.8468,\n",
       "                      0.8364, 0.8283, 0.8365, 0.8813, 0.8459, 0.8567, 0.8559, 0.7882, 0.8461,\n",
       "                      0.7970, 0.8428, 0.8578, 0.7452, 0.7790, 0.7468, 0.7983, 0.8162, 0.8031,\n",
       "                      0.7361, 0.8273, 0.8026, 0.8300, 0.7665, 0.7928, 0.7037, 0.8268, 0.8326,\n",
       "                      0.8327, 0.7428, 0.8156, 0.7679, 0.8543, 0.8301, 0.8272, 0.7918, 0.7854,\n",
       "                      0.7928, 0.7513, 0.8024, 0.7709, 0.8413, 0.8434, 0.8002, 0.8160, 0.8583,\n",
       "                      0.7919, 0.8583, 0.8247, 0.7894, 0.7473, 0.7767, 0.7719, 0.8322, 0.7863,\n",
       "                      0.8088, 0.8567, 0.8096, 0.4987, 0.8902, 0.8069, 0.7042, 0.7960, 0.7532,\n",
       "                      0.8325, 0.7538, 0.8307, 0.7344, 0.7784, 0.7682, 0.8441, 0.6672, 0.8955,\n",
       "                      0.8050, 0.8643, 0.8024, 0.7478, 0.7416, 0.7705, 0.8615, 0.7622, 0.8056,\n",
       "                      0.7641, 0.7847, 0.7749, 0.8246, 0.8277, 0.8079, 0.8382, 0.7372, 0.8001,\n",
       "                      0.7844, 0.8414, 0.8040, 0.8227, 0.8241, 0.8181, 0.7589, 0.8358, 0.7832,\n",
       "                      0.7497, 0.8570, 0.7667, 0.8052, 0.8328, 0.7587, 0.7772, 0.7991, 0.8326,\n",
       "                      0.7686, 0.7580, 0.7743, 0.8446, 0.7684, 0.8076, 0.7724, 0.8092, 0.7890,\n",
       "                      0.8221, 0.7694, 0.7734, 0.7887, 0.8179, 0.7855, 0.8566, 0.8074, 0.7741,\n",
       "                      0.8073, 0.7687, 0.7854, 0.8170, 0.8262, 0.8670, 0.8007, 0.8647, 0.8700,\n",
       "                      0.7993, 0.7947, 0.8471, 0.7460, 0.7941, 0.8442, 0.7979, 0.7654, 0.8060,\n",
       "                      0.7794, 0.7127, 0.7492, 0.5382, 0.8129, 0.7973, 0.8121, 0.7977, 0.8098,\n",
       "                      0.8268, 0.8269, 0.7557, 0.7625, 0.8142, 0.7724, 0.8053, 0.8010, 0.7610,\n",
       "                      0.8113, 0.8182, 0.7807, 0.8798, 0.7989, 0.7917, 0.8047, 0.7878, 0.8257,\n",
       "                      0.7650, 0.7506, 0.8077, 0.7572, 0.7916, 0.8363, 0.7458, 0.7899, 0.7801,\n",
       "                      0.8143, 0.8017, 0.8327, 0.7811, 0.8516, 0.8299, 0.8245, 0.7858, 0.8228,\n",
       "                      0.8064, 0.8416, 0.7865, 0.7632, 0.8084, 0.7920, 0.8483, 0.8301, 0.8445,\n",
       "                      0.7950, 0.8264, 0.7677, 0.8274, 0.7695, 0.7932, 0.7723, 0.7948, 0.7791,\n",
       "                      0.7767, 0.7461, 0.8010, 0.8395, 0.7685, 0.7922, 0.8043, 0.7457, 0.8391,\n",
       "                      0.8539, 0.8156, 0.8680, 0.8549, 0.8155, 0.8126, 0.7591, 0.7971, 0.8021,\n",
       "                      0.8689, 0.8093, 0.7794, 0.7710, 0.8162, 0.8227, 0.8507, 0.7998, 0.8209,\n",
       "                      0.8293, 0.8412, 0.8207, 0.7786, 0.8759, 0.8087, 0.8009, 0.8553, 0.7817,\n",
       "                      0.8359, 0.8127, 0.8230, 0.8020, 0.7489, 0.8170, 0.7951, 0.8195, 0.8416,\n",
       "                      0.7752, 0.8166, 0.7929, 0.8917, 0.7695, 0.7561, 0.8120, 0.7841, 0.8216,\n",
       "                      0.8324, 0.7975, 0.7701, 0.7978, 0.8146, 0.7901, 0.7933, 0.8497, 0.8068,\n",
       "                      0.7858, 0.7889, 0.8235, 0.8487, 0.7656, 0.8387, 0.8442, 0.8008, 0.8066,\n",
       "                      0.7894, 0.8322, 0.8380, 0.7668, 0.8335, 0.8543, 0.8221, 0.7903, 0.8420,\n",
       "                      0.7690, 0.7663, 0.8414, 0.8826, 0.8128, 0.8424, 0.8829, 0.8477, 0.8132,\n",
       "                      0.8022, 0.7844, 0.8018, 0.7945, 0.7913, 0.7733, 0.8413, 0.7866, 0.8207,\n",
       "                      0.7905, 0.8653, 0.7733, 0.7901, 0.8239, 0.8716, 0.8662, 0.7574, 0.8406,\n",
       "                      0.8269, 0.7775, 0.7583, 0.7699, 0.7948, 0.7880, 0.7933, 0.7998, 0.8001,\n",
       "                      0.8158, 0.8101, 0.8987, 0.7592, 0.8199, 0.7366, 0.8577, 0.8201, 0.8347,\n",
       "                      0.8393, 0.8381, 0.8062, 0.7983, 0.8322, 0.8035, 0.8156, 0.8337, 0.7169,\n",
       "                      0.7700, 0.7904, 0.8278, 0.8164, 0.8091, 0.8086, 0.8504, 0.8191, 0.8047,\n",
       "                      0.8564, 0.8103, 0.7869, 0.7990, 0.8202, 0.8384, 0.7761, 0.8089, 0.7966,\n",
       "                      0.7023, 0.7462, 0.8360, 0.8119, 0.7780, 0.8096, 0.8053, 0.8185, 0.8091,\n",
       "                      0.8143, 0.8118, 0.8275, 0.8148, 0.8515, 0.8255, 0.7932, 0.7395, 0.8324,\n",
       "                      0.7802, 0.7913, 0.8109, 0.7964, 0.8169, 0.8568, 0.7987, 0.8594, 0.7694,\n",
       "                      0.8069, 0.8340, 0.7796, 0.8381, 0.8622, 0.7755, 0.8305, 0.8215, 0.8791,\n",
       "                      0.8682, 0.8367, 0.8278, 0.7260, 0.8850, 0.8258, 0.8333, 0.7990, 0.8297,\n",
       "                      0.7247, 0.8175, 0.8238, 0.7763, 0.8256, 0.7422, 0.8502, 0.7695, 0.7541,\n",
       "                      0.8626, 0.8078, 0.8559], device='cuda:0')),\n",
       "             ('layers.3.fnorm.weight',\n",
       "              tensor([0.7683, 0.7850, 0.8068, 0.7524, 0.8028, 0.7496, 0.8472, 0.7606, 0.7500,\n",
       "                      0.6938, 0.7210, 0.7641, 0.7829, 0.7534, 0.7515, 0.7498, 0.7494, 0.7696,\n",
       "                      0.7268, 0.8162, 0.8105, 0.7132, 0.7674, 0.8058, 0.7948, 0.7658, 0.7377,\n",
       "                      0.7371, 0.7837, 0.7474, 0.7669, 0.7181, 0.6507, 0.7628, 0.7698, 0.7401,\n",
       "                      0.7181, 0.6955, 0.7471, 0.7371, 0.7450, 0.7989, 0.7836, 0.7402, 0.7015,\n",
       "                      0.7304, 0.8225, 0.7295, 0.6580, 0.7752, 0.7925, 0.7002, 0.6986, 0.7703,\n",
       "                      0.7151, 0.7884, 0.7431, 0.7189, 0.6963, 0.6968, 0.6860, 0.7942, 0.7555,\n",
       "                      0.7139, 0.6938, 0.7907, 0.8219, 0.8247, 0.7472, 0.6914, 0.7396, 0.8016,\n",
       "                      0.7692, 0.7641, 0.7338, 0.7603, 0.7503, 0.7729, 0.7418, 0.7516, 0.7814,\n",
       "                      0.7705, 0.5803, 0.7188, 0.7510, 0.7234, 0.7621, 0.7252, 0.7700, 0.7408,\n",
       "                      0.7751, 0.7337, 0.7148, 0.7311, 0.7002, 0.7334, 0.7439, 0.6932, 0.7140,\n",
       "                      0.7890, 0.7389, 0.8129, 0.7849, 0.6555, 0.7041, 0.7449, 0.7269, 0.7036,\n",
       "                      0.7430, 0.7102, 0.7473, 0.6895, 0.7763, 0.7556, 0.7752, 0.6899, 0.7557,\n",
       "                      0.7178, 0.7400, 0.7506, 0.7627, 0.7945, 0.7717, 0.7382, 0.7941, 0.7697,\n",
       "                      0.7254, 0.7994, 0.7448, 0.7322, 0.7556, 0.8369, 0.6928, 0.8044, 0.7401,\n",
       "                      0.7158, 0.7718, 0.6922, 0.7394, 0.7358, 0.7853, 0.7635, 0.7948, 0.7330,\n",
       "                      0.8037, 0.7535, 0.7133, 0.7588, 0.7290, 0.7778, 0.7840, 0.7128, 0.7787,\n",
       "                      0.7230, 0.6609, 0.7514, 0.7283, 0.7746, 0.7785, 0.7846, 0.7554, 0.7266,\n",
       "                      0.8081, 0.7185, 0.7866, 0.7814, 0.6597, 0.7182, 0.7188, 0.7597, 0.7590,\n",
       "                      0.7184, 0.7195, 0.7180, 0.7689, 0.7119, 0.7240, 0.7493, 0.6703, 0.7149,\n",
       "                      0.7825, 0.7685, 0.7813, 0.7974, 0.7380, 0.8106, 0.7071, 0.7464, 0.7419,\n",
       "                      0.7520, 0.7758, 0.7297, 0.7389, 0.7736, 0.7657, 0.7676, 0.7233, 0.7431,\n",
       "                      0.7492, 0.7158, 0.7407, 0.7032, 0.7859, 0.7135, 0.7121, 0.7216, 0.7783,\n",
       "                      0.7968, 0.8153, 0.7132, 0.7382, 0.7324, 0.7134, 0.7080, 0.7740, 0.7517,\n",
       "                      0.7034, 0.7773, 0.7163, 0.7601, 0.7807, 0.7103, 0.7565, 0.7343, 0.7663,\n",
       "                      0.7976, 0.1648, 0.7581, 0.7494, 0.7622, 0.7381, 0.7032, 0.7013, 0.6865,\n",
       "                      0.7947, 0.7594, 0.6706, 0.7346, 0.7284, 0.7298, 0.6955, 0.7557, 0.7766,\n",
       "                      0.7577, 0.7498, 0.7811, 0.7051, 0.6901, 0.7825, 0.7464, 0.7758, 0.7427,\n",
       "                      0.7938, 0.7525, 0.6869, 0.7325, 0.7821, 0.7278, 0.7954, 0.7383, 0.7719,\n",
       "                      0.6978, 0.8007, 0.7153, 0.7158, 0.7293, 0.7132, 0.7443, 0.7277, 0.7609,\n",
       "                      0.7424, 0.7783, 0.6962, 0.7982, 0.8170, 0.7580, 0.6784, 0.7126, 0.8129,\n",
       "                      0.7438, 0.7604, 0.7777, 0.7762, 0.7140, 0.7653, 0.7116, 0.7427, 0.7761,\n",
       "                      0.7174, 0.7378, 0.7913, 0.8226, 0.7708, 0.7154, 0.7133, 0.7243, 0.7323,\n",
       "                      0.7238, 0.7729, 0.7459, 0.7277, 0.6598, 0.6964, 0.7171, 0.7214, 0.7982,\n",
       "                      0.7432, 0.7121, 0.8357, 0.7849, 0.6565, 0.7740, 0.7725, 0.7539, 0.7662,\n",
       "                      0.7710, 0.7391, 0.7371, 0.7117, 0.7200, 0.8030, 0.8515, 0.7709, 0.7821,\n",
       "                      0.6941, 0.7996, 0.6586, 0.7157, 0.7219, 0.7083, 0.7522, 0.7853, 0.7502,\n",
       "                      0.6982, 0.7829, 0.7606, 0.7525, 0.7216, 0.7142, 0.7730, 0.7394, 0.6755,\n",
       "                      0.7191, 0.7736, 0.7284, 0.6873, 0.7395, 0.7126, 0.7741, 0.7393, 0.7348,\n",
       "                      0.7479, 0.8254, 0.7761, 0.7996, 0.7678, 0.6892, 0.7504, 0.7273, 0.7562,\n",
       "                      0.7105, 0.7028, 0.7799, 0.7277, 0.7937, 0.7384, 0.7384, 0.7392, 0.7205,\n",
       "                      0.7140, 0.6913, 0.7533, 0.7933, 0.7198, 0.7605, 0.7378, 0.7819, 0.7159,\n",
       "                      0.7705, 0.7315, 0.7970, 0.7183, 0.7473, 0.7228, 0.7941, 0.8084, 0.7252,\n",
       "                      0.7134, 0.7124, 0.7582, 0.7422, 0.6632, 0.7785, 0.7600, 0.7572, 0.7368,\n",
       "                      0.8262, 0.7616, 0.8086, 0.7180, 0.7204, 0.6835, 0.7718, 0.7327, 0.7094,\n",
       "                      0.8638, 0.7841, 0.7614, 0.1540, 0.7267, 0.7322, 0.6598, 0.7759, 0.7248,\n",
       "                      0.7252, 0.7418, 0.7667, 0.6744, 0.7465, 0.8330, 0.7265, 0.7534, 0.7460,\n",
       "                      0.7700, 0.7421, 0.7499, 0.7608, 0.7286, 0.7597, 0.7779, 0.7607, 0.7022,\n",
       "                      0.7006, 0.6660, 0.6991, 0.7520, 0.8146, 0.7139, 0.7797, 0.7200, 0.6956,\n",
       "                      0.7651, 0.7660, 0.7593, 0.7455, 0.7608, 0.7188, 0.7516, 0.8341, 0.7493,\n",
       "                      0.7137, 0.7713, 0.8118, 0.7915, 0.7546, 0.7190, 0.8049, 0.8041, 0.7390,\n",
       "                      0.7374, 0.6987, 0.7689, 0.7804, 0.7523, 0.7950, 0.7230, 0.7168, 0.7522,\n",
       "                      0.8150, 0.6995, 0.7408, 0.7086, 0.6608, 0.7820, 0.7581, 0.7850, 0.7657,\n",
       "                      0.7618, 0.7212, 0.7809, 0.7462, 0.7986, 0.7272, 0.7261, 0.7606, 0.7651,\n",
       "                      0.7866, 0.7793, 0.7148, 0.7105, 0.7853, 0.7611, 0.7691, 0.6795, 0.7435,\n",
       "                      0.7831, 0.7317, 0.7089, 0.4154, 0.7485, 0.7520, 0.7975, 0.7262, 0.7409,\n",
       "                      0.8056, 0.7396, 0.7874, 0.8178, 0.7827, 0.7367, 0.6790, 0.7577, 0.7129,\n",
       "                      0.7925, 0.7804, 0.7011, 0.7766, 0.7339, 0.7576, 0.7857, 0.7373, 0.7102,\n",
       "                      0.7420, 0.7748, 0.7759, 0.7578, 0.7712, 0.7608, 0.8168, 0.7835, 0.7315,\n",
       "                      0.7232, 0.7654, 0.7704, 0.7206, 0.6987, 0.7202, 0.7381, 0.7058, 0.7001,\n",
       "                      0.7353, 0.7754, 0.7935, 0.7803, 0.7835, 0.7844, 0.7246, 0.7461, 0.7672,\n",
       "                      0.7138, 0.8078, 0.6642, 0.7251, 0.7978, 0.7567, 0.7844, 0.7727, 0.8140,\n",
       "                      0.7028, 0.8152, 0.7177, 0.7169, 0.7366, 0.7721, 0.7398, 0.7069, 0.6971,\n",
       "                      0.7411, 0.7244, 0.7539, 0.7035, 0.7805, 0.8084, 0.7245, 0.7424, 0.7458,\n",
       "                      0.7680, 0.7958, 0.7232, 0.6848, 0.8220, 0.7076, 0.7424, 0.7075, 0.7427,\n",
       "                      0.7483, 0.7501, 0.8235, 0.7810, 0.7123, 0.7252, 0.7369, 0.7868, 0.7243,\n",
       "                      0.7193, 0.7334, 0.6951, 0.7377, 0.7677, 0.7697, 0.8272, 0.7585, 0.7667,\n",
       "                      0.7602, 0.6993, 0.7699, 0.7246, 0.7949, 0.6879, 0.7859, 0.7168, 0.7383,\n",
       "                      0.7411, 0.7820, 0.7181, 0.7338, 0.6560, 0.7539, 0.7269, 0.7370, 0.6866,\n",
       "                      0.8041, 0.7940, 0.7890, 0.7800, 0.7348, 0.7710, 0.7794, 0.7298, 0.7009,\n",
       "                      0.7643, 0.7221, 0.7127, 0.7381, 0.7416, 0.7859, 0.7198, 0.7236, 0.7136,\n",
       "                      0.6929, 0.7516, 0.7673, 0.7879, 0.6769, 0.7754, 0.7282, 0.8166, 0.7430,\n",
       "                      0.7606, 0.6891, 0.7400, 0.7952, 0.7793, 0.6923, 0.7678, 0.7340, 0.7086,\n",
       "                      0.7140, 0.7988, 0.7720, 0.7642, 0.7431, 0.7351, 0.8004, 0.7569, 0.7664,\n",
       "                      0.8053, 0.7419, 0.7695, 0.7088, 0.7522, 0.7438, 0.7633, 0.7346, 0.7036,\n",
       "                      0.7658, 0.7564, 0.7446, 0.7121, 0.7469, 0.7322, 0.7732, 0.8300, 0.7676,\n",
       "                      0.7436, 0.7358, 0.7602, 0.7771, 0.7104, 0.7005, 0.7222, 0.7598, 0.7696,\n",
       "                      0.6727, 0.7498, 0.7259, 0.7952, 0.7511, 0.8078, 0.7606, 0.7632, 0.7532,\n",
       "                      0.8062, 0.7680, 0.6987, 0.7140, 0.7247, 0.7121, 0.7490, 0.7483, 0.8236,\n",
       "                      0.6412, 0.7338, 0.7706, 0.6692, 0.7294, 0.7619, 0.7374, 0.6658, 0.7133,\n",
       "                      0.7381, 0.7301, 0.7671, 0.7681, 0.7085, 0.7679, 0.7249, 0.8118, 0.8051,\n",
       "                      0.7552, 0.7105, 0.7394, 0.7483, 0.7779, 0.7380, 0.7441, 0.7322, 0.7035,\n",
       "                      0.7692, 0.7888, 0.8163, 0.7788, 0.7736, 0.8017, 0.8518, 0.7138, 0.6964,\n",
       "                      0.7910, 0.7808, 0.7466, 0.6775, 0.7008, 0.6915, 0.7746, 0.7192, 0.7174,\n",
       "                      0.7493, 0.6687, 0.7772, 0.7667, 0.7270, 0.7731, 0.7773, 0.7365, 0.7418,\n",
       "                      0.7984, 0.7958, 0.7576], device='cuda:0')),\n",
       "             ('layers.4.mha.query.weight',\n",
       "              tensor([[-0.0324,  0.0271, -0.0223,  ..., -0.0267, -0.0190, -0.0513],\n",
       "                      [ 0.0172, -0.0149, -0.0415,  ..., -0.0832, -0.0048,  0.0005],\n",
       "                      [ 0.0541, -0.0031,  0.0294,  ...,  0.0043, -0.0001,  0.0353],\n",
       "                      ...,\n",
       "                      [-0.0703,  0.0139, -0.0462,  ...,  0.0167, -0.0067,  0.1243],\n",
       "                      [-0.0234,  0.0837,  0.0588,  ...,  0.0448,  0.0473,  0.0243],\n",
       "                      [ 0.0742,  0.0110,  0.0433,  ...,  0.0542, -0.0450, -0.0100]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.mha.key.weight',\n",
       "              tensor([[ 0.0356, -0.0292, -0.0314,  ..., -0.0275, -0.0172,  0.0227],\n",
       "                      [ 0.0176, -0.0050,  0.0205,  ..., -0.0138,  0.0279,  0.0091],\n",
       "                      [-0.0202,  0.0695,  0.0757,  ..., -0.0172, -0.0114,  0.0364],\n",
       "                      ...,\n",
       "                      [ 0.0254, -0.0046,  0.0464,  ..., -0.0167,  0.0246,  0.0100],\n",
       "                      [ 0.0203,  0.0588, -0.0848,  ...,  0.0508, -0.0099,  0.0158],\n",
       "                      [ 0.1031,  0.0214,  0.0161,  ..., -0.0143,  0.0010, -0.1244]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.mha.value.weight',\n",
       "              tensor([[ 0.0050, -0.0087, -0.0056,  ...,  0.0498, -0.0060, -0.0503],\n",
       "                      [-0.0443,  0.0797, -0.0124,  ...,  0.0889, -0.0583,  0.0055],\n",
       "                      [-0.0290,  0.0064,  0.0279,  ...,  0.0431,  0.0389, -0.0565],\n",
       "                      ...,\n",
       "                      [-0.0705,  0.0093, -0.0208,  ...,  0.0138,  0.0455, -0.0277],\n",
       "                      [ 0.0171, -0.0338, -0.0822,  ...,  0.0299,  0.0636,  0.0593],\n",
       "                      [ 0.0285, -0.0121,  0.0285,  ..., -0.0292,  0.0008, -0.0457]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.mha.proj.weight',\n",
       "              tensor([[ 0.0084, -0.1049,  0.0122,  ..., -0.0545,  0.0424,  0.0381],\n",
       "                      [-0.0812, -0.0085, -0.0113,  ..., -0.0186,  0.0635, -0.0187],\n",
       "                      [ 0.0441,  0.0553, -0.0132,  ...,  0.0047, -0.0256, -0.0038],\n",
       "                      ...,\n",
       "                      [-0.0283, -0.0825,  0.0721,  ..., -0.0151,  0.0232, -0.0106],\n",
       "                      [-0.0368, -0.0740, -0.0008,  ..., -0.0490, -0.0011, -0.0539],\n",
       "                      [-0.0192, -0.0130,  0.0148,  ...,  0.0095, -0.0603, -0.0789]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.w.weight',\n",
       "              tensor([[ 0.0673,  0.0115,  0.0424,  ..., -0.0610, -0.0003, -0.0247],\n",
       "                      [ 0.0470, -0.0440,  0.0535,  ..., -0.0874, -0.0270, -0.0181],\n",
       "                      [ 0.0237, -0.0184, -0.0045,  ...,  0.0390, -0.0330,  0.0396],\n",
       "                      ...,\n",
       "                      [ 0.0194,  0.0254,  0.0226,  ...,  0.0628,  0.0504,  0.0964],\n",
       "                      [ 0.0521, -0.0387, -0.0479,  ...,  0.0767,  0.0274,  0.0394],\n",
       "                      [-0.0388, -0.0021, -0.0055,  ..., -0.0288, -0.0802,  0.0306]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.w.bias',\n",
       "              tensor([-0.0871, -0.1137, -0.0949,  ..., -0.0893, -0.1287, -0.1027],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.v.weight',\n",
       "              tensor([[ 0.0133,  0.0556,  0.0155,  ..., -0.0154, -0.0179,  0.0525],\n",
       "                      [-0.0351,  0.0184, -0.0289,  ..., -0.0230,  0.0653,  0.0244],\n",
       "                      [ 0.1039,  0.0156,  0.0478,  ...,  0.0968, -0.0084, -0.0474],\n",
       "                      ...,\n",
       "                      [-0.0022, -0.0449, -0.0511,  ...,  0.0355,  0.0147, -0.0218],\n",
       "                      [ 0.0151,  0.0238, -0.0233,  ...,  0.0329, -0.0403, -0.0491],\n",
       "                      [-0.0581,  0.0515, -0.0041,  ...,  0.0080, -0.0522, -0.0408]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.v.bias',\n",
       "              tensor([ 0.0172, -0.0088,  0.0670,  ..., -0.0181,  0.0167,  0.0634],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.w2.weight',\n",
       "              tensor([[ 0.0823,  0.0676,  0.0169,  ..., -0.0054,  0.0116, -0.0576],\n",
       "                      [-0.0122,  0.0181,  0.0527,  ...,  0.0624, -0.0029, -0.0281],\n",
       "                      [ 0.0580,  0.0325,  0.0580,  ..., -0.0330, -0.0074, -0.0542],\n",
       "                      ...,\n",
       "                      [ 0.0013, -0.0173, -0.0528,  ..., -0.0125,  0.0629, -0.0232],\n",
       "                      [ 0.0492,  0.0199, -0.0731,  ...,  0.0372, -0.0093,  0.0143],\n",
       "                      [ 0.0143, -0.0504,  0.0195,  ...,  0.0893, -0.0098,  0.0053]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.4.ffn.w2.bias',\n",
       "              tensor([ 3.2834e-02,  1.6219e-02, -3.7956e-03, -3.4559e-02,  1.0435e-01,\n",
       "                      -1.0267e-03,  3.4001e-02, -6.2608e-02,  1.5935e-02, -6.2453e-02,\n",
       "                      -5.6243e-02,  4.4292e-02, -1.4582e-02, -1.8475e-02, -2.4235e-02,\n",
       "                      -4.5781e-02,  3.6634e-02,  2.0317e-02,  3.1527e-03, -6.7707e-02,\n",
       "                      -7.9539e-03, -3.3359e-02,  4.8303e-02, -2.6425e-02, -1.2909e-02,\n",
       "                      -8.0476e-02, -2.8212e-02, -6.7572e-02,  4.9795e-02, -4.7244e-02,\n",
       "                      -6.4056e-02, -6.3442e-02,  5.3528e-02, -2.2224e-02,  7.4112e-02,\n",
       "                       5.8375e-02,  3.6996e-04,  1.2337e-02,  2.1085e-02, -8.9188e-02,\n",
       "                      -1.1989e-02,  3.0237e-02,  5.7916e-02,  3.6488e-02, -1.0711e-01,\n",
       "                      -2.4895e-02,  4.4317e-02, -3.3191e-02, -4.6965e-02, -1.0359e-01,\n",
       "                      -2.9493e-02,  1.4422e-02, -3.5044e-02,  5.5651e-02, -2.2299e-03,\n",
       "                      -7.9211e-02,  3.5723e-02,  4.9314e-02,  1.1496e-01, -2.1983e-02,\n",
       "                       6.8601e-03, -3.0177e-02, -1.1372e-01,  8.8571e-04,  8.6056e-02,\n",
       "                       3.4738e-02, -2.8749e-02, -6.4286e-02, -1.0825e-01,  5.4435e-03,\n",
       "                      -5.1070e-02, -5.5828e-02,  1.6464e-02, -3.1819e-02, -5.4562e-03,\n",
       "                      -3.9359e-02, -1.6644e-02, -4.2388e-02,  2.3901e-02, -4.6561e-02,\n",
       "                       5.1428e-02,  8.5060e-02, -1.4337e-01,  7.2023e-02, -2.2037e-02,\n",
       "                      -1.1289e-01,  4.1277e-02, -5.1854e-02, -2.9727e-02,  2.2870e-02,\n",
       "                      -1.1770e-01,  9.1157e-03,  7.3698e-02,  7.4779e-02,  1.7218e-02,\n",
       "                      -4.8097e-02, -4.1558e-02, -2.1972e-02, -7.4261e-02,  3.4876e-02,\n",
       "                      -8.5668e-03,  4.2693e-02, -5.6068e-02,  2.9575e-02, -2.6315e-03,\n",
       "                       1.0476e-02, -1.7010e-02,  8.9852e-03, -5.1379e-03, -3.6423e-02,\n",
       "                      -5.6247e-03,  1.7160e-02,  1.0028e-02,  2.6624e-02, -2.9103e-02,\n",
       "                       8.7284e-02, -1.0597e-02,  5.0739e-03,  3.5822e-02, -9.8071e-02,\n",
       "                      -4.9000e-02,  8.5842e-04,  3.5806e-02, -3.7758e-02,  8.2193e-03,\n",
       "                      -1.1171e-01, -2.9674e-02, -5.4140e-02, -5.0945e-02,  2.4951e-03,\n",
       "                      -5.7530e-02,  7.5106e-03,  1.1019e-01, -4.8649e-02,  1.8300e-02,\n",
       "                      -3.4770e-02, -1.4116e-02,  3.1160e-02,  6.3315e-02, -8.5751e-02,\n",
       "                       4.7458e-02,  2.8683e-02, -2.2691e-02,  2.7160e-02,  4.0410e-02,\n",
       "                       2.2332e-02, -2.9405e-02,  6.6596e-03, -6.6082e-02, -1.0782e-01,\n",
       "                       4.4401e-02, -4.2895e-02,  1.7229e-02, -2.8710e-02, -8.9881e-02,\n",
       "                       7.7039e-03, -3.5671e-02,  4.5494e-02,  1.8320e-02,  5.3070e-02,\n",
       "                      -8.9376e-02,  1.1489e-02, -1.5918e-02, -3.8298e-02, -5.8427e-02,\n",
       "                       4.3071e-02, -1.7378e-02, -3.3681e-03,  1.3752e-01, -7.1125e-02,\n",
       "                      -1.3702e-01, -9.8997e-03,  2.6894e-02,  1.4154e-01,  6.0897e-02,\n",
       "                       4.2220e-02, -1.5447e-03,  1.0860e-02, -7.0028e-02,  5.5727e-02,\n",
       "                      -1.2375e-01, -1.0981e-01, -8.6687e-02, -4.5456e-02, -1.5425e-03,\n",
       "                      -1.0796e-02,  5.8463e-02, -4.3338e-02, -4.4473e-02, -7.8625e-02,\n",
       "                      -9.0781e-02, -1.1383e-01, -5.4161e-02,  5.7990e-02,  1.9052e-02,\n",
       "                      -3.5619e-03, -5.8873e-02, -1.2120e-02,  2.8190e-02, -1.1107e-03,\n",
       "                       4.4893e-02, -2.5971e-02, -8.1597e-02,  8.7681e-02, -1.2248e-02,\n",
       "                       2.0372e-02, -5.7911e-02, -1.2867e-01, -6.9003e-02, -4.5366e-02,\n",
       "                      -4.9604e-02, -1.3427e-02,  9.0724e-03, -9.5529e-03,  2.1763e-03,\n",
       "                       1.1779e-02,  6.4882e-02,  8.3525e-03,  7.4037e-02, -4.5290e-02,\n",
       "                      -7.1263e-03, -5.7039e-02,  3.4666e-03, -5.6435e-02,  3.7188e-02,\n",
       "                      -1.7677e-02, -7.8016e-01,  7.3072e-02,  2.8560e-03, -3.0095e-02,\n",
       "                       3.9885e-02, -6.1089e-05, -1.1816e-02,  3.5646e-02,  1.1964e-02,\n",
       "                       4.2676e-02, -3.7027e-02,  3.4307e-03, -6.3318e-02,  3.4516e-02,\n",
       "                       3.0311e-02, -7.7970e-02, -5.0107e-03, -1.3118e-02,  5.0212e-02,\n",
       "                       1.5822e-02,  1.9635e-02,  2.1768e-02, -4.9209e-02,  9.3544e-03,\n",
       "                       2.0422e-02,  9.7062e-03, -3.8459e-02,  2.8558e-03, -7.7452e-02,\n",
       "                       1.0401e-01, -2.4440e-02,  3.5661e-02, -1.0315e-01,  1.9493e-02,\n",
       "                      -4.5421e-02,  1.9692e-02,  2.2856e-01, -6.1141e-02, -5.0955e-02,\n",
       "                       7.6697e-02, -3.5579e-02,  6.7066e-02,  1.9341e-02,  2.2522e-02,\n",
       "                       1.1256e-02, -4.4381e-02, -2.4671e-03,  1.4232e-02,  1.0616e-02,\n",
       "                       1.0954e-02, -2.9709e-02, -6.0374e-02, -2.9501e-02,  7.0083e-03,\n",
       "                       1.9757e-02, -1.5481e-02,  1.0767e-01, -1.1788e-03, -1.4380e-01,\n",
       "                      -1.8574e-02, -5.4136e-03,  2.9406e-02,  2.7660e-02, -7.5093e-03,\n",
       "                       9.8910e-03, -4.5987e-03, -5.3931e-02,  1.4686e-02, -5.3009e-02,\n",
       "                      -4.9404e-02, -5.3414e-02,  3.6792e-02, -7.7420e-02, -1.6986e-02,\n",
       "                       3.1993e-02,  1.8254e-01,  4.2554e-02, -5.4561e-02, -7.9959e-02,\n",
       "                      -4.1422e-02, -7.7137e-02, -1.9196e-02, -3.3745e-02, -4.3085e-03,\n",
       "                       1.7438e-01,  1.0711e-02, -6.7012e-04,  1.3721e-02, -5.2818e-02,\n",
       "                       2.7324e-02,  1.1255e-02, -5.8725e-02,  4.4327e-02, -8.5608e-03,\n",
       "                       1.5517e-01,  5.9281e-02,  9.7200e-03,  1.0546e-02,  5.6790e-03,\n",
       "                       1.3442e-03, -3.2981e-03,  5.2764e-02, -6.1670e-02,  2.2748e-02,\n",
       "                       6.4640e-02,  6.8895e-02,  1.9512e-02,  4.2201e-03,  3.1812e-02,\n",
       "                       6.0564e-02, -9.1899e-02,  3.1146e-02,  2.2502e-02,  6.3289e-02,\n",
       "                       8.7035e-02, -8.1434e-03, -4.5552e-02,  2.1479e-02, -9.4719e-02,\n",
       "                       3.6681e-02, -1.1703e-01, -1.9518e-02, -4.5505e-02,  9.4655e-03,\n",
       "                      -6.8459e-02,  4.4996e-02,  1.2227e-01, -2.7262e-02,  1.1859e-01,\n",
       "                      -2.1243e-02, -8.8159e-02,  4.0405e-02, -2.2344e-02, -6.1896e-02,\n",
       "                       8.4541e-02,  4.5230e-02,  4.9789e-02,  1.8517e-02, -2.0360e-02,\n",
       "                       4.5575e-02, -7.3785e-03, -2.0584e-02,  2.2910e-02, -8.3173e-03,\n",
       "                      -9.6578e-02,  4.4158e-02,  6.2164e-02, -4.2778e-02, -2.7544e-02,\n",
       "                       1.1374e-01, -3.9235e-02,  5.3832e-02, -3.1870e-02,  3.2298e-03,\n",
       "                      -2.8406e-02,  4.7036e-02, -2.5095e-04,  4.7026e-02,  8.8456e-02,\n",
       "                       2.5620e-02, -6.8591e-02,  7.2665e-03, -3.3131e-02,  3.1771e-02,\n",
       "                      -9.0072e-02,  1.3717e-01,  2.7924e-03, -3.7361e-02, -3.3447e-02,\n",
       "                       8.9954e-03, -9.0985e-02,  7.1353e-02,  4.2701e-02,  1.3610e-02,\n",
       "                      -1.1784e-01, -5.5506e-02, -3.6813e-02,  6.7504e-02, -9.3953e-02,\n",
       "                      -3.0790e-02, -3.4710e-02,  7.8424e-02, -7.7691e-01, -4.3598e-02,\n",
       "                       8.7218e-02,  3.4273e-02, -3.2972e-02,  1.1537e-01,  3.4419e-02,\n",
       "                      -5.8209e-02,  7.0765e-03, -7.3808e-02,  3.9694e-02, -4.4302e-02,\n",
       "                       6.0101e-02,  3.1003e-02, -7.5053e-03, -2.8971e-02,  4.0892e-02,\n",
       "                       1.8830e-02, -1.0235e-01, -6.1946e-02, -7.4780e-02, -9.5649e-02,\n",
       "                       6.0908e-02,  4.1137e-03,  8.7168e-02, -6.5959e-03,  2.0487e-01,\n",
       "                      -5.8484e-02,  8.0275e-03,  3.7932e-02, -9.9420e-02,  1.5652e-02,\n",
       "                       2.6128e-02, -3.1453e-02, -1.6153e-02, -1.6655e-02, -4.7179e-02,\n",
       "                      -7.0900e-02,  2.7974e-02, -5.4580e-03,  1.0580e-01,  9.3908e-03,\n",
       "                       1.9718e-02,  2.7499e-03,  3.8792e-02,  7.8603e-02, -4.4807e-02,\n",
       "                       1.5334e-02,  4.4972e-02, -8.5063e-02,  1.1137e-02, -4.7798e-02,\n",
       "                       3.8107e-02, -6.2236e-02,  1.1936e-02,  1.9710e-02,  4.0153e-02,\n",
       "                      -3.6923e-02,  2.9456e-02, -3.2290e-02,  4.8803e-02,  2.8251e-02,\n",
       "                       5.3167e-02,  1.2337e-02,  3.5251e-02, -1.0540e-02, -3.7247e-02,\n",
       "                      -5.2312e-02, -5.6862e-02, -3.8542e-02,  7.2390e-02, -3.5934e-02,\n",
       "                       9.0067e-02,  6.1225e-02, -8.8473e-03, -1.9117e-02, -6.4285e-02,\n",
       "                       1.3419e-01, -1.0447e-01,  3.0060e-03,  1.7330e-02,  1.7410e-02,\n",
       "                      -3.7336e-02,  1.7004e-02,  1.4495e-02,  5.2094e-02,  5.0971e-02,\n",
       "                      -5.3675e-02,  1.8829e-02,  2.8183e-02,  3.8146e-01, -7.7262e-02,\n",
       "                       5.7868e-02, -4.7130e-02, -6.2664e-02,  7.2144e-02, -1.0506e-01,\n",
       "                       8.6391e-03, -2.5253e-02, -1.1566e-01,  4.4499e-02, -6.8697e-02,\n",
       "                      -2.0776e-02,  7.7610e-02,  3.4541e-02, -3.3844e-02, -1.3639e-02,\n",
       "                      -6.1347e-02,  3.4596e-02, -5.3206e-02, -3.1285e-02, -1.1938e-02,\n",
       "                      -4.9453e-02, -1.8414e-02,  1.8159e-02,  5.4504e-02, -3.7789e-02,\n",
       "                      -1.4767e-03, -8.1590e-02, -7.5250e-02, -8.2312e-02,  5.0934e-03,\n",
       "                      -7.9592e-02, -5.4294e-02, -2.7757e-02,  9.4216e-02,  6.4014e-02,\n",
       "                      -1.0173e-03,  4.5577e-02, -1.0745e-02,  6.3504e-02, -7.1118e-02,\n",
       "                      -6.7258e-02, -7.6728e-02, -5.8992e-02, -8.0250e-03, -4.8777e-02,\n",
       "                       9.7542e-02,  4.4053e-02, -7.7772e-02,  2.5282e-02, -4.7109e-02,\n",
       "                      -7.9458e-02, -1.5865e-02,  4.8091e-02,  8.4235e-03, -4.5084e-02,\n",
       "                      -2.9581e-02, -1.0704e-02,  9.0574e-02, -5.8032e-02, -2.8104e-02,\n",
       "                      -4.5943e-03,  4.1251e-02, -3.5457e-02,  7.3428e-02, -2.9778e-02,\n",
       "                      -3.9777e-03,  2.0232e-02, -1.0472e-01,  8.9000e-02, -6.7724e-02,\n",
       "                       1.6179e-01, -8.0627e-02,  6.5492e-02,  5.1173e-02, -7.4177e-02,\n",
       "                      -2.8685e-02,  1.4925e-03, -4.1682e-02, -9.5727e-02, -1.0753e-01,\n",
       "                       5.8498e-03, -1.0659e-02,  8.0267e-02, -5.2018e-02, -7.8055e-03,\n",
       "                      -4.9014e-02,  2.7218e-02,  5.7728e-02,  6.6293e-02, -3.6436e-02,\n",
       "                      -3.0818e-02, -1.0675e-01, -9.1695e-02,  5.2923e-02, -2.6814e-02,\n",
       "                       6.0321e-02,  1.3405e-02, -6.1026e-02,  2.2960e-02, -4.4035e-02,\n",
       "                      -2.7776e-02, -2.7793e-02, -1.5643e-02, -1.2121e-02, -2.0127e-02,\n",
       "                       5.8534e-02,  9.0809e-03,  4.2419e-02,  1.6845e-02, -5.2859e-02,\n",
       "                       7.0410e-04, -1.9468e-02, -3.4090e-02,  6.1626e-02, -8.6379e-02,\n",
       "                      -1.2174e-01,  1.1426e-02, -6.0392e-02,  9.7607e-02, -5.0969e-02,\n",
       "                      -4.7290e-02,  9.2744e-02, -4.3618e-02, -7.7180e-03,  6.4510e-02,\n",
       "                       5.4816e-02, -1.0987e-03, -3.9242e-02,  4.4305e-02,  2.3424e-02,\n",
       "                      -2.0130e-03,  3.9335e-03, -7.4500e-02, -7.3430e-03, -6.9187e-03,\n",
       "                       6.2760e-02, -5.2792e-02, -1.0144e-02, -8.0513e-02, -5.7000e-02,\n",
       "                      -1.8153e-02,  9.9984e-03,  3.0305e-02,  1.6963e-02,  5.7949e-02,\n",
       "                       1.1137e-01,  3.4295e-02, -8.9628e-03, -3.3824e-02,  4.8816e-02,\n",
       "                       6.6205e-02,  4.2199e-02, -8.9062e-02,  1.6500e-02,  3.7798e-02,\n",
       "                      -8.0935e-03, -1.0089e-01,  1.1670e-02, -5.0192e-02,  1.2561e-02,\n",
       "                      -5.1653e-02, -9.9832e-02, -4.8511e-02,  6.2894e-02,  5.2247e-02,\n",
       "                       6.6830e-02,  4.3837e-02, -2.4436e-02,  9.6655e-02,  1.9623e-01,\n",
       "                      -6.9909e-03,  3.0747e-02, -3.5084e-02, -4.0615e-02, -6.4432e-02,\n",
       "                       7.0651e-02, -1.3040e-01, -6.6778e-02,  2.8207e-02, -4.7391e-03,\n",
       "                      -2.5769e-02, -3.0601e-02, -8.0390e-02,  1.8869e-01, -9.3242e-02,\n",
       "                       2.8651e-02, -1.2709e-01, -1.4366e-01,  4.0064e-02, -1.7782e-02,\n",
       "                      -8.8333e-02,  1.8084e-02, -2.5472e-02,  4.8648e-02,  2.6448e-02,\n",
       "                      -6.0712e-02,  1.9884e-02,  1.5120e-03,  9.4745e-02,  2.6392e-02,\n",
       "                       5.2742e-03, -1.0330e-01,  4.2673e-02,  3.7092e-02, -9.6042e-03,\n",
       "                       3.1628e-02,  6.0394e-02, -2.5849e-02,  2.2371e-03,  2.6644e-02,\n",
       "                      -4.9648e-02,  1.0945e-03,  1.7065e-02,  3.9940e-02,  3.9805e-02,\n",
       "                      -7.0621e-03,  1.0388e-01, -1.7875e-02,  2.0138e-02, -1.3982e-02,\n",
       "                      -4.9351e-04,  1.5184e-02, -7.3594e-02,  5.7725e-02, -9.6211e-02,\n",
       "                      -1.8268e-02, -1.3901e-01, -4.7510e-02,  9.5273e-02, -1.5146e-02,\n",
       "                      -8.0817e-02,  9.3340e-02, -4.8238e-02, -7.5361e-02,  7.3739e-02,\n",
       "                       7.6447e-02,  9.9698e-02,  7.2419e-02,  4.4897e-02, -3.3331e-02,\n",
       "                      -5.0965e-02, -5.6134e-02,  2.3918e-02,  3.3535e-02,  5.3314e-02,\n",
       "                      -6.0932e-03,  5.8607e-02,  5.8723e-02, -4.6961e-02, -2.1269e-02,\n",
       "                      -5.8355e-02,  1.5141e-02, -3.9952e-03,  1.3512e-03, -3.7051e-02,\n",
       "                       4.2037e-02,  1.9797e-02, -1.4500e-03, -1.5222e-02, -2.9042e-03,\n",
       "                       2.7039e-02,  6.5685e-02, -2.9007e-02, -5.7281e-02, -2.5385e-02,\n",
       "                      -3.9769e-02, -8.5302e-03, -5.6509e-02], device='cuda:0')),\n",
       "             ('layers.4.anorm.weight',\n",
       "              tensor([0.8007, 0.8290, 0.8933, 0.9147, 0.8370, 0.8221, 0.8351, 0.8477, 0.8599,\n",
       "                      0.8157, 0.9073, 0.8896, 0.9155, 0.8358, 0.8172, 0.9032, 0.8778, 0.8698,\n",
       "                      0.9010, 0.8689, 0.8599, 0.8637, 0.8059, 0.8324, 0.8127, 0.8658, 0.8184,\n",
       "                      0.8633, 0.8153, 0.8979, 0.9055, 0.8263, 0.8234, 0.8805, 0.8684, 0.8349,\n",
       "                      0.8750, 0.7805, 0.8480, 0.8463, 0.8195, 0.8009, 0.8992, 0.8377, 0.9007,\n",
       "                      0.8706, 0.8998, 0.8159, 0.8443, 0.8683, 0.8807, 0.8680, 0.8959, 0.8084,\n",
       "                      0.8866, 0.8140, 0.8241, 0.8513, 0.7867, 0.8283, 0.8596, 0.8327, 0.8716,\n",
       "                      0.8323, 0.8504, 0.8071, 0.8786, 0.8856, 0.8578, 0.8432, 0.8699, 0.8324,\n",
       "                      0.8338, 0.8724, 0.8327, 0.7800, 0.8024, 0.8948, 0.8437, 0.8736, 0.9091,\n",
       "                      0.9087, 0.8418, 0.8810, 0.8348, 0.8336, 0.8805, 0.8917, 0.8229, 0.8746,\n",
       "                      0.8865, 0.8826, 0.8804, 0.9251, 0.8577, 0.8956, 0.7799, 0.8443, 0.8347,\n",
       "                      0.8469, 0.7748, 0.9178, 0.8710, 0.7617, 0.8349, 0.8695, 0.8874, 0.8705,\n",
       "                      0.8574, 0.8356, 0.8597, 0.8467, 0.8723, 0.8609, 0.8116, 0.8426, 0.8293,\n",
       "                      0.7630, 0.8666, 0.8796, 0.8576, 0.8097, 0.8206, 0.8137, 0.9059, 0.8414,\n",
       "                      0.8030, 0.8513, 0.8897, 0.8592, 0.8518, 0.8601, 0.8202, 0.8758, 0.8644,\n",
       "                      0.8347, 0.8430, 0.7780, 0.8312, 0.8815, 0.8987, 0.9021, 0.8445, 0.8617,\n",
       "                      0.8788, 0.7980, 0.8429, 0.9283, 0.8103, 0.8439, 0.7704, 0.8209, 0.8473,\n",
       "                      0.8398, 0.7899, 0.8576, 0.8644, 0.8595, 0.8204, 0.8467, 0.9130, 0.8583,\n",
       "                      0.8759, 0.7976, 0.8335, 0.8459, 0.8194, 0.7871, 0.8238, 0.8465, 0.8186,\n",
       "                      0.8488, 0.8589, 0.8468, 0.8022, 0.8536, 0.8621, 0.8588, 0.8247, 0.8333,\n",
       "                      0.9223, 0.9232, 0.8482, 0.8911, 0.8373, 0.8806, 0.8753, 0.8537, 0.8782,\n",
       "                      0.8650, 0.7984, 0.8628, 0.9126, 0.8664, 0.7849, 0.8391, 0.7878, 0.8349,\n",
       "                      0.8587, 0.8161, 0.8330, 0.9009, 0.8955, 0.7526, 0.8997, 0.8665, 0.8594,\n",
       "                      0.9173, 0.8731, 0.7991, 0.8250, 0.7588, 0.8850, 0.7992, 0.7708, 0.8613,\n",
       "                      0.8534, 0.8295, 0.8147, 0.8774, 0.8369, 0.8844, 0.8501, 0.8970, 0.8686,\n",
       "                      0.8700, 1.0270, 0.9095, 0.8136, 0.8527, 0.8227, 0.7851, 0.8857, 0.8589,\n",
       "                      0.8551, 0.9432, 0.8479, 0.8329, 0.8591, 0.8174, 0.8028, 0.8591, 0.8809,\n",
       "                      0.8732, 0.8796, 0.8536, 0.8643, 0.8246, 0.8754, 0.8357, 0.7800, 0.8231,\n",
       "                      0.8567, 0.8955, 0.8242, 0.8626, 0.9321, 0.8337, 0.9765, 0.8597, 0.8815,\n",
       "                      0.8553, 0.9087, 0.8717, 0.8476, 0.8266, 0.8503, 0.8766, 0.8421, 0.8438,\n",
       "                      0.8327, 0.8037, 0.8495, 0.8367, 0.8041, 0.8783, 0.8474, 0.8679, 0.8046,\n",
       "                      0.8715, 0.8726, 0.8999, 0.9102, 0.8626, 0.9177, 0.8346, 0.8023, 0.7795,\n",
       "                      0.8204, 0.8853, 0.8251, 0.8362, 0.7921, 0.8421, 0.8595, 0.7927, 0.8590,\n",
       "                      0.8665, 0.8506, 0.8709, 0.8838, 0.8530, 0.8357, 0.8446, 0.8439, 0.8858,\n",
       "                      0.8220, 0.8398, 0.8551, 0.8802, 0.8607, 0.8214, 0.8836, 0.7544, 0.8192,\n",
       "                      0.8454, 0.7609, 0.9203, 0.9069, 0.9016, 0.7785, 0.8978, 0.8519, 0.8161,\n",
       "                      0.8317, 0.8023, 0.8120, 0.8385, 0.8759, 0.8346, 0.8339, 0.8473, 0.8625,\n",
       "                      0.8187, 0.8715, 0.9181, 0.8842, 0.7607, 0.8308, 0.8688, 0.8673, 0.8615,\n",
       "                      0.8002, 0.9175, 0.7581, 0.8106, 0.8848, 0.8637, 0.8249, 0.8407, 0.8894,\n",
       "                      0.8397, 0.8680, 0.9196, 0.9040, 0.8352, 0.8848, 0.8903, 0.9129, 0.8496,\n",
       "                      0.8126, 0.8658, 0.8904, 0.8033, 0.8804, 0.7922, 0.8864, 0.8810, 0.8930,\n",
       "                      0.7422, 0.9362, 0.8489, 0.8828, 0.8822, 0.8454, 0.8343, 0.8516, 0.8733,\n",
       "                      0.8644, 0.8299, 0.8832, 0.8638, 0.8784, 0.8519, 0.8381, 0.8573, 0.8733,\n",
       "                      0.8470, 0.7900, 0.8280, 0.8312, 0.8372, 0.8789, 0.8519, 0.7927, 0.8675,\n",
       "                      0.8716, 0.8393, 0.8936, 0.7991, 0.8302, 0.8182, 0.8399, 0.8506, 0.8493,\n",
       "                      0.8342, 0.9094, 0.8469, 0.5905, 0.9046, 0.8680, 0.7606, 0.8145, 0.7432,\n",
       "                      0.8667, 0.8216, 0.8514, 0.8025, 0.8547, 0.9003, 0.8331, 0.7821, 0.8700,\n",
       "                      0.8384, 0.8949, 0.8905, 0.7935, 0.7740, 0.8226, 0.8484, 0.8045, 0.8278,\n",
       "                      0.7611, 0.8323, 0.7720, 0.8089, 0.8834, 0.8673, 0.9327, 0.8261, 0.8406,\n",
       "                      0.8520, 0.8845, 0.8327, 0.7811, 0.9043, 0.8663, 0.8411, 0.8456, 0.8262,\n",
       "                      0.7595, 0.8883, 0.8243, 0.8448, 0.8569, 0.8929, 0.7172, 0.8547, 0.8984,\n",
       "                      0.8222, 0.7600, 0.8632, 0.8770, 0.8589, 0.8678, 0.8446, 0.8530, 0.8976,\n",
       "                      0.8768, 0.8258, 0.8750, 0.8749, 0.8047, 0.8086, 0.8490, 0.8199, 0.8732,\n",
       "                      0.8321, 0.8280, 0.8776, 0.8309, 0.8454, 0.8472, 0.8329, 0.8494, 0.9101,\n",
       "                      0.8188, 0.8368, 0.8699, 0.7893, 0.8597, 0.8330, 0.8283, 0.8087, 0.8712,\n",
       "                      0.8787, 0.8424, 0.7938, 0.5964, 0.7897, 0.8550, 0.8596, 0.8211, 0.8448,\n",
       "                      0.8928, 0.8429, 0.8165, 0.7937, 0.8885, 0.7871, 0.8030, 0.8109, 0.7853,\n",
       "                      0.8668, 0.8345, 0.8103, 0.8812, 0.8895, 0.8425, 0.8167, 0.8091, 0.9102,\n",
       "                      0.8445, 0.8246, 0.7774, 0.7801, 0.8467, 0.8953, 0.8200, 0.8382, 0.8617,\n",
       "                      0.8913, 0.8977, 0.9071, 0.8421, 0.8665, 0.8365, 0.8581, 0.8458, 0.8014,\n",
       "                      0.9389, 0.9005, 0.8559, 0.8714, 0.8501, 0.8633, 0.8592, 0.8286, 0.8599,\n",
       "                      0.8316, 0.8456, 0.8129, 0.8503, 0.7888, 0.9118, 0.8627, 0.8339, 0.8788,\n",
       "                      0.8907, 0.8634, 0.8277, 0.7858, 0.8366, 0.8032, 0.8777, 0.8138, 0.8014,\n",
       "                      0.8304, 0.9032, 0.9304, 0.9261, 0.8414, 0.8141, 0.8643, 0.8170, 0.8600,\n",
       "                      0.8983, 0.8269, 0.8768, 0.7727, 0.8378, 0.8878, 0.8579, 0.8065, 0.8581,\n",
       "                      0.8939, 0.9154, 0.8395, 0.8499, 0.9057, 0.8621, 0.8931, 0.8540, 0.8548,\n",
       "                      0.9256, 0.9200, 0.8318, 0.8725, 0.8103, 0.8646, 0.8606, 0.8715, 0.8542,\n",
       "                      0.8575, 0.8827, 0.8658, 0.9415, 0.8171, 0.8413, 0.9154, 0.8624, 0.8579,\n",
       "                      0.8899, 0.8714, 0.8241, 0.8448, 0.8039, 0.8438, 0.8261, 0.8896, 0.8782,\n",
       "                      0.8110, 0.8926, 0.8145, 0.9354, 0.8452, 0.9120, 0.8782, 0.9011, 0.8567,\n",
       "                      0.8023, 0.9287, 0.8733, 0.7700, 0.8470, 0.8991, 0.8571, 0.7936, 0.8158,\n",
       "                      0.8750, 0.8144, 0.8683, 0.8691, 0.8880, 0.8563, 0.8512, 0.8432, 0.8077,\n",
       "                      0.8799, 0.8205, 0.8286, 0.8302, 0.8615, 0.8342, 0.8257, 0.8605, 0.8586,\n",
       "                      0.8367, 0.8880, 0.8747, 0.8338, 0.9166, 0.8711, 0.8317, 0.8038, 0.8529,\n",
       "                      0.8481, 0.8182, 0.8064, 0.7934, 0.8930, 0.8596, 0.8003, 0.8914, 0.8746,\n",
       "                      0.7885, 0.8681, 0.8485, 0.8128, 0.8121, 0.8287, 0.8330, 0.8641, 0.8489,\n",
       "                      0.8202, 0.8566, 0.8062, 0.8085, 0.8418, 0.8142, 0.8264, 0.9474, 0.8854,\n",
       "                      0.7593, 0.9036, 0.9070, 0.8095, 0.8995, 0.8600, 0.9110, 0.8830, 0.9197,\n",
       "                      0.9063, 0.9119, 0.7946, 0.8084, 0.8321, 0.7819, 0.7677, 0.8925, 0.9167,\n",
       "                      0.7813, 0.8240, 0.8639, 0.8375, 0.8866, 0.8791, 0.8369, 0.8325, 0.8541,\n",
       "                      0.8052, 0.8633, 0.8653, 0.8650, 0.8747, 0.9431, 0.8241, 0.7742, 0.8239,\n",
       "                      0.8458, 0.8310, 0.8032, 0.8888, 0.9031, 0.8426, 0.8397, 0.9074, 0.8250,\n",
       "                      0.8593, 0.8933, 0.8735, 0.8843, 0.8699, 0.8065, 0.8845, 0.8032, 0.8495,\n",
       "                      0.8769, 0.8841, 0.8730, 0.7579, 0.8962, 0.8426, 0.8132, 0.8947, 0.7976,\n",
       "                      0.7778, 0.8042, 0.8775, 0.8361, 0.8452, 0.7998, 0.9480, 0.8532, 0.8684,\n",
       "                      0.8545, 0.8766, 0.8038], device='cuda:0')),\n",
       "             ('layers.4.fnorm.weight',\n",
       "              tensor([0.8109, 0.8445, 0.8123, 0.8032, 0.8087, 0.8050, 0.8078, 0.8191, 0.7600,\n",
       "                      0.7664, 0.8048, 0.7907, 0.8147, 0.7732, 0.8289, 0.7598, 0.7620, 0.8381,\n",
       "                      0.7371, 0.8211, 0.8780, 0.7742, 0.8001, 0.8478, 0.8368, 0.8311, 0.8081,\n",
       "                      0.7975, 0.7741, 0.7624, 0.7601, 0.8177, 0.7150, 0.8296, 0.7776, 0.8026,\n",
       "                      0.8083, 0.7569, 0.7617, 0.7969, 0.7764, 0.8014, 0.7974, 0.7482, 0.8018,\n",
       "                      0.7802, 0.7598, 0.7621, 0.7609, 0.7647, 0.8311, 0.7533, 0.7805, 0.8005,\n",
       "                      0.8340, 0.8301, 0.8083, 0.7984, 0.6995, 0.8060, 0.7647, 0.8286, 0.7918,\n",
       "                      0.7880, 0.7546, 0.8215, 0.8437, 0.8135, 0.8262, 0.7472, 0.7890, 0.8885,\n",
       "                      0.8148, 0.8109, 0.8000, 0.8141, 0.7819, 0.8206, 0.7871, 0.7980, 0.8325,\n",
       "                      0.8380, 0.6195, 0.8417, 0.7736, 0.8411, 0.7898, 0.7909, 0.8343, 0.7856,\n",
       "                      0.7785, 0.8032, 0.7951, 0.7776, 0.7121, 0.7756, 0.7825, 0.8303, 0.8073,\n",
       "                      0.8222, 0.7978, 0.8442, 0.8320, 0.7205, 0.7973, 0.8041, 0.7805, 0.7643,\n",
       "                      0.8195, 0.7459, 0.7604, 0.7644, 0.8106, 0.8007, 0.8306, 0.7350, 0.7896,\n",
       "                      0.7748, 0.8160, 0.8072, 0.8448, 0.7976, 0.7890, 0.8083, 0.8237, 0.7931,\n",
       "                      0.7665, 0.8326, 0.8043, 0.8047, 0.7491, 0.8729, 0.7190, 0.8312, 0.7728,\n",
       "                      0.7725, 0.8179, 0.7883, 0.7634, 0.8135, 0.7967, 0.7821, 0.8369, 0.7803,\n",
       "                      0.8578, 0.7966, 0.7713, 0.7842, 0.8090, 0.7982, 0.7742, 0.7314, 0.8096,\n",
       "                      0.7693, 0.6457, 0.8099, 0.8115, 0.8107, 0.7876, 0.8197, 0.7449, 0.7460,\n",
       "                      0.8277, 0.7870, 0.8001, 0.7957, 0.8003, 0.7830, 0.7667, 0.7918, 0.7541,\n",
       "                      0.7913, 0.7437, 0.7699, 0.7725, 0.7326, 0.7441, 0.7772, 0.7568, 0.8145,\n",
       "                      0.8039, 0.8365, 0.7633, 0.8055, 0.8121, 0.8497, 0.8288, 0.7714, 0.8150,\n",
       "                      0.8370, 0.7702, 0.7253, 0.7968, 0.8425, 0.8114, 0.8101, 0.7931, 0.7941,\n",
       "                      0.7639, 0.7850, 0.7470, 0.7994, 0.8566, 0.7650, 0.7795, 0.8187, 0.8383,\n",
       "                      0.8130, 0.8221, 0.7615, 0.8247, 0.7751, 0.7781, 0.8234, 0.7565, 0.8154,\n",
       "                      0.7727, 0.8169, 0.7928, 0.8029, 0.8375, 0.8015, 0.7515, 0.7719, 0.7642,\n",
       "                      0.7921, 0.1688, 0.7884, 0.7655, 0.8094, 0.7703, 0.7503, 0.7726, 0.7568,\n",
       "                      0.8249, 0.7725, 0.7528, 0.7589, 0.7578, 0.7859, 0.7740, 0.7556, 0.8100,\n",
       "                      0.7605, 0.7556, 0.7804, 0.8270, 0.7654, 0.8039, 0.7710, 0.7670, 0.7855,\n",
       "                      0.8193, 0.8052, 0.7800, 0.7565, 0.7773, 0.7916, 0.7603, 0.7689, 0.8320,\n",
       "                      0.8025, 0.7343, 0.8169, 0.8052, 0.7559, 0.7354, 0.7743, 0.7376, 0.8432,\n",
       "                      0.7757, 0.8155, 0.6998, 0.8184, 0.8408, 0.8081, 0.7605, 0.7832, 0.7629,\n",
       "                      0.8085, 0.7795, 0.7792, 0.7824, 0.7930, 0.7816, 0.7605, 0.7655, 0.8253,\n",
       "                      0.7760, 0.7778, 0.7914, 0.8190, 0.8148, 0.7765, 0.7866, 0.7593, 0.7898,\n",
       "                      0.7997, 0.8079, 0.7784, 0.7775, 0.7109, 0.7517, 0.7677, 0.7167, 0.7930,\n",
       "                      0.7741, 0.8130, 0.8127, 0.8085, 0.7050, 0.7976, 0.8234, 0.8347, 0.8158,\n",
       "                      0.8237, 0.7874, 0.7968, 0.7720, 0.8688, 0.8149, 0.8640, 0.7972, 0.8232,\n",
       "                      0.7767, 0.8183, 0.7886, 0.7790, 0.8043, 0.8272, 0.7899, 0.7956, 0.7675,\n",
       "                      0.7917, 0.8230, 0.8189, 0.7978, 0.7533, 0.8014, 0.7692, 0.7791, 0.7678,\n",
       "                      0.7750, 0.8013, 0.8167, 0.7627, 0.7969, 0.8016, 0.8055, 0.8042, 0.7888,\n",
       "                      0.7479, 0.8483, 0.7835, 0.8035, 0.8409, 0.8326, 0.7806, 0.7364, 0.8154,\n",
       "                      0.8500, 0.7707, 0.8164, 0.8151, 0.8250, 0.7935, 0.7878, 0.7654, 0.7847,\n",
       "                      0.7666, 0.8039, 0.7829, 0.8197, 0.8180, 0.7905, 0.7754, 0.8225, 0.7581,\n",
       "                      0.8139, 0.7661, 0.8453, 0.8166, 0.7899, 0.8111, 0.8035, 0.8034, 0.8100,\n",
       "                      0.7982, 0.7707, 0.8049, 0.8107, 0.7713, 0.8245, 0.7898, 0.8259, 0.8352,\n",
       "                      0.8723, 0.8402, 0.8176, 0.7478, 0.7931, 0.7590, 0.7679, 0.7722, 0.7419,\n",
       "                      0.8455, 0.8105, 0.7702, 0.1894, 0.7575, 0.8039, 0.7087, 0.8139, 0.7410,\n",
       "                      0.7574, 0.7698, 0.8436, 0.7020, 0.8144, 0.8504, 0.7629, 0.7565, 0.8037,\n",
       "                      0.7956, 0.7481, 0.8002, 0.8292, 0.7613, 0.7805, 0.7918, 0.7502, 0.7258,\n",
       "                      0.7952, 0.7367, 0.7613, 0.7873, 0.8418, 0.7402, 0.8480, 0.7626, 0.7613,\n",
       "                      0.8339, 0.8246, 0.8121, 0.7789, 0.8071, 0.7960, 0.7724, 0.8068, 0.7558,\n",
       "                      0.7241, 0.8153, 0.7867, 0.8150, 0.8376, 0.7653, 0.7961, 0.7770, 0.7868,\n",
       "                      0.7810, 0.7285, 0.8333, 0.8513, 0.8185, 0.7873, 0.8129, 0.7696, 0.7869,\n",
       "                      0.7916, 0.7828, 0.7594, 0.7782, 0.7397, 0.8249, 0.7871, 0.7337, 0.8096,\n",
       "                      0.8015, 0.8072, 0.8127, 0.8073, 0.8241, 0.7816, 0.7980, 0.8067, 0.7360,\n",
       "                      0.8008, 0.7874, 0.7547, 0.7499, 0.8530, 0.8347, 0.8357, 0.7545, 0.8218,\n",
       "                      0.7865, 0.7659, 0.7978, 0.3894, 0.8504, 0.7731, 0.8261, 0.7407, 0.7641,\n",
       "                      0.8221, 0.8125, 0.7605, 0.7927, 0.7696, 0.7890, 0.7901, 0.8225, 0.7939,\n",
       "                      0.7787, 0.8290, 0.7628, 0.8443, 0.8131, 0.7981, 0.8500, 0.7822, 0.7747,\n",
       "                      0.8004, 0.8012, 0.8013, 0.7760, 0.8480, 0.8222, 0.8071, 0.8514, 0.8075,\n",
       "                      0.7686, 0.8000, 0.7981, 0.8115, 0.7688, 0.7491, 0.7228, 0.7477, 0.7759,\n",
       "                      0.7782, 0.7649, 0.7846, 0.8375, 0.8502, 0.8160, 0.7998, 0.7635, 0.7919,\n",
       "                      0.7674, 0.8072, 0.7170, 0.7488, 0.7790, 0.8036, 0.7731, 0.7725, 0.8084,\n",
       "                      0.7572, 0.8030, 0.8183, 0.7986, 0.7762, 0.8034, 0.7946, 0.7444, 0.7568,\n",
       "                      0.7951, 0.7944, 0.8090, 0.7813, 0.7715, 0.8304, 0.7473, 0.7707, 0.7929,\n",
       "                      0.8490, 0.8133, 0.7751, 0.7681, 0.8083, 0.7452, 0.7908, 0.7847, 0.7887,\n",
       "                      0.8005, 0.8240, 0.8498, 0.7918, 0.7476, 0.7663, 0.8024, 0.8205, 0.7919,\n",
       "                      0.8508, 0.7986, 0.7535, 0.7956, 0.7861, 0.7632, 0.8156, 0.7884, 0.8458,\n",
       "                      0.7718, 0.7691, 0.8593, 0.7867, 0.8163, 0.7609, 0.8570, 0.7807, 0.7828,\n",
       "                      0.7922, 0.7932, 0.8205, 0.7787, 0.7434, 0.8264, 0.8019, 0.8123, 0.7826,\n",
       "                      0.8344, 0.8061, 0.8114, 0.8005, 0.7803, 0.7919, 0.8019, 0.7385, 0.7827,\n",
       "                      0.7882, 0.7580, 0.8073, 0.7671, 0.7233, 0.8349, 0.7753, 0.7889, 0.7716,\n",
       "                      0.7417, 0.7831, 0.8026, 0.7882, 0.7945, 0.7643, 0.7396, 0.7784, 0.7709,\n",
       "                      0.7844, 0.7758, 0.8153, 0.8057, 0.7837, 0.7592, 0.8026, 0.7710, 0.7377,\n",
       "                      0.8107, 0.8225, 0.7758, 0.8137, 0.8077, 0.8135, 0.8199, 0.7819, 0.8193,\n",
       "                      0.7908, 0.8064, 0.8322, 0.7516, 0.8061, 0.7866, 0.7909, 0.7705, 0.7614,\n",
       "                      0.8109, 0.7965, 0.7852, 0.7405, 0.7640, 0.7610, 0.8046, 0.7544, 0.7751,\n",
       "                      0.7528, 0.7939, 0.7879, 0.7937, 0.7812, 0.7738, 0.7937, 0.8260, 0.7642,\n",
       "                      0.7547, 0.8200, 0.7725, 0.8045, 0.7985, 0.8024, 0.8253, 0.8154, 0.8002,\n",
       "                      0.7738, 0.8337, 0.7896, 0.7982, 0.8070, 0.7690, 0.7912, 0.8549, 0.8316,\n",
       "                      0.7193, 0.7525, 0.8217, 0.7602, 0.7805, 0.7610, 0.7303, 0.7394, 0.7572,\n",
       "                      0.7604, 0.7657, 0.8412, 0.8389, 0.7747, 0.7908, 0.7885, 0.8079, 0.8271,\n",
       "                      0.7793, 0.7712, 0.7534, 0.7275, 0.7792, 0.7623, 0.7928, 0.8190, 0.7563,\n",
       "                      0.7640, 0.8124, 0.8612, 0.8186, 0.7720, 0.7788, 0.8421, 0.7736, 0.7361,\n",
       "                      0.7742, 0.8332, 0.7825, 0.7500, 0.7480, 0.7713, 0.8604, 0.7940, 0.7652,\n",
       "                      0.7967, 0.7843, 0.7949, 0.8153, 0.7616, 0.7891, 0.7947, 0.8067, 0.8154,\n",
       "                      0.8211, 0.7933, 0.8134], device='cuda:0')),\n",
       "             ('layers.5.mha.query.weight',\n",
       "              tensor([[ 0.0161,  0.0269,  0.0227,  ...,  0.0105,  0.0467,  0.0082],\n",
       "                      [-0.0212, -0.0834,  0.0385,  ...,  0.0528,  0.0226, -0.0366],\n",
       "                      [-0.0135,  0.0450, -0.0628,  ..., -0.0530,  0.0335, -0.0129],\n",
       "                      ...,\n",
       "                      [-0.0705,  0.0643,  0.0800,  ...,  0.0158, -0.0079, -0.0270],\n",
       "                      [-0.0007, -0.0669,  0.0387,  ..., -0.0348,  0.0240, -0.0816],\n",
       "                      [-0.0406,  0.0324,  0.0307,  ..., -0.0522,  0.0206, -0.0075]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.mha.key.weight',\n",
       "              tensor([[ 0.0102,  0.0391, -0.0552,  ..., -0.0084,  0.0014,  0.0311],\n",
       "                      [ 0.0012,  0.0886,  0.0147,  ...,  0.0083, -0.0076,  0.0509],\n",
       "                      [-0.0429, -0.0327,  0.0808,  ..., -0.0478, -0.0413, -0.0730],\n",
       "                      ...,\n",
       "                      [-0.0679, -0.0168, -0.0676,  ..., -0.0034, -0.0486, -0.0218],\n",
       "                      [ 0.0339,  0.0031,  0.0454,  ..., -0.0009, -0.0016,  0.0255],\n",
       "                      [-0.0139,  0.0757,  0.0090,  ...,  0.0090, -0.0365, -0.0205]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.mha.value.weight',\n",
       "              tensor([[-0.0425,  0.0039,  0.0766,  ...,  0.0303,  0.0044,  0.0435],\n",
       "                      [ 0.0246, -0.0696, -0.0391,  ...,  0.0921,  0.0143,  0.0557],\n",
       "                      [-0.0145, -0.0020, -0.0735,  ..., -0.0098, -0.0453, -0.0382],\n",
       "                      ...,\n",
       "                      [ 0.0063, -0.0601,  0.0012,  ..., -0.0106,  0.0046,  0.0026],\n",
       "                      [-0.0877, -0.0295, -0.0409,  ..., -0.1542, -0.0256, -0.0326],\n",
       "                      [ 0.0028, -0.0363,  0.0294,  ..., -0.0157, -0.0242, -0.0308]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.mha.proj.weight',\n",
       "              tensor([[ 0.0667, -0.0334,  0.0062,  ...,  0.0360,  0.0182,  0.0425],\n",
       "                      [ 0.0390,  0.0130, -0.0020,  ...,  0.0481,  0.0573, -0.0317],\n",
       "                      [-0.0195, -0.0168, -0.0185,  ..., -0.0689,  0.0152, -0.0336],\n",
       "                      ...,\n",
       "                      [ 0.0326,  0.0429, -0.0214,  ...,  0.0481, -0.0393, -0.0604],\n",
       "                      [-0.0321,  0.0527, -0.0164,  ...,  0.0456, -0.0580,  0.0643],\n",
       "                      [-0.0062, -0.0411, -0.0029,  ...,  0.0669,  0.0214,  0.0531]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.w.weight',\n",
       "              tensor([[ 0.0304,  0.0359,  0.0561,  ...,  0.0278,  0.0113, -0.0486],\n",
       "                      [ 0.0173,  0.0422,  0.1134,  ..., -0.0434,  0.0491, -0.0473],\n",
       "                      [-0.0525,  0.0399,  0.0769,  ...,  0.0255, -0.0035, -0.0631],\n",
       "                      ...,\n",
       "                      [-0.0182,  0.0644,  0.0005,  ...,  0.0356, -0.0169,  0.0058],\n",
       "                      [-0.0489, -0.0421, -0.1072,  ..., -0.0758,  0.0189,  0.0538],\n",
       "                      [ 0.0012,  0.0645,  0.0114,  ...,  0.0819,  0.0356, -0.0298]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.w.bias',\n",
       "              tensor([-0.1045, -0.1058,  0.0204,  ..., -0.0332,  0.0337, -0.0598],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.v.weight',\n",
       "              tensor([[ 0.0301, -0.0279,  0.0760,  ...,  0.0006, -0.0202, -0.0344],\n",
       "                      [ 0.0174,  0.0049, -0.0016,  ...,  0.0433, -0.0752,  0.0061],\n",
       "                      [ 0.0044,  0.0148, -0.0041,  ..., -0.0175,  0.0517, -0.0256],\n",
       "                      ...,\n",
       "                      [-0.0107,  0.0519,  0.0743,  ..., -0.0665,  0.0197, -0.0424],\n",
       "                      [-0.0593,  0.0424, -0.0047,  ...,  0.0050,  0.0781,  0.0045],\n",
       "                      [-0.0529, -0.0326, -0.0376,  ..., -0.0292, -0.0008,  0.0164]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.v.bias',\n",
       "              tensor([-0.0714,  0.0678,  0.0039,  ...,  0.0106,  0.0783, -0.0575],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.w2.weight',\n",
       "              tensor([[ 0.0520, -0.0180, -0.0532,  ...,  0.0455,  0.0031, -0.0268],\n",
       "                      [ 0.0609, -0.0016, -0.0123,  ..., -0.0141,  0.0515,  0.0060],\n",
       "                      [-0.0083, -0.0084, -0.0069,  ...,  0.0084, -0.0697,  0.0391],\n",
       "                      ...,\n",
       "                      [-0.0016, -0.0166,  0.0249,  ..., -0.0077, -0.0303,  0.0203],\n",
       "                      [-0.0334, -0.0512, -0.0858,  ..., -0.0106,  0.0661,  0.0034],\n",
       "                      [-0.0135,  0.0204,  0.0383,  ...,  0.0119,  0.0492,  0.0754]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.5.ffn.w2.bias',\n",
       "              tensor([ 5.9386e-02, -2.4641e-02,  3.2155e-02, -7.4843e-02,  1.1010e-01,\n",
       "                       4.8653e-02, -3.4017e-02, -2.3201e-02, -1.9285e-03, -7.7841e-02,\n",
       "                      -3.9081e-02,  3.6233e-02, -7.0078e-02, -3.3873e-02, -3.2809e-02,\n",
       "                      -3.9690e-02,  1.3382e-02, -3.6632e-02,  8.2195e-04, -4.7078e-02,\n",
       "                       4.0575e-02, -5.5531e-02, -4.1546e-02, -1.3860e-02, -1.4277e-02,\n",
       "                      -3.8556e-02, -1.7810e-02, -5.6789e-02,  4.2674e-02, -5.8463e-02,\n",
       "                      -9.7705e-03, -9.0093e-02,  3.5481e-02, -6.1052e-02,  7.0971e-02,\n",
       "                       3.9411e-02,  3.4402e-02, -8.0855e-02,  7.2061e-02, -6.2712e-02,\n",
       "                      -7.1217e-02,  8.2548e-02,  7.1341e-02,  4.1533e-03, -6.9070e-02,\n",
       "                      -9.8404e-02,  3.9987e-02, -3.5236e-02, -1.8925e-02, -9.7945e-02,\n",
       "                      -3.6063e-02,  8.1204e-03,  1.1241e-02,  5.8383e-02,  6.0668e-03,\n",
       "                      -2.9347e-02,  6.0401e-02,  6.0918e-02,  1.7189e-01,  2.3036e-02,\n",
       "                       5.4009e-02,  1.1117e-02, -1.4415e-01,  2.6261e-02,  1.0767e-01,\n",
       "                       2.6193e-02, -7.5683e-03, -1.7329e-02, -9.3911e-02, -9.8956e-03,\n",
       "                      -1.9789e-02, -7.0055e-02, -1.5259e-02, -5.6652e-02,  3.7232e-02,\n",
       "                      -3.0678e-02, -6.3923e-02, -1.0512e-01,  1.4915e-02, -4.6319e-02,\n",
       "                       5.4192e-02,  6.7867e-02, -1.4954e-01,  8.4355e-02, -6.8986e-03,\n",
       "                      -1.2303e-01,  6.0590e-02,  3.2787e-04, -2.6387e-02,  1.4031e-02,\n",
       "                      -1.4443e-01,  2.2043e-02,  1.0141e-01,  4.3137e-02,  5.5492e-02,\n",
       "                      -6.9802e-02, -5.3315e-02, -3.1516e-02, -5.5255e-02, -6.5816e-03,\n",
       "                      -3.1674e-02,  2.1484e-02, -6.7211e-02,  5.6645e-02,  1.7486e-02,\n",
       "                       6.4453e-03, -5.6632e-03, -2.3468e-02,  1.3915e-02, -1.6002e-02,\n",
       "                      -8.2706e-02,  1.1034e-02,  3.2373e-02, -6.9651e-02,  4.8552e-03,\n",
       "                       9.6845e-02,  2.7255e-02,  4.6171e-03,  3.0361e-02, -9.3073e-02,\n",
       "                      -7.6222e-02,  1.7743e-02,  6.7310e-02, -1.9146e-02, -2.9484e-02,\n",
       "                      -1.1929e-01,  3.0291e-04, -1.4205e-01, -1.1362e-01, -1.3663e-03,\n",
       "                      -3.3677e-02,  5.1093e-02,  1.8390e-01, -1.4622e-02,  3.3115e-02,\n",
       "                      -1.1568e-01, -4.5335e-02,  1.5130e-02,  1.3365e-02, -1.4679e-01,\n",
       "                       8.3169e-02,  3.0184e-02, -1.0119e-02,  4.3952e-02,  6.9630e-02,\n",
       "                       5.9853e-02, -4.5542e-02,  1.9697e-02, -6.3281e-02, -1.7825e-01,\n",
       "                       6.7409e-02, -1.4615e-02,  2.0737e-02,  4.0769e-03, -1.4835e-01,\n",
       "                      -2.8418e-03, -9.7734e-02,  2.9203e-02, -1.0936e-02,  5.4868e-03,\n",
       "                      -7.6553e-02, -1.2777e-05,  7.3857e-02,  1.7798e-02, -7.6850e-02,\n",
       "                       1.8789e-02, -3.6430e-02,  1.5959e-02,  1.0805e-01, -2.7971e-02,\n",
       "                      -1.3341e-01, -2.9099e-02, -5.9121e-02,  1.5804e-01,  7.5063e-02,\n",
       "                       4.6517e-02,  2.6294e-03,  8.3593e-02, -8.7346e-02,  7.3268e-02,\n",
       "                      -1.2370e-01, -6.7912e-02, -4.3369e-02, -6.9006e-03, -2.5186e-02,\n",
       "                      -7.0355e-02,  8.6045e-02, -3.9438e-02, -1.4291e-02, -1.9675e-02,\n",
       "                      -4.1693e-02, -1.3598e-01, -1.6485e-01,  1.6503e-02, -2.1467e-02,\n",
       "                      -1.5614e-02, -1.9757e-02, -2.0157e-02,  7.9924e-02, -2.4071e-02,\n",
       "                       9.5360e-02,  3.6813e-04, -7.0391e-02,  1.1476e-01, -9.9058e-05,\n",
       "                       1.7821e-03, -8.9967e-02, -7.4043e-02, -6.6680e-02, -5.1188e-02,\n",
       "                      -1.6798e-02, -9.0141e-02,  5.3899e-02, -4.0169e-02,  1.0899e-01,\n",
       "                       9.2706e-03,  7.0765e-02, -1.8740e-03,  8.6900e-02, -5.7749e-02,\n",
       "                      -4.7336e-02, -3.4824e-02, -1.4841e-02, -6.7158e-02,  7.5993e-03,\n",
       "                       3.5418e-02, -7.3135e-01,  5.3897e-02, -2.9172e-02, -7.1133e-02,\n",
       "                       6.7026e-02,  2.6500e-02, -4.8364e-02, -3.3118e-02,  3.7555e-02,\n",
       "                       3.6093e-02, -7.4626e-03,  4.0858e-03, -8.2923e-02,  5.6271e-02,\n",
       "                       1.7367e-02, -7.9206e-02, -7.5727e-02, -4.6935e-02,  3.5442e-02,\n",
       "                       2.9929e-02,  6.6251e-02,  4.5465e-02, -4.3899e-02,  1.1845e-02,\n",
       "                      -1.1971e-02,  3.2229e-02, -1.8616e-02,  2.0976e-02, -1.0296e-01,\n",
       "                       1.0499e-01, -1.4784e-02,  2.7599e-02, -1.1825e-01, -6.6375e-03,\n",
       "                      -8.7034e-03,  7.0243e-02,  2.2572e-01, -2.5610e-02, -5.7688e-02,\n",
       "                       1.2709e-01, -4.4758e-02,  2.7093e-02, -4.7308e-02,  4.8845e-02,\n",
       "                       6.3556e-03, -1.0839e-01,  5.7814e-02,  2.4261e-02,  7.7043e-02,\n",
       "                       6.1519e-02, -9.4723e-04, -5.4680e-02,  5.7024e-02, -1.0153e-01,\n",
       "                       3.8328e-02, -8.5807e-02,  1.0910e-01, -4.9657e-02, -1.3517e-01,\n",
       "                      -2.0663e-02,  2.7986e-03,  1.2901e-02,  2.7780e-02,  2.5345e-02,\n",
       "                       1.3521e-02, -6.1361e-02, -1.1146e-01,  2.3721e-02, -3.1530e-02,\n",
       "                      -7.1934e-02,  3.4215e-02,  4.4723e-02, -1.4667e-01, -2.2908e-02,\n",
       "                       1.6939e-03,  2.2478e-01,  5.6694e-02, -4.5678e-02, -9.9261e-02,\n",
       "                      -5.7163e-02, -1.0770e-01,  5.9552e-03,  1.0992e-02, -5.5735e-02,\n",
       "                       2.4730e-01,  2.0337e-02,  3.1793e-02, -1.8120e-02, -6.1036e-02,\n",
       "                       8.0297e-02, -6.2574e-03, -7.0893e-02,  4.3013e-02,  4.5156e-03,\n",
       "                       1.4810e-01,  3.9584e-02, -1.3214e-02,  1.1988e-02,  1.1707e-02,\n",
       "                      -3.2156e-03, -2.9401e-02, -9.8575e-03, -6.2415e-02,  5.7360e-02,\n",
       "                      -5.5025e-03,  1.3529e-01,  1.6155e-02,  5.6145e-03,  1.5337e-02,\n",
       "                       6.6793e-02, -1.1234e-01,  2.4495e-02,  3.5228e-02,  5.2979e-02,\n",
       "                       9.4783e-02,  3.5775e-02, -3.3628e-03,  2.7933e-02, -2.3786e-02,\n",
       "                       6.1298e-02, -9.8861e-02, -6.2730e-02, -6.1255e-02,  4.3548e-02,\n",
       "                      -8.4678e-02,  1.9190e-02,  1.0358e-01, -1.8075e-02,  9.7234e-02,\n",
       "                      -2.6354e-02, -1.2723e-01,  7.0630e-02, -7.3784e-02, -2.5761e-02,\n",
       "                       7.1796e-02,  5.3030e-02,  8.6742e-02, -2.7018e-02, -5.7740e-02,\n",
       "                       1.0483e-01,  1.5590e-02, -8.4215e-02,  3.6856e-02, -3.3652e-03,\n",
       "                      -5.1032e-02,  6.4556e-02,  5.0721e-02,  3.8120e-02, -5.0000e-02,\n",
       "                       1.1389e-01, -6.4040e-02,  1.0570e-01, -7.5065e-02, -4.8455e-02,\n",
       "                      -3.6797e-02,  6.7181e-02,  1.3683e-03,  7.4953e-02,  4.3815e-02,\n",
       "                       3.1522e-02, -5.5546e-02, -3.2054e-03, -4.9326e-02,  8.5206e-02,\n",
       "                      -1.8564e-02,  1.3162e-01, -3.5699e-02, -4.6670e-02,  2.7334e-02,\n",
       "                       2.1080e-02, -3.6589e-02,  7.6167e-02, -1.7993e-02,  1.6691e-02,\n",
       "                      -1.0577e-01, -3.9507e-02, -7.6359e-02,  7.2279e-02, -8.4618e-02,\n",
       "                      -1.4100e-02, -1.2880e-02,  8.9026e-02, -1.1047e+00, -8.8598e-03,\n",
       "                       6.3900e-02,  7.5843e-02, -2.9881e-02,  2.1715e-01,  6.1342e-02,\n",
       "                      -8.9407e-02, -3.9358e-02, -1.5231e-01, -4.4226e-02, -7.9947e-02,\n",
       "                       9.7367e-02, -1.0659e-02, -2.7939e-02, -2.4268e-02, -1.2737e-02,\n",
       "                       3.2056e-02, -1.2115e-01, -9.3526e-02, -4.9550e-03, -8.3266e-02,\n",
       "                      -4.5184e-03, -9.2715e-03,  1.0137e-01,  3.5128e-02,  2.3958e-01,\n",
       "                      -2.0133e-02, -1.4541e-02,  7.0288e-02, -4.1791e-02,  2.8630e-02,\n",
       "                       9.2839e-03, -2.4229e-02, -7.6631e-02, -1.8657e-02, -6.7939e-02,\n",
       "                      -8.4447e-02,  1.7074e-02,  2.5114e-02,  9.1665e-02, -1.3591e-02,\n",
       "                      -3.1329e-02, -2.0547e-02,  1.0196e-01,  7.3034e-02, -8.0938e-02,\n",
       "                       2.3526e-02,  1.0404e-01, -9.0891e-02, -6.3980e-02, -8.1995e-02,\n",
       "                       8.1058e-02, -5.1262e-02,  1.6530e-02,  3.3518e-02,  2.0431e-02,\n",
       "                      -3.8361e-02,  5.7970e-02, -3.0233e-02,  6.3542e-02,  3.2410e-02,\n",
       "                       6.6365e-02,  2.7474e-02,  1.4615e-02, -5.4228e-02, -4.5687e-02,\n",
       "                      -9.2728e-02, -2.6530e-02,  2.7928e-02,  2.1801e-02, -4.9928e-02,\n",
       "                       9.8595e-02,  1.2362e-01, -1.4351e-03, -2.6189e-02,  4.6073e-03,\n",
       "                       1.7361e-01, -1.3129e-01, -4.2465e-03,  3.5531e-02, -1.0198e-03,\n",
       "                      -9.1926e-03,  3.4462e-02, -3.9786e-03,  5.7076e-02,  3.7982e-02,\n",
       "                      -5.5675e-02,  1.8265e-02, -2.8540e-02,  3.6052e-01, -1.0108e-01,\n",
       "                       1.2777e-01, -6.4047e-02, -7.4996e-02,  9.3833e-02, -7.9283e-02,\n",
       "                       4.3917e-02, -8.4911e-02, -9.5800e-02,  1.0734e-01, -9.9174e-02,\n",
       "                      -2.9496e-02,  2.2111e-02,  1.0823e-02, -1.2791e-01, -3.8874e-02,\n",
       "                      -8.1960e-03,  5.1458e-02, -2.9739e-02, -1.3262e-02,  2.0693e-02,\n",
       "                      -6.8890e-02, -2.9952e-02,  1.9488e-02,  6.8771e-02, -2.0526e-02,\n",
       "                       1.6973e-02, -3.8923e-02, -1.1587e-01, -1.7048e-02, -6.2216e-03,\n",
       "                      -8.5891e-02, -5.0875e-02, -2.2615e-02,  7.7790e-02,  8.9450e-02,\n",
       "                       4.7505e-02,  4.8119e-02, -2.8143e-02,  2.6816e-02, -3.9070e-02,\n",
       "                      -6.7336e-02, -1.2520e-01, -5.1891e-02,  4.6975e-02, -4.3177e-02,\n",
       "                       1.8308e-02,  3.8917e-02, -1.1640e-01,  6.4066e-02, -6.3787e-02,\n",
       "                       1.7108e-02,  9.6653e-03,  2.9628e-02,  1.9465e-02, -2.4784e-02,\n",
       "                      -3.9954e-02,  3.2035e-02,  8.7672e-02, -3.8882e-02, -3.9300e-03,\n",
       "                      -1.6337e-02,  2.4020e-02, -3.5533e-02,  1.2344e-02, -4.3657e-03,\n",
       "                       3.6552e-03,  8.7669e-03, -1.8466e-01,  9.5896e-03, -1.6696e-02,\n",
       "                       1.6097e-01, -8.7817e-02,  8.3580e-02,  8.0201e-02, -1.0470e-01,\n",
       "                       1.6088e-02, -4.6352e-02, -2.1010e-02, -7.1022e-02, -1.0927e-01,\n",
       "                       6.5804e-03,  4.7164e-03,  9.4202e-02, -5.5901e-02,  3.8250e-02,\n",
       "                      -2.5602e-02,  7.5516e-03,  4.1052e-02,  7.6474e-02, -6.0632e-02,\n",
       "                      -1.7368e-02, -8.5313e-03, -6.4170e-02,  1.5460e-02,  7.0273e-03,\n",
       "                       1.0131e-02, -2.8547e-02, -1.2351e-01,  6.8618e-02, -5.3791e-02,\n",
       "                       5.7777e-03, -2.2655e-02, -4.3480e-02,  5.2742e-02, -4.1278e-02,\n",
       "                       6.1504e-02,  5.9920e-02,  3.8504e-02, -9.0791e-03, -7.3352e-02,\n",
       "                       7.2615e-02, -2.3753e-02, -5.8660e-02,  3.8970e-02, -9.7749e-02,\n",
       "                      -1.2983e-01,  2.5633e-02, -7.0888e-02,  1.3890e-01, -6.4184e-02,\n",
       "                      -5.9608e-02,  8.1501e-02, -7.8375e-02,  1.2205e-02,  8.0337e-02,\n",
       "                       7.6636e-02,  1.8734e-02, -5.6270e-02,  8.2324e-02,  2.8930e-02,\n",
       "                       4.6086e-02, -1.7230e-02, -3.0953e-02,  3.1661e-02,  1.9888e-02,\n",
       "                       5.3502e-02, -7.5246e-02,  5.7766e-02, -9.3988e-02, -5.8099e-02,\n",
       "                       4.8757e-02,  9.9160e-03,  1.0115e-01,  1.1026e-02,  7.7259e-02,\n",
       "                       9.8408e-02,  2.9089e-02,  4.9811e-02,  3.7194e-02,  4.7594e-04,\n",
       "                       6.0436e-02,  7.5400e-02, -6.7267e-02, -4.2795e-02,  1.6868e-02,\n",
       "                      -3.5268e-03, -7.7561e-02,  2.9092e-03, -4.9743e-02,  5.6064e-02,\n",
       "                      -5.5867e-02, -6.8680e-02, -4.2737e-02,  8.9314e-02,  1.5172e-02,\n",
       "                       6.3710e-02,  8.7531e-02, -3.5851e-03,  1.0035e-01,  2.1269e-01,\n",
       "                      -2.9959e-02,  2.1287e-02, -2.6250e-02, -9.6300e-02, -3.1926e-02,\n",
       "                       8.6262e-02, -2.0428e-01, -4.6591e-02, -2.4291e-02,  1.1113e-02,\n",
       "                      -7.6629e-02, -9.0885e-02, -4.9605e-02,  1.4998e-01, -1.0066e-01,\n",
       "                       2.8361e-02, -1.1372e-01, -1.8452e-01,  3.6291e-02, -1.9837e-02,\n",
       "                      -8.6856e-02,  5.2576e-02,  2.7558e-03,  7.3395e-02,  6.3217e-02,\n",
       "                      -1.2586e-01,  2.7908e-02, -5.2878e-03,  9.2300e-02, -1.3711e-02,\n",
       "                      -1.1961e-02, -1.6291e-01,  3.1069e-02,  8.5891e-02,  1.0689e-02,\n",
       "                      -8.1467e-03,  7.8807e-02, -5.8772e-02, -8.0041e-04, -1.0828e-03,\n",
       "                      -4.2040e-02,  2.2112e-02,  4.7464e-02,  6.7794e-02,  4.5788e-02,\n",
       "                       1.8179e-02,  1.0557e-01,  1.2764e-02, -1.1608e-02, -3.2980e-05,\n",
       "                      -5.2637e-02,  3.6104e-03, -5.6118e-02,  8.1312e-02, -1.2749e-01,\n",
       "                       1.2604e-02, -5.0498e-02, -9.7030e-02,  5.2282e-02,  3.3424e-02,\n",
       "                      -1.1995e-01,  1.0423e-01, -8.7599e-02, -6.1369e-02,  9.3744e-02,\n",
       "                       1.1003e-01,  1.0235e-01,  1.2685e-01,  5.1668e-02, -5.0306e-02,\n",
       "                      -2.8143e-02,  1.8235e-03, -1.0131e-02,  1.3794e-02,  3.5927e-02,\n",
       "                       3.0675e-02,  5.5714e-02,  6.8375e-02, -1.6981e-02,  1.0470e-03,\n",
       "                      -1.0399e-01,  1.8652e-02,  1.3939e-02,  6.4356e-03, -4.8698e-02,\n",
       "                       3.9253e-02,  7.0715e-02,  1.2413e-03, -7.4540e-02, -1.4902e-02,\n",
       "                       5.1033e-02,  5.7252e-02, -6.2487e-02, -6.7098e-02, -3.5471e-02,\n",
       "                       1.6308e-02,  2.4502e-02, -8.8833e-02], device='cuda:0')),\n",
       "             ('layers.5.anorm.weight',\n",
       "              tensor([0.9035, 0.9360, 1.0231, 1.0134, 0.9035, 0.9914, 1.0191, 0.9685, 0.9414,\n",
       "                      0.9319, 0.9485, 1.0451, 0.9972, 0.9893, 1.0109, 0.9843, 0.9815, 1.0063,\n",
       "                      0.9793, 0.9881, 0.9967, 1.0273, 0.9158, 0.9803, 0.9405, 0.9634, 0.9787,\n",
       "                      0.9761, 1.0008, 0.9881, 0.9845, 0.9850, 0.9326, 0.9817, 0.9168, 0.9914,\n",
       "                      0.9727, 0.8564, 0.9344, 0.9759, 0.9661, 0.9746, 1.0277, 0.9403, 1.0449,\n",
       "                      1.0301, 0.9601, 0.9336, 0.9680, 1.0046, 1.0004, 0.9497, 0.9517, 1.0013,\n",
       "                      0.9910, 0.9846, 0.9861, 0.9432, 0.8743, 1.0223, 0.9568, 0.9274, 0.9455,\n",
       "                      0.9271, 1.0301, 0.9669, 0.9627, 1.0070, 1.0079, 0.9275, 0.9549, 0.9962,\n",
       "                      0.9813, 0.9607, 0.8986, 0.9543, 0.9668, 1.0353, 1.0225, 1.0879, 1.0181,\n",
       "                      0.9538, 0.9198, 0.9513, 0.9114, 0.9389, 0.9589, 0.9788, 0.9482, 0.9957,\n",
       "                      0.9377, 1.0323, 1.0173, 0.9979, 0.9659, 1.0575, 0.9170, 0.9595, 1.0255,\n",
       "                      0.8913, 0.9011, 0.9903, 0.9736, 0.8480, 0.9903, 0.9600, 0.9960, 0.9800,\n",
       "                      1.0367, 0.9646, 0.9838, 0.9363, 1.0208, 0.9193, 0.9911, 0.8917, 0.9553,\n",
       "                      0.9263, 0.9657, 0.9909, 1.0336, 1.0173, 0.9386, 0.9571, 1.0463, 0.9648,\n",
       "                      0.8946, 0.9846, 0.9803, 0.9612, 0.9381, 0.9960, 0.8819, 0.9817, 1.0047,\n",
       "                      0.8745, 0.9813, 0.9443, 0.9920, 0.9834, 0.9596, 1.0152, 0.9659, 0.9502,\n",
       "                      1.0491, 0.9504, 1.0082, 1.0347, 1.0144, 0.9658, 0.9096, 1.0328, 0.9217,\n",
       "                      1.0198, 0.8567, 0.9971, 0.9990, 0.9898, 1.0022, 0.9409, 0.9937, 0.9463,\n",
       "                      1.0151, 0.9981, 0.9510, 1.0463, 0.9647, 0.8870, 0.9056, 1.0018, 0.9315,\n",
       "                      0.9123, 0.9651, 0.9317, 0.9645, 0.9765, 0.9865, 0.9329, 0.9898, 1.0508,\n",
       "                      1.0363, 0.9981, 0.9473, 0.9936, 0.9379, 1.0079, 1.0165, 0.9778, 0.9920,\n",
       "                      1.0484, 0.9146, 0.9305, 1.0115, 0.9659, 0.9683, 0.9550, 0.9366, 1.0215,\n",
       "                      0.9566, 1.0013, 0.9293, 0.9763, 1.0264, 0.8812, 0.9817, 0.9784, 0.9400,\n",
       "                      1.0071, 0.9782, 0.8912, 0.9899, 0.8827, 1.0239, 0.9676, 0.8981, 1.0190,\n",
       "                      0.9049, 0.9804, 0.9653, 0.9867, 0.9510, 1.0112, 0.9807, 1.0316, 0.9917,\n",
       "                      0.9566, 1.0678, 0.9719, 0.9156, 0.9716, 0.9673, 0.9759, 0.9555, 1.0131,\n",
       "                      1.0034, 1.0103, 0.9956, 0.9651, 0.9949, 0.9583, 0.9780, 0.9747, 0.9805,\n",
       "                      0.9427, 0.9591, 0.9770, 1.0237, 0.9701, 0.9830, 0.9621, 0.9767, 0.9784,\n",
       "                      0.9975, 1.0457, 0.9020, 0.9780, 0.9965, 0.9840, 1.0335, 0.9250, 1.0336,\n",
       "                      0.9729, 1.0251, 1.0116, 0.9932, 0.8895, 0.9150, 0.9486, 0.9352, 0.9884,\n",
       "                      0.9980, 0.9163, 0.9279, 0.9907, 0.9539, 0.9785, 1.0112, 0.8954, 0.9329,\n",
       "                      0.9861, 0.9251, 1.0118, 1.0246, 0.9612, 1.0418, 0.9793, 0.9660, 0.9109,\n",
       "                      0.9604, 0.9894, 1.0310, 0.9416, 0.9409, 0.9199, 1.0339, 0.8992, 0.9815,\n",
       "                      1.0106, 0.9861, 1.0129, 1.0450, 0.8736, 0.9068, 0.9845, 0.9056, 1.0019,\n",
       "                      0.9355, 1.0640, 0.9416, 1.0529, 0.9388, 0.9421, 0.9720, 0.9174, 0.9069,\n",
       "                      1.0191, 0.9717, 1.0610, 1.0058, 0.9706, 0.9370, 0.9477, 0.9591, 0.9106,\n",
       "                      0.9519, 0.9591, 0.9700, 0.9627, 0.9911, 1.0155, 0.9345, 1.0143, 1.0069,\n",
       "                      0.9918, 0.9934, 0.9706, 0.9730, 0.9707, 0.9727, 0.9586, 0.9304, 1.0132,\n",
       "                      0.9703, 0.9771, 0.9345, 1.0079, 1.0436, 1.0150, 0.9423, 0.9898, 0.9819,\n",
       "                      0.9809, 0.9117, 0.9941, 1.0451, 0.9902, 1.0269, 1.0326, 0.9935, 1.0297,\n",
       "                      0.9956, 0.9893, 0.9475, 0.9572, 1.0138, 1.0012, 0.9645, 0.9574, 0.9998,\n",
       "                      0.8665, 0.9981, 0.9604, 1.0227, 1.0291, 0.9745, 0.9836, 0.9466, 0.9363,\n",
       "                      0.9906, 0.9898, 0.9534, 1.0517, 0.9681, 0.9804, 0.9950, 0.9549, 1.0218,\n",
       "                      0.9730, 0.8791, 0.9448, 0.9651, 1.0376, 0.9341, 1.0015, 0.9497, 0.9713,\n",
       "                      1.0153, 0.9998, 0.9778, 0.9608, 0.9935, 0.9487, 0.9864, 1.0488, 0.9688,\n",
       "                      0.9355, 1.0472, 0.9662, 0.6049, 0.9773, 0.9540, 0.8234, 1.0273, 0.8181,\n",
       "                      0.9994, 0.8982, 1.0734, 0.8720, 0.9026, 0.9356, 0.9229, 0.9419, 1.0375,\n",
       "                      0.9809, 1.0329, 1.0144, 1.0108, 0.9047, 0.9802, 0.9913, 0.9388, 0.9394,\n",
       "                      0.9839, 0.9799, 0.8855, 0.9691, 1.0277, 1.0209, 1.0514, 0.9455, 0.9647,\n",
       "                      0.9934, 0.9863, 0.9746, 0.9027, 0.9673, 1.0306, 0.9964, 0.9656, 0.9489,\n",
       "                      0.8252, 1.0815, 0.9548, 0.9549, 1.0535, 0.9520, 0.8777, 0.9243, 1.0045,\n",
       "                      0.9940, 0.8494, 0.9987, 1.0200, 0.9539, 0.9706, 0.9897, 0.9512, 1.0252,\n",
       "                      1.0127, 1.0501, 0.9509, 0.9519, 1.0044, 1.0152, 0.9587, 0.8769, 1.0553,\n",
       "                      1.0465, 0.9599, 1.0274, 0.9595, 0.9717, 0.9765, 0.9183, 0.9889, 0.9516,\n",
       "                      0.9663, 0.9992, 0.9340, 0.9271, 0.9695, 0.9315, 1.0290, 0.9610, 1.0323,\n",
       "                      0.9452, 0.9602, 0.9431, 0.6879, 0.9753, 0.9779, 0.9763, 0.9343, 0.9951,\n",
       "                      1.0040, 1.0433, 0.9469, 0.8849, 0.9075, 0.8708, 0.9609, 0.9351, 0.9453,\n",
       "                      0.9525, 1.0107, 0.9461, 1.0542, 1.0754, 0.9524, 0.9125, 0.9246, 0.9930,\n",
       "                      0.9668, 0.9579, 1.0163, 0.9686, 0.9842, 1.0457, 0.9264, 0.9838, 0.9934,\n",
       "                      0.9305, 0.8788, 1.0767, 1.0214, 1.0442, 0.9864, 0.9741, 1.0154, 0.9418,\n",
       "                      1.0134, 0.9780, 0.9237, 0.9646, 1.0118, 0.8988, 0.9481, 0.9487, 1.0287,\n",
       "                      0.9890, 0.9329, 0.9514, 0.9922, 0.9464, 0.9686, 0.9348, 0.9465, 0.9151,\n",
       "                      0.9980, 1.0197, 0.9901, 0.9365, 0.9063, 0.9939, 1.0033, 0.9461, 0.9319,\n",
       "                      0.9380, 1.0198, 1.0025, 1.0333, 0.9439, 0.8872, 0.9556, 1.0127, 1.0414,\n",
       "                      1.0334, 0.9087, 0.9885, 0.9296, 0.9599, 0.9748, 0.9834, 0.9411, 0.9987,\n",
       "                      0.9462, 1.0550, 1.0518, 0.9313, 0.9957, 0.9883, 0.9642, 0.9557, 0.9851,\n",
       "                      1.0282, 1.0065, 0.9923, 0.9461, 1.0030, 0.9648, 1.0503, 1.0379, 1.0456,\n",
       "                      0.9800, 1.0329, 0.9867, 0.9640, 0.9138, 0.9612, 0.9942, 0.9366, 1.0445,\n",
       "                      1.0433, 1.0351, 0.9900, 1.0350, 0.9974, 0.9453, 0.9438, 1.0205, 1.0173,\n",
       "                      0.9958, 0.9782, 0.9323, 1.0432, 0.9738, 0.9811, 0.9225, 1.0032, 1.0064,\n",
       "                      0.9450, 1.0669, 0.9312, 0.8403, 0.9949, 0.9816, 0.9988, 0.9042, 0.9478,\n",
       "                      0.9263, 0.9366, 0.9647, 0.9497, 1.0026, 0.9866, 1.0181, 0.9848, 0.8940,\n",
       "                      0.9670, 1.0534, 0.9309, 0.9805, 0.9253, 0.9825, 0.9461, 0.9742, 1.0496,\n",
       "                      0.9673, 1.0114, 1.0284, 0.9559, 0.9778, 1.0472, 0.9590, 0.9591, 0.9307,\n",
       "                      0.9578, 0.9808, 0.9324, 0.8742, 1.0124, 0.9180, 0.9536, 0.9296, 0.9341,\n",
       "                      0.9413, 0.9898, 1.0708, 0.9674, 0.9041, 0.9596, 1.0434, 0.9464, 0.9468,\n",
       "                      0.9304, 1.0042, 0.8809, 0.8762, 1.0045, 0.9563, 0.9755, 1.0675, 0.9930,\n",
       "                      0.9294, 1.0073, 1.0210, 0.9343, 1.0238, 0.9395, 0.9837, 1.0393, 0.9671,\n",
       "                      1.0115, 1.0233, 0.8905, 0.9055, 1.0469, 0.9669, 1.0141, 0.9692, 1.0334,\n",
       "                      0.8999, 0.9506, 0.9822, 1.1007, 1.0112, 1.0107, 0.9742, 1.0488, 0.9467,\n",
       "                      0.8715, 0.9694, 0.9587, 1.0902, 0.9517, 0.9804, 0.9997, 0.9310, 0.9995,\n",
       "                      0.9885, 0.9450, 0.9457, 0.9328, 1.0163, 0.9698, 1.0160, 1.0244, 0.9532,\n",
       "                      0.9931, 1.0106, 0.9768, 0.9846, 1.0663, 0.9138, 0.9837, 1.0149, 0.9476,\n",
       "                      0.9790, 1.0440, 0.9480, 0.7919, 0.9728, 1.0258, 0.9463, 0.9764, 1.0105,\n",
       "                      0.8616, 0.9575, 0.9901, 0.9352, 1.0684, 0.9444, 1.0202, 1.0189, 0.9531,\n",
       "                      0.9841, 0.9759, 1.0495], device='cuda:0')),\n",
       "             ('layers.5.fnorm.weight',\n",
       "              tensor([0.8457, 0.8541, 0.9014, 0.8541, 0.8774, 0.8355, 0.8244, 0.8250, 0.8433,\n",
       "                      0.8308, 0.8218, 0.8517, 0.8289, 0.8406, 0.8416, 0.8566, 0.8234, 0.8545,\n",
       "                      0.8324, 0.8442, 0.8957, 0.8150, 0.8320, 0.9178, 0.8700, 0.8712, 0.8400,\n",
       "                      0.8197, 0.8425, 0.8392, 0.8518, 0.8389, 0.8202, 0.8424, 0.8560, 0.8531,\n",
       "                      0.8256, 0.7805, 0.7796, 0.8546, 0.8729, 0.7910, 0.8629, 0.8670, 0.8248,\n",
       "                      0.8803, 0.8387, 0.8430, 0.8039, 0.8446, 0.9023, 0.8739, 0.8817, 0.8825,\n",
       "                      0.9008, 0.8809, 0.8868, 0.8392, 0.7228, 0.8169, 0.7865, 0.8668, 0.8388,\n",
       "                      0.8364, 0.8093, 0.8360, 0.9016, 0.8780, 0.8394, 0.7991, 0.7766, 0.9295,\n",
       "                      0.8271, 0.8563, 0.8120, 0.8381, 0.8625, 0.8328, 0.8489, 0.8092, 0.8411,\n",
       "                      0.8180, 0.7475, 0.9088, 0.8170, 0.8574, 0.8459, 0.8285, 0.8236, 0.8607,\n",
       "                      0.8224, 0.8409, 0.8439, 0.8493, 0.8090, 0.8427, 0.8124, 0.8349, 0.8468,\n",
       "                      0.8988, 0.8395, 0.9037, 0.8515, 0.7668, 0.8428, 0.8593, 0.8330, 0.8305,\n",
       "                      0.8907, 0.8850, 0.8415, 0.8627, 0.8436, 0.8543, 0.8461, 0.7888, 0.8286,\n",
       "                      0.8639, 0.8304, 0.8514, 0.8510, 0.8893, 0.8154, 0.8196, 0.8202, 0.8283,\n",
       "                      0.8440, 0.8479, 0.8245, 0.8157, 0.8344, 0.8238, 0.7324, 0.8404, 0.8485,\n",
       "                      0.8347, 0.8581, 0.8600, 0.8518, 0.8608, 0.8569, 0.8347, 0.8785, 0.8353,\n",
       "                      0.8719, 0.8555, 0.8656, 0.8274, 0.8347, 0.8369, 0.8137, 0.8419, 0.8609,\n",
       "                      0.8900, 0.7122, 0.8370, 0.8558, 0.8499, 0.8326, 0.8417, 0.7932, 0.8579,\n",
       "                      0.8706, 0.8107, 0.8453, 0.8774, 0.8325, 0.9084, 0.8350, 0.8723, 0.7950,\n",
       "                      0.8107, 0.8176, 0.7991, 0.8229, 0.8314, 0.8055, 0.8492, 0.8125, 0.8287,\n",
       "                      0.8381, 0.8188, 0.8518, 0.8965, 0.8518, 0.9057, 0.8818, 0.8498, 0.8746,\n",
       "                      0.8765, 0.8372, 0.7741, 0.8629, 0.8600, 0.8333, 0.8916, 0.7656, 0.8670,\n",
       "                      0.7552, 0.8578, 0.7547, 0.8422, 0.8773, 0.7794, 0.8483, 0.8851, 0.8904,\n",
       "                      0.8877, 0.8579, 0.8250, 0.8813, 0.8254, 0.8502, 0.8297, 0.8174, 0.8291,\n",
       "                      0.8460, 0.8579, 0.8924, 0.8678, 0.8391, 0.8946, 0.8303, 0.8245, 0.8388,\n",
       "                      0.8202, 0.2698, 0.8136, 0.8271, 0.8761, 0.8837, 0.8401, 0.8158, 0.8411,\n",
       "                      0.8399, 0.8832, 0.8311, 0.8479, 0.8273, 0.8123, 0.8033, 0.7876, 0.8746,\n",
       "                      0.8294, 0.8438, 0.8662, 0.8773, 0.8688, 0.8432, 0.8093, 0.8289, 0.8495,\n",
       "                      0.8406, 0.8311, 0.8209, 0.8595, 0.8630, 0.8414, 0.8354, 0.8439, 0.9004,\n",
       "                      0.8687, 0.7729, 0.8363, 0.8136, 0.8319, 0.7609, 0.8225, 0.8153, 0.8759,\n",
       "                      0.8318, 0.8330, 0.7796, 0.8393, 0.8541, 0.8664, 0.8558, 0.8222, 0.7731,\n",
       "                      0.8964, 0.8460, 0.8546, 0.8527, 0.8178, 0.7969, 0.8384, 0.8732, 0.8324,\n",
       "                      0.8407, 0.8639, 0.8700, 0.8436, 0.8727, 0.8417, 0.8602, 0.8160, 0.8222,\n",
       "                      0.8433, 0.8403, 0.8761, 0.8684, 0.8243, 0.8242, 0.8441, 0.8006, 0.8558,\n",
       "                      0.8114, 0.8266, 0.8354, 0.8842, 0.7334, 0.8223, 0.8533, 0.8513, 0.8106,\n",
       "                      0.8196, 0.8388, 0.8848, 0.8506, 0.8684, 0.8430, 0.8464, 0.8484, 0.8322,\n",
       "                      0.8247, 0.8636, 0.8590, 0.7998, 0.8788, 0.8346, 0.8520, 0.8134, 0.7912,\n",
       "                      0.8204, 0.8769, 0.8553, 0.8486, 0.8211, 0.8400, 0.8725, 0.8132, 0.8518,\n",
       "                      0.7905, 0.8616, 0.8001, 0.8110, 0.8412, 0.8630, 0.8180, 0.8450, 0.8371,\n",
       "                      0.8598, 0.8899, 0.8406, 0.8663, 0.8635, 0.8088, 0.8622, 0.8187, 0.8242,\n",
       "                      0.8778, 0.8321, 0.8250, 0.8333, 0.8938, 0.8052, 0.8533, 0.8254, 0.8182,\n",
       "                      0.8498, 0.8037, 0.8578, 0.8487, 0.8672, 0.8607, 0.8253, 0.8397, 0.8504,\n",
       "                      0.8320, 0.8363, 0.8325, 0.8389, 0.8513, 0.8358, 0.8330, 0.8129, 0.8996,\n",
       "                      0.8220, 0.8432, 0.8271, 0.8215, 0.8944, 0.8383, 0.8356, 0.8792, 0.8620,\n",
       "                      0.8826, 0.8645, 0.8738, 0.8588, 0.8194, 0.8098, 0.8713, 0.8397, 0.8081,\n",
       "                      0.8779, 0.8650, 0.8375, 0.1807, 0.8300, 0.8376, 0.7541, 0.8345, 0.7404,\n",
       "                      0.8193, 0.7796, 0.8645, 0.7641, 0.8593, 0.8521, 0.8511, 0.7885, 0.8609,\n",
       "                      0.8011, 0.8302, 0.8564, 0.8345, 0.7777, 0.8461, 0.8408, 0.8343, 0.8107,\n",
       "                      0.8147, 0.8318, 0.8606, 0.8636, 0.8683, 0.8373, 0.8496, 0.8118, 0.8466,\n",
       "                      0.8755, 0.8222, 0.8664, 0.8087, 0.8540, 0.8250, 0.8766, 0.8621, 0.8273,\n",
       "                      0.8256, 0.8145, 0.8127, 0.8206, 0.9065, 0.7968, 0.8422, 0.8617, 0.8276,\n",
       "                      0.8447, 0.7918, 0.9052, 0.8804, 0.8194, 0.8450, 0.9056, 0.8104, 0.8533,\n",
       "                      0.8301, 0.8388, 0.8326, 0.8422, 0.8133, 0.8661, 0.8295, 0.8160, 0.8458,\n",
       "                      0.8381, 0.8734, 0.8379, 0.8524, 0.8418, 0.8324, 0.8166, 0.8614, 0.7747,\n",
       "                      0.8903, 0.8364, 0.8164, 0.8019, 0.8602, 0.8231, 0.8333, 0.8721, 0.8550,\n",
       "                      0.8520, 0.8161, 0.8918, 0.4670, 0.8887, 0.8032, 0.8522, 0.7706, 0.8164,\n",
       "                      0.8507, 0.8509, 0.7581, 0.8253, 0.8563, 0.7826, 0.8134, 0.8379, 0.8254,\n",
       "                      0.7704, 0.8761, 0.7986, 0.9044, 0.8061, 0.8270, 0.8620, 0.8107, 0.7819,\n",
       "                      0.9029, 0.8472, 0.8337, 0.8684, 0.8759, 0.8320, 0.7803, 0.8846, 0.9288,\n",
       "                      0.8034, 0.8446, 0.8617, 0.8216, 0.8033, 0.8506, 0.8601, 0.7906, 0.8424,\n",
       "                      0.7947, 0.8257, 0.8525, 0.8767, 0.8666, 0.8071, 0.8209, 0.8409, 0.8324,\n",
       "                      0.8494, 0.7898, 0.8440, 0.7878, 0.8341, 0.8461, 0.8680, 0.8172, 0.8605,\n",
       "                      0.8402, 0.8408, 0.8957, 0.8162, 0.8117, 0.8547, 0.8353, 0.8126, 0.8247,\n",
       "                      0.8542, 0.8639, 0.8869, 0.8644, 0.8166, 0.8431, 0.8227, 0.8272, 0.8350,\n",
       "                      0.9262, 0.8492, 0.8358, 0.8138, 0.8248, 0.8277, 0.8309, 0.8307, 0.8135,\n",
       "                      0.8485, 0.8275, 0.8846, 0.8419, 0.8773, 0.8371, 0.8343, 0.8343, 0.8916,\n",
       "                      0.8657, 0.9193, 0.8458, 0.8167, 0.8074, 0.8813, 0.8678, 0.8475, 0.8247,\n",
       "                      0.8368, 0.8280, 0.8424, 0.8287, 0.8969, 0.8579, 0.8900, 0.7983, 0.8846,\n",
       "                      0.8581, 0.8513, 0.8113, 0.8235, 0.8233, 0.8585, 0.8539, 0.8050, 0.8450,\n",
       "                      0.9099, 0.8281, 0.9112, 0.8276, 0.8348, 0.8620, 0.8124, 0.8144, 0.8208,\n",
       "                      0.7987, 0.8138, 0.8473, 0.8462, 0.8218, 0.8397, 0.8686, 0.8122, 0.8335,\n",
       "                      0.8017, 0.8388, 0.8847, 0.8087, 0.8267, 0.8847, 0.8191, 0.7994, 0.8524,\n",
       "                      0.8513, 0.8620, 0.8923, 0.8756, 0.8274, 0.7817, 0.8511, 0.7980, 0.8535,\n",
       "                      0.8773, 0.8789, 0.8759, 0.8495, 0.8595, 0.8736, 0.8457, 0.8548, 0.8483,\n",
       "                      0.7938, 0.8434, 0.8457, 0.7992, 0.8234, 0.8502, 0.8545, 0.8371, 0.8218,\n",
       "                      0.8578, 0.8259, 0.8380, 0.8336, 0.8246, 0.8407, 0.8151, 0.7883, 0.8518,\n",
       "                      0.8294, 0.8988, 0.8082, 0.8039, 0.8314, 0.8292, 0.8495, 0.8924, 0.8696,\n",
       "                      0.8585, 0.8598, 0.8358, 0.8062, 0.8405, 0.8507, 0.8861, 0.9091, 0.8712,\n",
       "                      0.8276, 0.8790, 0.8537, 0.8751, 0.8932, 0.8518, 0.8673, 0.8759, 0.8640,\n",
       "                      0.8177, 0.7864, 0.8597, 0.8528, 0.8366, 0.8673, 0.8662, 0.8471, 0.7847,\n",
       "                      0.7732, 0.8336, 0.8880, 0.8713, 0.8595, 0.8517, 0.8539, 0.8512, 0.8815,\n",
       "                      0.8615, 0.8464, 0.8098, 0.7963, 0.8417, 0.8296, 0.8061, 0.8028, 0.7715,\n",
       "                      0.8761, 0.8676, 0.8703, 0.9011, 0.8259, 0.8128, 0.8848, 0.8283, 0.7885,\n",
       "                      0.8464, 0.8901, 0.8265, 0.7616, 0.8458, 0.8441, 0.8925, 0.8836, 0.8685,\n",
       "                      0.8265, 0.8329, 0.8307, 0.8685, 0.8653, 0.8122, 0.8344, 0.8436, 0.8462,\n",
       "                      0.8697, 0.8398, 0.9000], device='cuda:0')),\n",
       "             ('layers.6.mha.query.weight',\n",
       "              tensor([[-0.0345,  0.0464,  0.0270,  ...,  0.0206,  0.0004, -0.0020],\n",
       "                      [-0.0558, -0.0262,  0.0006,  ..., -0.0018,  0.0210, -0.0488],\n",
       "                      [-0.0576,  0.0130,  0.0107,  ...,  0.0199,  0.0222, -0.0080],\n",
       "                      ...,\n",
       "                      [ 0.0675, -0.0525, -0.1003,  ..., -0.0260, -0.0347, -0.0320],\n",
       "                      [-0.0789,  0.0086,  0.0228,  ...,  0.0497, -0.0742, -0.0408],\n",
       "                      [ 0.0226,  0.0010, -0.0983,  ...,  0.0389, -0.0212, -0.0163]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.mha.key.weight',\n",
       "              tensor([[-0.0766,  0.0416, -0.0017,  ..., -0.0399,  0.0263,  0.0106],\n",
       "                      [ 0.0218, -0.0559, -0.0020,  ..., -0.0501,  0.0244, -0.0106],\n",
       "                      [ 0.0035,  0.0553,  0.0214,  ...,  0.0413,  0.0113, -0.0088],\n",
       "                      ...,\n",
       "                      [-0.0100,  0.0098,  0.0333,  ..., -0.0115,  0.0196, -0.0052],\n",
       "                      [ 0.0076,  0.0359,  0.0315,  ...,  0.0004, -0.0233, -0.0241],\n",
       "                      [ 0.0441, -0.0444,  0.0227,  ...,  0.0405,  0.0033,  0.0181]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.mha.value.weight',\n",
       "              tensor([[ 0.0312, -0.0420, -0.0537,  ..., -0.0396,  0.0408,  0.0170],\n",
       "                      [ 0.0077,  0.0470, -0.0700,  ...,  0.1002, -0.0736, -0.0305],\n",
       "                      [ 0.0703, -0.0128, -0.0831,  ...,  0.0237, -0.0315, -0.0470],\n",
       "                      ...,\n",
       "                      [ 0.0084, -0.0166, -0.0170,  ...,  0.0823,  0.0170,  0.0484],\n",
       "                      [-0.0096, -0.0245, -0.0130,  ..., -0.0097,  0.0628, -0.0091],\n",
       "                      [ 0.0468, -0.0140,  0.0912,  ...,  0.0582, -0.1162,  0.0373]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.mha.proj.weight',\n",
       "              tensor([[-0.0067,  0.0814, -0.0446,  ..., -0.0417,  0.0835, -0.0680],\n",
       "                      [ 0.0069,  0.0166,  0.0084,  ..., -0.0189, -0.0348, -0.0449],\n",
       "                      [ 0.0216,  0.0421, -0.0122,  ...,  0.0171,  0.0366,  0.0491],\n",
       "                      ...,\n",
       "                      [-0.0080,  0.0054,  0.0679,  ...,  0.0185,  0.0786, -0.0777],\n",
       "                      [ 0.1156,  0.0791, -0.1102,  ..., -0.0468, -0.0067,  0.0467],\n",
       "                      [ 0.0657,  0.0086, -0.0511,  ...,  0.0331, -0.0696, -0.0253]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.w.weight',\n",
       "              tensor([[-0.0154,  0.0084, -0.0161,  ..., -0.0912, -0.0057,  0.0327],\n",
       "                      [ 0.0600, -0.0863,  0.0046,  ..., -0.0173,  0.0113,  0.0041],\n",
       "                      [ 0.0362, -0.0188,  0.0730,  ..., -0.0180, -0.0593,  0.0687],\n",
       "                      ...,\n",
       "                      [-0.0417,  0.0404, -0.0215,  ...,  0.0216,  0.0287,  0.0186],\n",
       "                      [ 0.0408, -0.0462,  0.0165,  ...,  0.0605,  0.0594, -0.0130],\n",
       "                      [-0.1271, -0.0247, -0.0330,  ...,  0.0116, -0.0447,  0.0201]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.w.bias',\n",
       "              tensor([ 0.0315, -0.0941,  0.0100,  ..., -0.0414, -0.1495, -0.0944],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.v.weight',\n",
       "              tensor([[-0.0508,  0.0951, -0.0294,  ..., -0.0771, -0.0151, -0.0453],\n",
       "                      [ 0.0580,  0.0335,  0.0234,  ...,  0.0031,  0.0143,  0.0338],\n",
       "                      [-0.0812,  0.0670, -0.0245,  ..., -0.0363, -0.0886, -0.0223],\n",
       "                      ...,\n",
       "                      [ 0.0193,  0.0488, -0.0532,  ..., -0.0961, -0.0231,  0.0103],\n",
       "                      [-0.0116, -0.0513, -0.0105,  ..., -0.0142,  0.0088, -0.0426],\n",
       "                      [-0.0533,  0.0363, -0.0071,  ...,  0.0273, -0.0348, -0.0433]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.v.bias',\n",
       "              tensor([ 0.0161, -0.0168, -0.0297,  ..., -0.0177, -0.0227, -0.1561],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.w2.weight',\n",
       "              tensor([[-0.0434, -0.0857,  0.1069,  ...,  0.1385, -0.0017,  0.0612],\n",
       "                      [-0.0515,  0.0148,  0.0554,  ..., -0.0193,  0.0140,  0.0201],\n",
       "                      [ 0.0398, -0.0598, -0.0511,  ..., -0.0270,  0.0380, -0.0085],\n",
       "                      ...,\n",
       "                      [-0.0113, -0.0459,  0.0358,  ...,  0.0399,  0.0252, -0.0053],\n",
       "                      [ 0.0279,  0.0226,  0.0013,  ...,  0.0083, -0.0348, -0.0124],\n",
       "                      [-0.0217,  0.0202,  0.0289,  ...,  0.0287,  0.1085,  0.0226]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.6.ffn.w2.bias',\n",
       "              tensor([ 7.4882e-02, -4.7564e-03,  3.5752e-02, -2.3404e-02,  9.4263e-02,\n",
       "                       6.9808e-02, -3.2098e-02, -4.2287e-02, -2.3878e-02, -5.2872e-02,\n",
       "                       4.7832e-02, -8.0810e-03, -3.7095e-02, -5.7302e-03, -1.9024e-02,\n",
       "                      -1.4640e-02,  5.6926e-02,  2.3293e-02, -7.2919e-02, -3.6129e-02,\n",
       "                       1.6769e-02, -1.1272e-01, -4.5486e-02, -3.7858e-02, -2.5877e-04,\n",
       "                      -6.0652e-02, -2.5225e-02, -1.1099e-01,  6.5092e-02, -4.3816e-02,\n",
       "                      -8.0538e-02, -8.1626e-02,  7.3569e-02, -3.1372e-02,  1.2243e-01,\n",
       "                       1.4470e-01,  9.2801e-02, -8.5779e-02,  1.2965e-01, -8.7074e-02,\n",
       "                      -4.7078e-02,  7.0576e-02,  5.2005e-02, -1.8636e-02, -9.7982e-02,\n",
       "                      -1.0544e-01,  1.7436e-02,  3.5294e-02,  4.8276e-02, -1.2171e-01,\n",
       "                      -9.6009e-02,  1.9307e-02, -4.0649e-02,  8.3634e-02,  1.2384e-02,\n",
       "                      -9.6198e-02,  8.9601e-02,  6.1875e-02,  1.3647e-01,  4.6055e-02,\n",
       "                       7.7567e-02,  4.6311e-02, -7.7409e-02, -2.9690e-02,  8.6563e-03,\n",
       "                       2.1101e-02,  2.9518e-02, -5.4472e-02, -7.4006e-02, -4.4347e-02,\n",
       "                      -6.5491e-02, -1.4994e-01, -7.9353e-02, -6.2584e-02,  7.5363e-02,\n",
       "                       1.8202e-02, -3.4378e-02, -1.3970e-01,  1.8998e-02, -1.3167e-01,\n",
       "                       8.5713e-02, -3.3908e-03, -9.3279e-02,  1.3982e-01,  6.8548e-02,\n",
       "                      -5.8971e-02,  2.0889e-02, -5.7681e-02, -4.2983e-02,  9.4958e-02,\n",
       "                      -1.1877e-01,  1.5272e-02,  1.1822e-01,  3.4331e-02,  3.1390e-02,\n",
       "                      -1.3078e-01, -7.8874e-02, -4.7448e-02, -4.8958e-02, -7.2885e-02,\n",
       "                      -7.5851e-02, -1.0073e-02, -6.7545e-02, -4.4387e-03,  1.4806e-02,\n",
       "                      -1.2970e-02, -1.8116e-02, -5.2325e-02,  4.3675e-02,  2.2061e-02,\n",
       "                      -1.1412e-01,  2.4668e-02,  8.2470e-02, -6.1758e-02, -2.9145e-02,\n",
       "                       8.6706e-02,  3.3674e-02, -8.4228e-03,  1.0858e-01, -1.4390e-01,\n",
       "                      -5.2052e-02, -2.9312e-02,  7.5845e-02, -1.3359e-01,  9.6479e-03,\n",
       "                      -1.3312e-01, -4.8286e-02, -1.3709e-01, -1.4889e-01,  9.4000e-03,\n",
       "                      -6.0177e-02, -1.6291e-02,  1.4428e-01,  5.3247e-02,  3.4385e-02,\n",
       "                      -1.0610e-01, -1.6473e-02,  4.3489e-02, -1.9457e-02, -1.3278e-01,\n",
       "                       1.5844e-01,  1.2224e-01, -6.9419e-02,  4.6117e-02,  7.1064e-02,\n",
       "                       5.6145e-02, -6.8311e-02, -1.4755e-02, -7.3054e-02, -1.2212e-01,\n",
       "                       5.9735e-02, -6.1834e-02,  5.0788e-02,  5.3470e-02, -1.2121e-01,\n",
       "                       2.5506e-02, -1.1810e-01,  7.4293e-02, -3.7163e-02, -2.3516e-03,\n",
       "                      -5.0994e-02,  1.8168e-02,  6.6322e-02,  7.1466e-02, -8.7447e-02,\n",
       "                       6.7244e-02, -6.3528e-02, -1.8775e-02,  1.2941e-01, -6.6456e-02,\n",
       "                      -7.7522e-02, -4.9286e-02, -4.8898e-02,  1.1401e-01,  6.2730e-02,\n",
       "                       7.0759e-02,  2.6085e-02,  4.3613e-02, -1.3664e-01,  8.5180e-02,\n",
       "                      -9.4300e-02, -4.6695e-02, -9.4891e-02, -7.1576e-02, -7.2472e-02,\n",
       "                      -7.8078e-02,  1.7057e-01,  4.3679e-02, -1.8456e-03,  5.1814e-02,\n",
       "                      -6.1951e-02, -6.4131e-02, -1.5354e-01,  7.5771e-04, -2.8041e-02,\n",
       "                       5.7478e-02, -5.2535e-02, -2.2495e-02,  2.2830e-02,  3.6056e-02,\n",
       "                       8.0885e-02, -3.4422e-02, -4.7427e-02,  7.3976e-02, -7.0639e-03,\n",
       "                      -4.2486e-02, -8.5718e-02,  9.4088e-03, -6.4287e-02, -6.0745e-02,\n",
       "                      -4.8532e-03, -5.1350e-02,  9.2830e-02, -5.8927e-02,  8.6964e-02,\n",
       "                       6.8174e-02,  6.4020e-02, -1.9791e-02,  1.7522e-01, -8.8467e-02,\n",
       "                      -1.2326e-01, -1.1538e-02, -1.7384e-02, -4.7802e-02, -1.1208e-02,\n",
       "                       5.2912e-03, -4.3816e-01,  2.5774e-02, -5.1219e-02, -7.8800e-02,\n",
       "                       2.4493e-02,  8.9850e-02, -2.4297e-02, -8.7052e-02,  5.1470e-02,\n",
       "                       1.9030e-02, -6.8667e-02, -7.8975e-02, -1.2939e-01,  9.1458e-02,\n",
       "                       5.7071e-02, -1.1686e-01, -4.1996e-02, -7.7028e-02,  5.0762e-02,\n",
       "                       8.1218e-02,  6.8642e-02,  3.7634e-02, -1.0500e-01,  5.2936e-03,\n",
       "                       2.4051e-02,  5.6702e-02,  7.9614e-02,  3.0225e-02, -1.3219e-01,\n",
       "                       2.0358e-01, -3.0243e-02,  1.1706e-01, -1.2046e-01, -4.5366e-02,\n",
       "                      -3.7738e-02,  4.7672e-02,  1.5802e-01, -4.6018e-03, -5.7282e-02,\n",
       "                       1.0627e-01, -3.2291e-02,  2.5351e-02, -5.9950e-02,  1.0731e-01,\n",
       "                       4.1465e-02, -7.6619e-02,  9.1382e-02,  6.3358e-02,  1.0440e-01,\n",
       "                       1.7617e-02,  1.1888e-03, -4.2150e-02,  8.2629e-02, -5.0983e-02,\n",
       "                       8.4851e-02, -8.9233e-02,  2.4808e-02, -1.1202e-01, -9.0500e-02,\n",
       "                      -2.0177e-02, -1.7897e-02,  5.0540e-03,  5.2919e-02, -2.8813e-02,\n",
       "                       3.8428e-02, -1.3182e-01, -1.0415e-01,  7.4037e-02,  5.1862e-02,\n",
       "                      -1.8372e-02,  1.1687e-01,  1.0086e-01, -1.3799e-01, -8.6081e-02,\n",
       "                       2.5272e-02,  1.8994e-01, -5.2509e-03,  1.8355e-03, -7.0073e-02,\n",
       "                      -5.7741e-02, -5.9866e-02,  1.4895e-04, -2.8485e-02, -1.2194e-02,\n",
       "                       1.4480e-01,  4.4789e-02,  4.4701e-02, -8.1575e-02, -8.3132e-02,\n",
       "                       3.1573e-02,  8.6083e-02, -1.8828e-01,  2.9637e-02,  3.7049e-02,\n",
       "                       1.1164e-01,  8.1992e-02, -3.9357e-02,  4.5425e-02,  2.5482e-02,\n",
       "                      -5.4954e-02, -9.7378e-02, -1.0157e-02,  8.2967e-03,  8.5885e-02,\n",
       "                      -9.9719e-02,  1.4671e-01, -1.3759e-02, -3.0311e-02, -4.7686e-02,\n",
       "                       5.1752e-02, -1.5492e-01, -4.7209e-02, -4.3430e-02,  8.2953e-02,\n",
       "                       4.0327e-02, -5.8788e-03, -2.9175e-03,  4.6339e-02, -6.2867e-02,\n",
       "                      -7.7368e-03, -3.2344e-02, -5.7670e-02, -8.4551e-02, -5.6251e-02,\n",
       "                      -8.2600e-02,  7.1406e-02,  7.0462e-02, -2.5502e-02,  1.3514e-01,\n",
       "                      -1.7843e-02, -1.1971e-01,  9.6402e-02, -9.9931e-02,  3.7432e-03,\n",
       "                       1.1672e-02,  1.0035e-01,  7.5693e-02, -1.2216e-02, -2.4481e-02,\n",
       "                       5.4141e-02,  4.6643e-02, -6.4867e-02,  1.0280e-01, -2.6631e-02,\n",
       "                      -9.9375e-02,  1.8583e-02,  8.5262e-02,  6.0733e-02, -4.6613e-02,\n",
       "                       4.1731e-02,  6.5244e-03,  7.8801e-02, -7.7441e-02, -6.9450e-02,\n",
       "                      -9.3204e-02,  1.1097e-01,  5.6949e-02,  5.6530e-02,  8.1720e-02,\n",
       "                       6.2649e-02, -1.5799e-01, -3.2547e-03, -3.4436e-02,  8.5912e-02,\n",
       "                      -2.7221e-02,  1.1289e-01, -8.1404e-02,  5.8747e-03,  4.3527e-02,\n",
       "                       3.1561e-02,  1.7307e-02,  7.3097e-02, -2.4854e-03,  6.0823e-02,\n",
       "                      -1.1506e-01, -4.1665e-04, -7.4263e-02,  8.9114e-02, -2.9519e-02,\n",
       "                       2.5637e-02, -2.7269e-02,  1.5804e-01, -8.7728e-01, -4.2163e-02,\n",
       "                       1.5680e-03,  7.7936e-02, -9.1020e-02,  1.3168e-01,  3.2408e-02,\n",
       "                      -5.6035e-02, -3.5524e-02, -1.1060e-01, -5.8187e-02, -4.1619e-02,\n",
       "                       1.2231e-01, -1.0527e-02, -2.2250e-03, -3.1502e-02, -1.6361e-03,\n",
       "                       4.3249e-02, -8.5335e-02, -1.0385e-01, -2.6561e-02, -5.4708e-02,\n",
       "                      -2.0915e-03,  3.5872e-03,  1.0827e-01,  3.8724e-02,  1.4900e-01,\n",
       "                       1.0457e-02, -3.8462e-02, -2.5979e-04,  6.9426e-02,  9.0220e-03,\n",
       "                       5.4128e-02, -6.0507e-02, -4.8245e-02, -6.7305e-02, -4.2836e-02,\n",
       "                      -1.2281e-01,  8.9333e-02, -2.3285e-03,  3.0887e-02,  1.2306e-02,\n",
       "                      -4.0848e-02, -4.6181e-02,  4.1369e-03,  1.4873e-01, -1.0810e-01,\n",
       "                      -5.1641e-03,  6.0800e-02, -7.9344e-02, -1.5650e-01, -7.6771e-02,\n",
       "                       9.4690e-02, -5.3378e-02, -3.3272e-02,  4.4692e-03,  4.6900e-02,\n",
       "                       9.7719e-03,  9.8061e-02,  4.6375e-02,  7.0464e-02, -3.3411e-02,\n",
       "                       6.2938e-02,  4.5978e-02,  5.0639e-02,  5.1665e-02, -9.3539e-02,\n",
       "                      -3.3447e-02,  7.5852e-03,  3.3008e-02,  6.5455e-02, -1.0525e-01,\n",
       "                       4.9514e-02,  1.1532e-01,  1.1550e-02, -8.0631e-03,  1.5488e-02,\n",
       "                       1.0685e-01, -1.7510e-01, -1.1391e-02,  6.3643e-02, -3.6804e-03,\n",
       "                       1.6622e-02,  7.7773e-02,  7.7032e-02,  5.2743e-02,  3.6319e-02,\n",
       "                      -1.0154e-01,  2.7019e-02, -1.0563e-01,  2.5392e-01, -1.6252e-01,\n",
       "                       9.5876e-02, -4.2407e-02, -6.0532e-02,  1.1263e-01, -1.1756e-01,\n",
       "                       3.0745e-02, -1.0804e-01, -1.1010e-01,  1.0489e-01, -4.1297e-02,\n",
       "                      -1.8104e-02,  5.1143e-02,  8.5653e-02, -7.1956e-02, -2.5654e-02,\n",
       "                      -3.8852e-02,  7.8508e-02,  3.2001e-02, -4.5223e-02,  4.7863e-02,\n",
       "                      -3.3913e-02, -1.4443e-02,  4.8253e-02,  1.2087e-01, -6.4270e-02,\n",
       "                      -7.1754e-03, -5.8838e-02, -7.3652e-02, -4.0128e-02,  5.5292e-02,\n",
       "                      -8.4025e-02, -5.2970e-02, -3.6535e-02, -1.7959e-02,  9.2928e-02,\n",
       "                       8.1625e-02,  8.0312e-03, -4.5135e-02,  1.5287e-02,  3.1032e-02,\n",
       "                      -1.5572e-01, -1.1569e-01, -2.6853e-02,  1.8313e-02, -4.2110e-02,\n",
       "                      -6.5123e-02,  7.4984e-02, -7.5947e-02,  9.8827e-02, -1.1301e-01,\n",
       "                       1.3382e-01,  3.9100e-02,  4.4558e-02, -8.3868e-02, -7.3587e-02,\n",
       "                      -2.1014e-02,  4.9985e-03,  1.3221e-01, -2.1691e-02, -4.8815e-03,\n",
       "                      -5.9179e-02,  5.9480e-02, -1.1377e-02,  7.9319e-02,  3.6612e-03,\n",
       "                       1.0398e-02, -2.3515e-02, -7.6091e-02,  3.6687e-02, -1.4815e-02,\n",
       "                       1.6374e-01, -4.1615e-02,  8.9598e-02,  1.0224e-01, -6.8406e-02,\n",
       "                      -8.2888e-04,  2.3130e-03, -2.4209e-02, -6.9353e-02, -7.0841e-02,\n",
       "                      -2.3497e-02, -7.2921e-02,  9.7287e-02, -1.0647e-01,  3.6191e-02,\n",
       "                      -1.1469e-01,  5.9798e-02,  5.4042e-02,  1.0788e-01, -7.1568e-02,\n",
       "                      -4.5906e-02, -3.1724e-02, -8.2938e-02,  1.9593e-02,  3.7112e-02,\n",
       "                      -3.9392e-02,  5.2237e-03, -7.8036e-02,  4.2021e-02, -4.0525e-02,\n",
       "                       1.9882e-02, -9.4716e-02, -4.4150e-02,  3.0549e-02, -5.7910e-02,\n",
       "                       4.9377e-02,  2.6927e-02,  7.8273e-02, -2.1258e-02, -4.0731e-02,\n",
       "                       1.2570e-02, -3.9366e-02, -3.9924e-02,  3.9020e-02, -7.1772e-02,\n",
       "                      -9.6398e-02, -2.3013e-02, -4.2449e-02,  1.2876e-01, -7.6487e-02,\n",
       "                      -1.0633e-01,  1.2100e-01,  1.9908e-02,  7.9903e-02,  1.3190e-01,\n",
       "                       3.0120e-02,  1.2627e-02, -7.7271e-02,  3.1774e-02,  4.3318e-02,\n",
       "                       2.0037e-02,  5.8174e-02, -5.7479e-02,  4.8121e-02, -2.3306e-02,\n",
       "                       1.0226e-01, -1.1908e-02, -3.6801e-02, -7.2993e-02, -9.3539e-02,\n",
       "                       9.2184e-02, -9.5848e-02,  9.6545e-02, -1.0294e-01,  9.9218e-02,\n",
       "                       1.3795e-01, -4.1032e-02,  1.7149e-02,  2.1400e-02,  6.1348e-02,\n",
       "                       1.0633e-01,  1.1304e-01, -9.1579e-02, -2.3387e-02, -8.7767e-03,\n",
       "                       1.6055e-02, -2.1430e-02,  2.5397e-02, -6.0825e-02,  2.3065e-02,\n",
       "                      -1.8273e-02, -1.2828e-01, -1.1580e-01,  5.0142e-02,  6.2883e-03,\n",
       "                       1.2896e-02,  1.1513e-01, -3.1145e-03,  1.3830e-01,  1.5177e-01,\n",
       "                       2.1226e-02,  6.0790e-02, -4.5974e-02, -1.1526e-01, -5.4883e-02,\n",
       "                       6.4487e-02, -1.3707e-01, -3.5920e-02, -5.6914e-02,  1.9478e-02,\n",
       "                      -1.0276e-01,  2.5334e-02, -8.9548e-02,  6.9878e-02, -1.5301e-01,\n",
       "                      -4.1776e-02, -6.8182e-02, -1.5688e-01,  6.0006e-02, -4.6940e-02,\n",
       "                      -1.1789e-01,  8.4395e-02, -2.7166e-02,  1.0983e-01,  5.9599e-02,\n",
       "                      -1.7186e-01, -2.0578e-02,  8.4171e-04,  8.8324e-02, -2.8552e-02,\n",
       "                      -2.5289e-02, -8.3636e-02,  1.8224e-02,  9.0970e-02,  4.9809e-02,\n",
       "                       4.7836e-02,  5.6146e-02, -9.4009e-02,  3.2468e-02,  3.9311e-02,\n",
       "                      -5.8372e-02, -7.1802e-03,  1.0840e-01,  1.0329e-01,  7.3118e-02,\n",
       "                       6.7714e-02,  9.1318e-02,  3.7212e-02,  4.2110e-03,  6.4421e-02,\n",
       "                       2.5509e-02,  3.1013e-03, -2.3791e-02,  9.5238e-02, -7.9208e-02,\n",
       "                       2.1037e-02, -4.6200e-02, -1.0112e-01,  7.1246e-02, -5.1918e-02,\n",
       "                      -1.3785e-01,  4.0789e-02, -8.7537e-02, -9.7823e-02,  6.5310e-02,\n",
       "                       4.9116e-02,  9.2397e-02,  1.4864e-01,  5.1477e-02, -1.1550e-01,\n",
       "                      -3.8155e-02, -8.9575e-02,  1.1687e-02,  7.9295e-02, -4.1453e-02,\n",
       "                       7.0197e-02,  4.6869e-02,  9.3091e-02,  8.0765e-02, -2.5372e-02,\n",
       "                      -7.9545e-02, -6.1562e-03, -2.8136e-02,  2.9046e-02, -2.4861e-02,\n",
       "                      -3.7054e-02,  8.0360e-02, -1.3558e-02, -5.6904e-02,  3.3836e-03,\n",
       "                       7.0046e-02,  2.5533e-02, -5.9044e-02, -1.0419e-01, -7.2464e-02,\n",
       "                       5.4184e-02,  6.9011e-02,  3.1674e-02], device='cuda:0')),\n",
       "             ('layers.6.anorm.weight',\n",
       "              tensor([0.9603, 0.9464, 1.0262, 1.0469, 0.9924, 1.0065, 0.9508, 1.0106, 0.9854,\n",
       "                      0.9741, 0.9576, 1.0728, 0.9817, 0.9974, 0.9574, 0.9456, 0.9586, 1.0037,\n",
       "                      1.0212, 0.9535, 1.0260, 0.9916, 0.9224, 1.0059, 1.0278, 1.0706, 0.9735,\n",
       "                      0.9878, 1.0093, 0.9615, 1.0103, 1.0876, 0.9522, 0.9721, 0.9362, 1.0434,\n",
       "                      1.0439, 0.8968, 0.9292, 0.9913, 0.9655, 0.9567, 1.0128, 1.0026, 1.0124,\n",
       "                      1.0082, 1.0318, 1.0126, 1.0250, 0.9646, 1.0600, 1.0628, 1.0822, 1.0201,\n",
       "                      0.9935, 1.0153, 1.0336, 1.0339, 0.8781, 1.0139, 0.9723, 0.9902, 0.9382,\n",
       "                      1.0109, 0.9918, 0.9851, 0.9575, 1.0592, 0.9875, 0.9620, 0.9966, 0.9266,\n",
       "                      1.0635, 1.0198, 0.9331, 0.9773, 0.9820, 1.0406, 1.0070, 1.0424, 1.0072,\n",
       "                      0.9587, 0.9647, 1.0126, 0.9951, 0.9991, 0.9810, 1.0041, 0.9764, 1.0442,\n",
       "                      0.9504, 0.9883, 1.0198, 1.0214, 1.0121, 1.0405, 0.9614, 1.0400, 1.0425,\n",
       "                      1.0351, 0.8814, 1.0151, 1.0112, 0.9400, 1.0571, 1.0201, 1.0866, 0.9786,\n",
       "                      1.0345, 1.0414, 0.9726, 0.9907, 0.9837, 0.9438, 0.9377, 0.9492, 0.9995,\n",
       "                      0.9978, 0.9974, 0.9904, 1.0362, 1.0457, 0.9646, 0.9876, 0.9852, 0.9374,\n",
       "                      1.0034, 0.9951, 0.9080, 1.0174, 0.9991, 1.0001, 0.9162, 0.9955, 1.0178,\n",
       "                      0.8739, 0.9911, 1.0264, 0.9819, 1.0664, 1.0327, 1.0005, 1.0276, 1.0140,\n",
       "                      1.0203, 1.0092, 1.0162, 0.9990, 1.0495, 0.9692, 0.9486, 1.0523, 1.0111,\n",
       "                      1.0261, 0.8881, 1.0515, 0.9985, 0.9910, 0.9505, 0.9378, 1.0379, 0.9685,\n",
       "                      1.0052, 1.0528, 0.9766, 1.0205, 0.9905, 0.9955, 0.9858, 1.0136, 0.9931,\n",
       "                      0.9664, 0.9435, 0.9055, 0.9339, 0.9892, 1.0496, 1.0006, 1.0169, 0.9896,\n",
       "                      1.0114, 0.9982, 0.9702, 1.0040, 0.9662, 0.9944, 1.0429, 1.0549, 1.0497,\n",
       "                      1.0817, 0.8826, 1.0029, 1.0504, 0.9724, 1.0075, 1.0008, 0.9405, 1.0533,\n",
       "                      0.9354, 1.0175, 0.9175, 1.0160, 0.9831, 0.9066, 1.0132, 0.9800, 1.0274,\n",
       "                      0.9981, 1.0405, 0.9583, 1.0155, 0.9208, 1.0205, 0.9461, 0.9409, 1.0283,\n",
       "                      0.9817, 1.0473, 0.9937, 1.0601, 0.9076, 1.0523, 1.0216, 0.9900, 0.9543,\n",
       "                      0.9315, 1.2087, 0.9473, 0.9888, 0.9410, 0.9814, 0.9787, 1.0614, 1.0362,\n",
       "                      0.9747, 1.0796, 0.9848, 0.9023, 1.0514, 0.9506, 1.0230, 0.9642, 0.9589,\n",
       "                      1.0048, 1.0375, 1.0182, 1.0211, 1.0218, 1.0531, 0.9864, 0.9555, 0.9782,\n",
       "                      0.9333, 1.1138, 0.9817, 1.0193, 1.0281, 0.9559, 1.0445, 0.9919, 1.0801,\n",
       "                      0.9662, 0.9678, 1.0590, 1.0050, 1.0094, 1.0199, 0.9730, 0.9809, 1.0433,\n",
       "                      1.0390, 0.9319, 1.0285, 0.9788, 0.9322, 0.9951, 1.0111, 0.9697, 0.9108,\n",
       "                      0.9978, 1.0058, 1.0243, 1.0726, 1.0127, 0.9900, 1.0121, 0.9634, 0.9600,\n",
       "                      0.9933, 1.0327, 1.0309, 1.0231, 0.9961, 0.9551, 1.0043, 0.9714, 0.9365,\n",
       "                      1.0330, 1.0283, 1.0108, 1.0541, 0.9836, 0.9968, 1.0440, 0.9979, 1.0863,\n",
       "                      0.9469, 1.0231, 0.9251, 1.0474, 0.9268, 0.9467, 1.0259, 0.9478, 0.9554,\n",
       "                      1.0277, 0.9331, 1.0743, 1.0222, 0.9964, 0.9798, 0.9936, 1.0287, 0.9665,\n",
       "                      0.9798, 1.0173, 0.9941, 1.0532, 1.0446, 1.0220, 0.9277, 1.0315, 1.0627,\n",
       "                      0.9873, 0.9830, 0.9659, 1.0415, 1.0176, 0.9840, 1.0279, 0.9171, 1.0634,\n",
       "                      0.9945, 1.0054, 0.9473, 1.0012, 1.0300, 1.0362, 0.9502, 1.0235, 1.0764,\n",
       "                      1.0279, 0.9633, 1.0102, 1.0644, 1.0190, 0.9939, 1.0173, 0.9801, 1.0141,\n",
       "                      1.0127, 0.9792, 0.9854, 0.9344, 1.0537, 1.0101, 1.0406, 0.9741, 0.9913,\n",
       "                      0.8971, 1.0386, 1.0152, 1.0084, 1.0102, 1.0201, 1.0459, 1.0462, 0.9689,\n",
       "                      1.0297, 0.9412, 1.0687, 0.9766, 0.9905, 0.9512, 0.9133, 0.9362, 1.0213,\n",
       "                      1.0183, 0.8828, 0.8705, 0.9721, 1.0224, 1.0464, 1.0238, 0.9739, 1.0087,\n",
       "                      1.0070, 1.0242, 0.9716, 1.0027, 1.0004, 0.9677, 0.9775, 1.0054, 0.9604,\n",
       "                      0.9707, 1.0086, 0.9838, 0.6818, 0.9601, 1.0120, 0.8795, 1.0135, 0.8943,\n",
       "                      0.9784, 0.9426, 1.0829, 0.9467, 0.9468, 0.9387, 0.9609, 0.9730, 1.0601,\n",
       "                      0.9140, 1.0437, 1.0661, 0.9729, 0.9303, 1.0705, 1.0171, 0.9990, 1.0312,\n",
       "                      1.0549, 0.9211, 0.9536, 0.9886, 0.9966, 0.9732, 0.9990, 1.0124, 0.9957,\n",
       "                      1.0366, 1.0235, 0.9547, 0.9811, 1.0114, 1.0626, 1.0612, 0.9875, 0.9808,\n",
       "                      0.8911, 0.9596, 0.8954, 1.0341, 1.0114, 0.9909, 0.9035, 0.9274, 1.0632,\n",
       "                      1.0352, 0.9017, 1.0001, 0.9823, 0.9713, 1.0564, 1.0192, 1.0196, 1.0404,\n",
       "                      0.9866, 1.0358, 1.0093, 0.9208, 1.0165, 0.9574, 0.9873, 0.9463, 1.0508,\n",
       "                      1.0105, 1.0048, 0.9574, 1.0465, 0.9327, 1.0570, 0.9834, 1.0435, 0.9224,\n",
       "                      1.0147, 1.0386, 1.0074, 0.9749, 0.9716, 0.9263, 1.0075, 1.0392, 1.0945,\n",
       "                      0.9870, 0.9945, 1.0306, 0.7649, 1.0199, 0.9747, 1.0036, 0.9415, 0.9898,\n",
       "                      1.0420, 0.9849, 1.0304, 0.9345, 0.9557, 0.9104, 0.9524, 0.9372, 1.0266,\n",
       "                      0.9828, 1.0096, 0.9978, 1.0289, 1.0203, 0.9730, 0.9196, 0.9361, 0.9965,\n",
       "                      0.9667, 1.0233, 1.0005, 1.0250, 0.9630, 1.0412, 0.9663, 1.0154, 1.0627,\n",
       "                      0.9396, 0.9848, 1.0079, 1.0013, 1.0346, 1.0288, 1.0391, 0.9588, 0.9408,\n",
       "                      1.0112, 0.9353, 0.9498, 1.0396, 1.0224, 0.9037, 1.0110, 0.9175, 1.0413,\n",
       "                      1.0442, 0.9450, 1.0232, 0.9568, 0.9397, 1.0739, 0.9495, 0.9440, 0.9103,\n",
       "                      1.0154, 1.0475, 1.0021, 1.0442, 0.9782, 0.9625, 1.0268, 0.9982, 1.0298,\n",
       "                      0.9651, 1.0219, 1.0302, 1.1364, 1.0022, 0.9466, 0.9961, 0.9977, 1.0178,\n",
       "                      1.0391, 0.9733, 1.0458, 0.9484, 0.9756, 1.0090, 1.0135, 0.9773, 1.0232,\n",
       "                      1.0103, 0.9807, 1.0634, 0.9582, 0.9986, 1.0105, 0.9990, 1.0032, 1.0013,\n",
       "                      1.0348, 1.0645, 1.0254, 0.9378, 0.9258, 0.9972, 0.9850, 0.9778, 1.0095,\n",
       "                      0.9404, 1.0417, 0.9946, 1.0209, 0.9890, 0.9554, 1.0282, 0.9363, 1.0247,\n",
       "                      1.0044, 0.9953, 0.9494, 0.9939, 1.0107, 0.9788, 0.9295, 1.0265, 1.0335,\n",
       "                      1.0584, 1.0188, 0.9678, 1.0297, 1.0118, 1.0211, 0.9013, 0.9762, 1.0629,\n",
       "                      0.9637, 1.0641, 0.9684, 0.9572, 1.0126, 0.9752, 1.0339, 0.9208, 0.9940,\n",
       "                      0.9725, 0.9663, 1.0637, 1.0117, 0.9924, 1.0045, 1.0424, 1.0051, 0.9462,\n",
       "                      0.9604, 0.9851, 1.0351, 1.0279, 0.9123, 1.0091, 0.9752, 0.9753, 1.0070,\n",
       "                      1.0240, 1.0921, 1.0263, 0.9604, 1.0315, 1.0437, 0.9954, 1.0183, 0.9925,\n",
       "                      0.9120, 0.9927, 0.9898, 0.9456, 1.0107, 1.0001, 0.9960, 0.9866, 0.9681,\n",
       "                      0.9994, 0.9617, 1.0399, 1.0143, 0.9584, 1.0216, 1.0202, 1.0074, 0.9804,\n",
       "                      0.9293, 1.0140, 0.9409, 0.9219, 1.0293, 0.9719, 0.9431, 1.0649, 1.0020,\n",
       "                      0.9313, 1.0336, 1.0289, 0.9609, 1.0256, 0.9788, 1.0605, 1.0478, 1.0216,\n",
       "                      1.0189, 1.0440, 1.0001, 1.0279, 1.0208, 0.9765, 0.9922, 1.0438, 1.0238,\n",
       "                      0.9632, 0.9480, 0.9968, 1.0233, 0.9669, 0.9999, 1.1145, 1.0222, 0.9678,\n",
       "                      0.9332, 0.9990, 0.9797, 1.0667, 0.9712, 1.0351, 0.9621, 0.9523, 1.0421,\n",
       "                      1.0007, 1.0373, 0.9873, 0.9392, 0.9536, 1.0099, 0.9542, 0.9977, 1.0579,\n",
       "                      0.9635, 1.0718, 0.9806, 1.0433, 1.0625, 0.9571, 1.0326, 0.9822, 0.9892,\n",
       "                      0.9868, 1.0348, 1.0118, 0.8669, 1.0383, 1.0443, 1.0191, 0.9743, 1.0323,\n",
       "                      0.9095, 0.9824, 0.9825, 1.0264, 1.0392, 0.9682, 0.9781, 1.0540, 0.9473,\n",
       "                      1.0091, 1.0679, 1.0308], device='cuda:0')),\n",
       "             ('layers.6.fnorm.weight',\n",
       "              tensor([0.9270, 0.9414, 0.9378, 0.9439, 1.0023, 0.9377, 0.9468, 0.9444, 0.9297,\n",
       "                      0.9460, 0.9450, 0.9691, 0.9475, 0.9510, 0.9658, 0.9098, 0.9140, 0.9390,\n",
       "                      0.9579, 0.9457, 0.9660, 0.9380, 0.9088, 1.0258, 0.9849, 0.9523, 0.9388,\n",
       "                      0.9066, 0.9697, 0.9362, 0.9463, 1.0015, 0.9396, 0.9710, 0.9414, 0.9944,\n",
       "                      0.9481, 0.8795, 0.8955, 0.8978, 0.9454, 0.9264, 0.9915, 0.9468, 0.9797,\n",
       "                      0.9734, 0.9397, 0.9722, 0.9344, 0.9875, 0.9848, 0.9345, 0.9563, 0.9292,\n",
       "                      0.9718, 0.9549, 1.0044, 0.9627, 0.8387, 0.9833, 0.9311, 0.9558, 0.9245,\n",
       "                      0.9861, 0.9489, 0.9912, 0.9809, 0.9297, 0.9442, 0.9486, 0.9515, 0.9986,\n",
       "                      0.9564, 0.9783, 0.8868, 0.9738, 0.9294, 0.9297, 0.9344, 0.9361, 0.9843,\n",
       "                      0.8987, 0.8398, 0.9890, 0.9107, 0.9523, 0.9295, 0.9292, 0.9451, 0.9869,\n",
       "                      0.9214, 0.9677, 0.9350, 0.9505, 0.9776, 0.9489, 0.9039, 0.9649, 0.9251,\n",
       "                      1.0057, 0.9447, 1.0088, 0.9883, 0.9352, 0.9652, 0.9617, 0.9742, 0.9380,\n",
       "                      0.9971, 0.9320, 0.9669, 0.9198, 0.9674, 0.9559, 0.9704, 0.8976, 0.9354,\n",
       "                      0.9705, 0.9730, 0.9472, 0.9724, 0.9665, 0.8888, 0.9551, 0.9425, 0.9159,\n",
       "                      0.9781, 0.9467, 0.8860, 0.9968, 0.9642, 0.9469, 0.8714, 0.9586, 0.9241,\n",
       "                      0.9255, 0.9548, 0.9546, 0.9617, 0.9688, 0.9519, 0.9664, 0.9643, 0.9293,\n",
       "                      0.9410, 0.9596, 1.0059, 0.9933, 0.9268, 0.9369, 0.8900, 0.9552, 0.9799,\n",
       "                      0.9797, 0.8390, 0.9678, 0.9360, 0.9610, 0.9546, 0.9983, 0.9269, 0.9494,\n",
       "                      0.9798, 0.9646, 0.9238, 0.9996, 0.9474, 0.9871, 0.9782, 0.9606, 0.9477,\n",
       "                      0.9034, 0.9530, 0.9410, 0.9285, 0.8846, 0.9470, 0.9304, 0.9293, 0.9422,\n",
       "                      0.9580, 0.9461, 0.9639, 0.9865, 1.0004, 0.9844, 0.9529, 0.9293, 0.9495,\n",
       "                      0.9504, 0.9957, 0.8983, 0.9715, 0.9245, 0.9398, 0.9830, 0.9731, 0.9426,\n",
       "                      0.8939, 0.9771, 0.8723, 0.9685, 0.9724, 0.9315, 0.9522, 0.9488, 1.0134,\n",
       "                      1.0000, 0.9070, 0.9811, 1.0021, 0.9175, 0.9763, 0.9760, 0.9452, 0.9609,\n",
       "                      0.9340, 0.9700, 0.9817, 0.9511, 0.9407, 0.9521, 0.9071, 0.9063, 0.9661,\n",
       "                      0.9616, 0.2949, 0.9507, 0.9769, 0.9771, 0.9417, 0.9255, 0.9308, 0.9323,\n",
       "                      0.9722, 0.9621, 0.9332, 0.9595, 0.9450, 0.9663, 0.8930, 0.9211, 0.9728,\n",
       "                      0.9689, 0.9929, 0.9429, 0.9594, 0.9688, 0.9168, 0.9755, 0.9430, 0.9623,\n",
       "                      0.9820, 0.9950, 0.9062, 0.9460, 0.9685, 0.9191, 0.8962, 0.9453, 0.9536,\n",
       "                      0.9481, 0.8519, 0.9425, 0.9493, 0.9505, 0.9010, 0.9446, 0.9290, 0.9738,\n",
       "                      0.9730, 0.9234, 0.8578, 0.9516, 0.9668, 0.9896, 0.9625, 0.9928, 0.8902,\n",
       "                      0.9415, 0.9665, 0.9555, 0.9428, 0.9379, 0.9493, 0.9690, 0.9659, 0.9440,\n",
       "                      0.9518, 0.9524, 0.9718, 0.9485, 0.9619, 0.9643, 0.9534, 0.9542, 0.9252,\n",
       "                      0.9925, 0.9482, 0.9751, 0.9772, 0.9465, 0.9489, 0.9746, 0.9202, 0.9393,\n",
       "                      0.9119, 0.8870, 0.9048, 0.9833, 0.8528, 0.9368, 0.9887, 0.9628, 0.9393,\n",
       "                      0.9561, 0.9654, 0.9944, 0.9996, 0.9912, 0.9301, 0.9551, 0.9380, 0.9473,\n",
       "                      0.9286, 0.9831, 0.9507, 0.9162, 0.9891, 0.9123, 0.9512, 0.9071, 0.8995,\n",
       "                      0.9147, 1.0138, 0.9619, 0.9594, 0.9773, 0.9322, 0.9631, 0.9482, 0.9503,\n",
       "                      0.9411, 0.9051, 0.9648, 0.9289, 1.0202, 0.9892, 0.9751, 0.9551, 0.9675,\n",
       "                      0.9364, 0.9752, 0.9368, 0.9300, 0.9059, 0.9413, 0.9629, 0.9083, 0.9624,\n",
       "                      0.9621, 0.9172, 0.9714, 0.9610, 0.9446, 0.9350, 0.9480, 0.9208, 0.8662,\n",
       "                      0.9594, 0.9330, 0.9434, 0.9584, 0.9959, 0.9789, 0.9491, 0.9407, 0.9598,\n",
       "                      0.9404, 0.9128, 0.8986, 1.0018, 0.9516, 0.9693, 0.9525, 0.9319, 0.9650,\n",
       "                      0.9400, 0.9240, 0.9210, 0.9512, 0.9764, 0.9574, 0.9460, 0.9201, 1.0073,\n",
       "                      0.9606, 0.9868, 0.9774, 0.9787, 0.9360, 0.9527, 0.9461, 0.9539, 0.9443,\n",
       "                      0.9400, 0.9773, 0.9489, 0.1642, 0.9247, 0.9536, 0.9055, 0.9101, 0.8550,\n",
       "                      0.9290, 0.9215, 0.9662, 0.9091, 0.9477, 0.9542, 0.9679, 0.9172, 0.9732,\n",
       "                      0.9857, 0.9063, 0.9485, 0.9363, 0.9141, 0.9439, 0.9648, 0.9547, 0.9342,\n",
       "                      0.8992, 0.9179, 0.9406, 0.9598, 0.9422, 0.9596, 0.9069, 0.9519, 0.9647,\n",
       "                      0.9785, 0.9518, 0.9455, 0.9056, 0.9967, 0.9429, 0.9988, 0.9970, 0.9090,\n",
       "                      0.9176, 0.9005, 0.9034, 0.9205, 0.9950, 0.9082, 0.9709, 0.9297, 0.9826,\n",
       "                      0.9461, 0.9181, 0.9908, 0.9551, 0.9003, 0.9550, 0.9838, 0.9276, 0.9480,\n",
       "                      0.9215, 0.9049, 0.9359, 0.9668, 0.9310, 0.9773, 0.9315, 0.9747, 0.9822,\n",
       "                      0.9387, 0.9921, 1.0014, 0.9880, 0.9066, 0.9231, 0.9737, 0.9758, 0.9094,\n",
       "                      0.9779, 0.9362, 0.8976, 0.9427, 0.9427, 0.9372, 0.9188, 0.9678, 1.0005,\n",
       "                      0.9696, 0.9109, 0.9813, 0.5491, 0.9647, 0.9349, 0.9601, 0.9453, 0.9404,\n",
       "                      0.9655, 0.9697, 0.8862, 0.9097, 0.9335, 0.9317, 0.9191, 0.9677, 0.9328,\n",
       "                      0.8863, 0.9598, 0.9451, 0.9977, 0.9213, 0.9728, 0.9804, 0.9419, 0.9347,\n",
       "                      1.0020, 0.9065, 0.9588, 0.9613, 0.9720, 0.9100, 0.9217, 0.9398, 1.0227,\n",
       "                      0.9125, 0.9607, 1.0113, 1.0041, 0.9309, 0.9447, 0.9330, 0.9113, 0.9451,\n",
       "                      0.9605, 0.9403, 0.9698, 0.9526, 0.9662, 0.9485, 0.9441, 0.9182, 0.9401,\n",
       "                      0.9477, 0.8565, 0.9480, 0.9292, 0.9533, 0.9405, 0.9856, 0.9200, 0.9231,\n",
       "                      0.9347, 0.9546, 0.9948, 0.9886, 0.9571, 0.9442, 0.9383, 0.9520, 0.9446,\n",
       "                      0.9525, 1.0024, 0.9579, 0.9766, 0.9041, 0.9485, 0.9378, 0.9630, 0.9272,\n",
       "                      0.9668, 0.9273, 0.9412, 0.9319, 0.9404, 0.9486, 0.9669, 0.9411, 0.9360,\n",
       "                      0.9661, 0.9829, 0.9931, 0.9565, 0.9609, 0.9520, 0.9846, 0.9566, 0.9785,\n",
       "                      0.9905, 0.9999, 0.9635, 0.9194, 0.9320, 0.9692, 0.9324, 1.0015, 0.9445,\n",
       "                      0.9747, 0.9608, 0.9208, 0.9979, 0.9853, 0.9823, 1.0130, 0.9700, 0.9608,\n",
       "                      0.9678, 1.0084, 0.8993, 0.9490, 0.9180, 0.9264, 0.9680, 0.9001, 0.9861,\n",
       "                      0.9395, 0.9518, 0.9732, 0.9269, 0.9810, 0.9358, 0.9847, 0.9560, 0.8923,\n",
       "                      0.9473, 0.9138, 0.9224, 0.9066, 0.9315, 0.9197, 0.9654, 0.9076, 0.9523,\n",
       "                      0.9063, 0.9219, 0.9716, 0.9279, 0.9469, 0.9367, 0.9098, 0.9359, 0.9470,\n",
       "                      0.9384, 0.9691, 1.0130, 0.9228, 0.9079, 0.9287, 0.9244, 0.9392, 0.9469,\n",
       "                      0.9669, 0.9742, 0.9726, 0.9513, 0.9456, 0.9581, 0.9698, 1.0011, 0.9883,\n",
       "                      0.8746, 0.9279, 0.9256, 0.9240, 0.9239, 0.9372, 0.9578, 0.9006, 0.9232,\n",
       "                      0.9409, 0.8948, 1.0095, 0.9144, 0.9317, 0.9768, 0.9428, 0.8917, 0.9658,\n",
       "                      0.9110, 0.9922, 0.9154, 0.9337, 0.9202, 0.9126, 0.9306, 0.9806, 0.9091,\n",
       "                      0.9418, 0.9390, 0.9539, 0.9148, 0.9539, 0.9824, 0.9793, 0.9842, 0.9809,\n",
       "                      0.9443, 0.9497, 0.9865, 0.9516, 0.9627, 0.9844, 0.9359, 0.9611, 0.9529,\n",
       "                      0.9411, 0.9223, 0.9480, 0.9724, 0.9544, 0.9586, 0.9391, 0.9833, 0.9325,\n",
       "                      0.9344, 0.9574, 0.9446, 0.9572, 0.9175, 0.9477, 0.9311, 0.9295, 0.9739,\n",
       "                      0.9504, 0.9900, 0.9296, 0.9493, 0.9646, 0.8761, 0.9156, 0.8982, 0.9021,\n",
       "                      0.9491, 0.9708, 0.9699, 0.9899, 0.9386, 0.9413, 0.9697, 0.9321, 0.9237,\n",
       "                      0.9543, 0.9834, 0.9474, 0.8928, 0.9812, 0.9724, 0.9933, 0.9335, 0.9550,\n",
       "                      0.8925, 0.9866, 0.9199, 0.9760, 0.9409, 0.8958, 0.9468, 0.9465, 0.9407,\n",
       "                      0.9712, 0.9153, 1.0405], device='cuda:0')),\n",
       "             ('layers.7.mha.query.weight',\n",
       "              tensor([[ 0.0302, -0.0781,  0.0283,  ..., -0.0119,  0.0263, -0.0067],\n",
       "                      [-0.0327,  0.0338,  0.0129,  ...,  0.0125,  0.0530,  0.0336],\n",
       "                      [-0.0023,  0.0034,  0.0243,  ...,  0.0484,  0.0011,  0.0341],\n",
       "                      ...,\n",
       "                      [-0.0257, -0.0173, -0.0166,  ..., -0.0105,  0.0124, -0.0470],\n",
       "                      [-0.0204,  0.0003,  0.0135,  ..., -0.0510,  0.0317,  0.0400],\n",
       "                      [-0.0305,  0.0091,  0.0675,  ...,  0.0502,  0.0279,  0.0660]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.mha.key.weight',\n",
       "              tensor([[-0.0596, -0.0122,  0.0062,  ...,  0.0059,  0.0159, -0.0434],\n",
       "                      [-0.0165,  0.0091,  0.0105,  ..., -0.0123, -0.0044,  0.0563],\n",
       "                      [ 0.0172, -0.0291, -0.0084,  ..., -0.0086, -0.0147,  0.0068],\n",
       "                      ...,\n",
       "                      [ 0.0351,  0.0384, -0.0191,  ...,  0.0202, -0.0433, -0.0055],\n",
       "                      [-0.0674,  0.0332, -0.0388,  ...,  0.0258,  0.0156,  0.0166],\n",
       "                      [-0.0357,  0.0153,  0.0121,  ...,  0.0195, -0.0483, -0.0234]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.mha.value.weight',\n",
       "              tensor([[-3.1653e-02,  1.4778e-02, -8.4067e-03,  ...,  2.1870e-02,\n",
       "                        3.6721e-02,  4.5560e-02],\n",
       "                      [-7.2294e-02,  5.4423e-02, -6.5041e-03,  ...,  6.5275e-03,\n",
       "                       -1.1550e-01, -5.9213e-02],\n",
       "                      [ 2.4915e-02,  3.7768e-02,  1.3919e-04,  ..., -5.5518e-02,\n",
       "                        7.3905e-02, -5.9333e-02],\n",
       "                      ...,\n",
       "                      [ 4.6063e-03,  1.9552e-02,  6.3335e-02,  ..., -6.2238e-02,\n",
       "                       -1.2356e-02,  5.0162e-02],\n",
       "                      [-2.5254e-02,  2.4426e-02,  1.4434e-01,  ...,  6.1814e-02,\n",
       "                        5.4613e-02,  1.1443e-01],\n",
       "                      [ 6.5231e-03, -1.5453e-01,  6.7234e-03,  ...,  5.7074e-02,\n",
       "                        7.9696e-03, -1.7670e-02]], device='cuda:0')),\n",
       "             ('layers.7.mha.proj.weight',\n",
       "              tensor([[-0.0134,  0.0359, -0.0279,  ...,  0.0436,  0.0834, -0.0473],\n",
       "                      [-0.0222, -0.0185, -0.0419,  ..., -0.0618, -0.0159, -0.0146],\n",
       "                      [-0.0145, -0.0107, -0.0550,  ...,  0.1280,  0.0390, -0.0083],\n",
       "                      ...,\n",
       "                      [ 0.0384,  0.0120, -0.0421,  ...,  0.0323,  0.0412,  0.0632],\n",
       "                      [ 0.0108, -0.0141,  0.0013,  ..., -0.0559,  0.0125,  0.0058],\n",
       "                      [ 0.0194,  0.0243,  0.0115,  ..., -0.0212, -0.0691,  0.0120]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.ffn.w.weight',\n",
       "              tensor([[ 2.8688e-02,  5.2218e-02, -8.0053e-02,  ...,  4.1666e-02,\n",
       "                       -1.7975e-02, -5.4661e-02],\n",
       "                      [ 9.2035e-03,  1.5199e-02, -4.4990e-02,  ..., -5.4524e-02,\n",
       "                       -4.2433e-02,  4.7393e-03],\n",
       "                      [-3.9758e-05, -6.4829e-02,  8.7847e-02,  ...,  4.0066e-02,\n",
       "                        2.7534e-02,  4.5233e-02],\n",
       "                      ...,\n",
       "                      [ 6.5911e-02,  4.0133e-02,  2.2049e-04,  ..., -6.1215e-03,\n",
       "                        7.2783e-02,  3.6633e-02],\n",
       "                      [-4.4028e-02,  2.1883e-02, -5.7276e-02,  ...,  1.2606e-02,\n",
       "                        4.8818e-02, -2.2619e-02],\n",
       "                      [-3.4144e-02,  1.9942e-02, -5.9597e-02,  ..., -2.0364e-02,\n",
       "                        8.4687e-03,  2.1771e-02]], device='cuda:0')),\n",
       "             ('layers.7.ffn.w.bias',\n",
       "              tensor([-0.1217, -0.0773, -0.0352,  ..., -0.0800, -0.0530, -0.0105],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.ffn.v.weight',\n",
       "              tensor([[ 0.0019,  0.0063,  0.0389,  ..., -0.0238,  0.0134,  0.1059],\n",
       "                      [ 0.0127, -0.0428, -0.0736,  ...,  0.0340,  0.0149,  0.0071],\n",
       "                      [-0.0302,  0.0794,  0.0042,  ...,  0.0629, -0.0048, -0.0073],\n",
       "                      ...,\n",
       "                      [-0.0482,  0.0446,  0.0193,  ...,  0.0647, -0.0180,  0.0221],\n",
       "                      [ 0.0159, -0.0084, -0.0083,  ...,  0.0841, -0.0112,  0.0060],\n",
       "                      [ 0.0567, -0.0210, -0.0111,  ...,  0.0125, -0.0002,  0.0145]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.ffn.v.bias',\n",
       "              tensor([ 0.0100,  0.0590, -0.0262,  ...,  0.0144, -0.0750,  0.0108],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.ffn.w2.weight',\n",
       "              tensor([[-0.0179, -0.0369,  0.0096,  ...,  0.0281, -0.0493,  0.0295],\n",
       "                      [-0.0260, -0.0239, -0.0315,  ...,  0.0143, -0.0455,  0.0242],\n",
       "                      [ 0.0014,  0.0344, -0.0284,  ...,  0.0236, -0.0404,  0.0144],\n",
       "                      ...,\n",
       "                      [-0.0299,  0.0017,  0.0394,  ...,  0.0387, -0.0398,  0.0309],\n",
       "                      [ 0.0055,  0.0178, -0.0162,  ...,  0.1126,  0.0547,  0.0197],\n",
       "                      [-0.1139, -0.0257, -0.0286,  ..., -0.0259, -0.0153,  0.0631]],\n",
       "                     device='cuda:0')),\n",
       "             ('layers.7.ffn.w2.bias',\n",
       "              tensor([ 1.9888e-01, -6.8053e-02,  4.8583e-02, -3.2046e-02,  8.0604e-02,\n",
       "                       7.5773e-02, -1.9831e-01, -7.1076e-03, -3.5277e-02, -8.7973e-02,\n",
       "                       9.8088e-02, -1.4501e-01, -6.7452e-02,  1.9131e-01, -1.3805e-01,\n",
       "                      -1.1586e-01,  9.8394e-02, -7.4305e-02, -1.7688e-01, -1.5313e-01,\n",
       "                       2.8004e-02, -1.2408e-01, -1.0886e-02, -1.1202e-01, -5.6254e-02,\n",
       "                      -9.0281e-02, -1.1058e-01, -1.6640e-01,  2.9629e-02, -1.1624e-01,\n",
       "                      -2.7324e-01, -1.2195e-01,  1.3945e-01,  6.8928e-02,  1.8502e-01,\n",
       "                       1.2888e-01,  1.2303e-01, -1.6901e-01,  1.7196e-01, -3.0592e-02,\n",
       "                       8.6294e-02,  1.0493e-01, -7.0303e-02, -1.4538e-01, -1.4984e-01,\n",
       "                      -7.2845e-02,  1.5223e-01,  1.2166e-01, -2.9608e-02, -1.9857e-01,\n",
       "                      -1.5903e-01,  2.1616e-03,  2.0473e-02,  9.9858e-02, -6.5088e-02,\n",
       "                      -6.2157e-02, -4.6594e-02,  8.0026e-02,  2.1554e-01, -2.0777e-02,\n",
       "                       1.5071e-01,  6.1324e-02, -4.0599e-02,  5.5220e-02,  1.2920e-02,\n",
       "                       4.9070e-02,  4.5176e-02, -1.4240e-01, -9.3661e-03, -8.6809e-03,\n",
       "                      -1.9160e-01, -1.9317e-01, -1.0093e-01, -1.5836e-01,  1.7433e-01,\n",
       "                      -8.8568e-02, -1.2128e-02, -1.2928e-01,  8.4183e-02, -5.8070e-02,\n",
       "                       2.3901e-02, -1.6528e-01, -2.1079e-02,  1.2145e-01,  8.2335e-02,\n",
       "                      -1.2169e-01,  8.8270e-02,  5.8597e-02, -7.1512e-02,  1.4748e-01,\n",
       "                      -1.3086e-01,  1.2879e-01,  8.7634e-02,  8.0273e-02,  1.9502e-02,\n",
       "                      -2.8433e-02, -8.2402e-02, -7.9590e-02, -7.4840e-02, -1.6991e-01,\n",
       "                      -1.6623e-01, -7.5216e-02, -7.3064e-02, -8.3853e-03,  1.2015e-01,\n",
       "                       4.2171e-02, -1.3188e-01, -4.1465e-02,  1.0033e-01, -4.6544e-02,\n",
       "                      -2.0568e-01,  1.0208e-02,  1.2584e-01, -1.3859e-01, -2.4547e-02,\n",
       "                       1.0633e-01,  1.1555e-01,  7.3913e-02,  1.3236e-01, -1.9313e-01,\n",
       "                      -9.1956e-02, -1.4717e-01,  2.2353e-01, -1.8123e-01,  1.5607e-02,\n",
       "                      -1.7270e-01, -1.6498e-01, -1.2601e-01, -2.4043e-01, -1.1401e-01,\n",
       "                      -6.2840e-02,  1.3351e-01,  1.5705e-01,  9.2677e-02,  1.7965e-01,\n",
       "                      -1.3051e-01, -1.1361e-01,  1.3857e-01,  1.2804e-03, -8.2191e-03,\n",
       "                       1.8189e-01,  1.9704e-01, -1.4085e-01,  1.0954e-01,  1.9215e-01,\n",
       "                       1.2933e-01, -1.1159e-01, -1.2837e-01, -1.2372e-01, -2.0710e-01,\n",
       "                       4.8362e-02, -1.5356e-01,  1.8307e-02, -6.8151e-02, -8.3632e-02,\n",
       "                      -6.7017e-02, -8.2732e-02, -3.3248e-02, -4.8054e-02, -5.9512e-02,\n",
       "                      -4.0086e-02, -1.6094e-01,  9.5457e-02,  1.4914e-02, -1.6859e-01,\n",
       "                       2.1392e-01, -2.9284e-03, -1.3403e-01,  1.0441e-01, -3.1776e-02,\n",
       "                      -2.0487e-01, -9.2030e-02, -1.1274e-01,  1.2751e-01,  1.2066e-01,\n",
       "                       9.3896e-02, -1.8885e-01,  1.1709e-01, -1.3787e-01,  1.8939e-01,\n",
       "                      -1.4852e-01, -3.1351e-02, -8.9847e-02, -1.6223e-01, -1.1872e-01,\n",
       "                      -2.4394e-01,  1.4251e-01,  1.4879e-01, -1.3849e-01,  1.2813e-01,\n",
       "                      -1.2521e-01, -1.9343e-01, -9.7797e-02,  1.2746e-01, -6.1211e-02,\n",
       "                      -9.9513e-02, -1.8661e-01, -1.0470e-01, -7.9298e-03, -1.3154e-02,\n",
       "                       4.2874e-02, -1.4637e-01, -8.2358e-02,  1.2733e-01, -1.2712e-01,\n",
       "                      -1.3969e-01, -1.7428e-01,  6.3306e-02, -5.4431e-02,  1.3169e-01,\n",
       "                       2.5408e-04, -7.4565e-02,  1.1925e-01, -2.9178e-02, -5.9032e-02,\n",
       "                       1.4146e-01,  1.7949e-01, -2.6177e-02,  1.6746e-01, -3.6748e-02,\n",
       "                      -7.2221e-02, -1.4896e-01,  2.0422e-02, -7.9531e-03, -1.4875e-01,\n",
       "                      -1.2440e-01, -9.7341e-02,  1.1532e-01, -1.4715e-01, -4.2852e-02,\n",
       "                       1.3738e-02,  2.8229e-01,  7.2534e-02, -1.3510e-01,  1.5042e-01,\n",
       "                       1.0802e-01, -6.6726e-02, -1.0071e-01, -9.3452e-02,  9.1990e-02,\n",
       "                       1.7992e-01, -1.4816e-01, -5.8259e-02, -5.4502e-02,  1.5265e-01,\n",
       "                       6.0448e-02,  1.1639e-01,  1.1491e-01, -1.7131e-01,  5.3231e-02,\n",
       "                       3.8088e-02,  1.1712e-01,  1.9656e-01,  1.1222e-01, -1.1104e-01,\n",
       "                       1.0796e-01,  8.5439e-02,  6.6678e-02, -1.9261e-01,  1.9165e-02,\n",
       "                       6.5535e-02,  5.0921e-02,  1.6232e-01,  7.3588e-02, -1.4379e-01,\n",
       "                       1.1904e-01, -9.6592e-02, -9.0968e-03, -3.6466e-02,  1.6002e-01,\n",
       "                       5.6536e-02, -1.0960e-01,  3.1988e-03, -1.4209e-02,  1.4891e-01,\n",
       "                      -2.6698e-02, -6.2387e-02, -6.1542e-02,  3.3106e-02, -1.2339e-01,\n",
       "                       1.6564e-01, -2.9048e-01, -6.0811e-02, -1.6745e-01, -2.5386e-02,\n",
       "                      -5.1610e-02, -9.0396e-02,  1.1873e-01,  4.8067e-02, -6.6323e-02,\n",
       "                       2.3187e-01, -1.4505e-01,  5.3237e-02,  9.0835e-02,  8.7671e-02,\n",
       "                      -1.1300e-01,  1.0058e-01,  1.0575e-01, -2.3158e-01, -4.5488e-02,\n",
       "                       8.6151e-02,  1.4793e-01,  7.6393e-02,  4.4482e-02, -5.1333e-02,\n",
       "                      -2.2553e-02, -2.0375e-01, -1.4898e-01,  1.5344e-01, -7.0132e-02,\n",
       "                       1.3172e-01,  1.6883e-01, -4.6623e-02, -1.3623e-01, -1.9222e-01,\n",
       "                      -7.7534e-02,  4.1115e-02, -2.3429e-01, -1.0436e-01,  1.4100e-01,\n",
       "                       8.1101e-02,  1.2347e-01, -5.4662e-02,  5.2587e-02,  1.8891e-02,\n",
       "                       1.3420e-01, -9.1149e-02,  1.0766e-02, -2.9649e-02,  1.2815e-01,\n",
       "                      -5.7605e-02,  1.0483e-01,  3.1903e-02, -3.1530e-02, -1.2014e-01,\n",
       "                       1.0550e-01, -2.3012e-01, -1.7127e-01, -1.5156e-02,  3.9280e-02,\n",
       "                       1.3645e-01,  1.6999e-03, -1.6431e-01, -3.3765e-02, -4.6939e-02,\n",
       "                       5.0172e-02, -1.5260e-02, -9.0908e-02, -1.8914e-01, -5.7131e-02,\n",
       "                      -9.9630e-02,  9.6563e-03,  1.0608e-01, -5.5396e-02,  1.4630e-01,\n",
       "                      -3.9579e-02, -1.3973e-01, -1.1439e-02,  9.9177e-03, -6.6111e-02,\n",
       "                      -2.2817e-02,  2.5616e-02,  5.7908e-02,  3.4081e-02, -1.1153e-01,\n",
       "                      -6.1931e-03,  1.1837e-01, -1.0657e-01,  1.9336e-02, -1.6351e-01,\n",
       "                      -1.2032e-01,  2.9101e-02,  2.0251e-01,  6.3397e-02, -1.8195e-01,\n",
       "                       2.5604e-02,  5.5992e-02,  1.4279e-01, -1.6969e-01, -1.0367e-01,\n",
       "                      -1.9674e-01,  3.7630e-02,  1.8477e-01, -2.9811e-02,  9.3863e-02,\n",
       "                       2.2210e-01, -2.0779e-01, -1.7222e-01,  7.1549e-02,  1.5098e-01,\n",
       "                       2.1175e-02,  6.4461e-02, -1.3549e-01, -3.4799e-02,  1.1940e-01,\n",
       "                      -5.3360e-02,  1.1659e-01,  9.0911e-02, -1.3595e-01,  8.8837e-02,\n",
       "                      -1.7938e-01,  7.0823e-02, -1.1278e-01,  6.3891e-02, -6.4492e-02,\n",
       "                       1.5412e-01, -1.6638e-01,  3.1683e-01,  5.2594e-01, -6.8628e-02,\n",
       "                      -2.4178e-02,  8.6001e-02, -9.7348e-02,  6.3996e-02,  2.1560e-01,\n",
       "                      -1.7375e-01, -2.1713e-01, -8.7511e-02, -1.7599e-01,  4.7468e-02,\n",
       "                       1.8604e-01, -6.2831e-02, -1.9009e-01,  7.9262e-02,  1.2911e-01,\n",
       "                       3.3121e-02, -1.8330e-02, -4.7041e-02, -6.9942e-02, -8.5017e-02,\n",
       "                       5.2830e-02,  1.6854e-02,  8.0743e-02,  6.9059e-02,  5.7827e-02,\n",
       "                      -1.4424e-01, -1.5377e-01, -4.0290e-02,  1.8742e-01,  4.9782e-02,\n",
       "                       1.8710e-02, -5.5230e-02, -5.1987e-02, -1.5454e-01, -1.4420e-01,\n",
       "                      -7.4718e-02,  4.9229e-02, -1.2541e-01,  1.3016e-01, -9.7864e-02,\n",
       "                      -1.5547e-01, -9.4828e-02,  1.1294e-01,  1.7426e-01, -1.9245e-02,\n",
       "                      -6.6077e-02,  5.9691e-02, -2.2619e-02, -1.4609e-01, -1.1093e-01,\n",
       "                       1.0543e-01, -2.2425e-02,  4.7809e-02,  7.1684e-02,  3.0925e-02,\n",
       "                      -1.2417e-01,  2.2479e-01,  4.7443e-03,  2.2176e-01, -2.3129e-02,\n",
       "                       1.3055e-01, -4.4010e-02, -2.3441e-02,  4.9373e-02, -1.6893e-01,\n",
       "                      -9.8459e-02,  1.3324e-01,  1.9945e-01,  1.5933e-01, -7.2162e-02,\n",
       "                       4.4040e-02,  1.4389e-01,  1.3563e-01, -8.8038e-02,  6.3694e-02,\n",
       "                       1.4411e-01, -1.5796e-01,  4.9085e-02,  2.1529e-02,  2.6545e-02,\n",
       "                       1.6657e-01,  1.0551e-01,  1.6770e-01,  1.3155e-01, -5.6727e-02,\n",
       "                      -1.9168e-01,  5.9013e-02, -1.9000e-01,  3.3713e-02, -1.4888e-01,\n",
       "                       6.7922e-02, -1.5909e-01, -1.4669e-01,  2.2794e-01, -1.3840e-01,\n",
       "                       3.1537e-02, -2.0502e-01, -1.5453e-01,  1.0488e-01, -1.0894e-01,\n",
       "                       7.2504e-02,  1.6885e-01,  1.8372e-01, -1.7389e-01, -9.7984e-02,\n",
       "                       1.6384e-02,  1.7816e-01,  1.4693e-01, -1.1219e-01,  1.3392e-01,\n",
       "                      -5.7146e-03, -1.5678e-01,  1.7548e-01,  1.3425e-01,  6.1504e-03,\n",
       "                      -6.1658e-02, -1.9539e-01, -5.8223e-02, -1.9285e-01,  1.0751e-01,\n",
       "                      -7.3123e-02, -1.4616e-01, -1.2088e-01, -7.1418e-02,  1.6649e-01,\n",
       "                       1.1118e-01, -1.0095e-01, -7.5878e-02,  1.1660e-01,  6.6112e-02,\n",
       "                      -1.1516e-01, -1.9224e-01, -1.6432e-01,  3.8225e-02, -7.9462e-02,\n",
       "                      -2.3923e-01,  1.0871e-01,  4.0084e-02,  1.4995e-01, -1.6634e-01,\n",
       "                       1.3369e-01, -3.5537e-02,  2.1574e-01, -1.6625e-01, -2.1640e-01,\n",
       "                       3.5281e-02,  6.1358e-02,  1.7520e-01, -1.9255e-01,  1.3453e-01,\n",
       "                      -1.1322e-01,  7.8509e-02,  6.8598e-02,  1.4809e-01, -7.2705e-02,\n",
       "                       1.0345e-01,  2.8742e-02,  4.2720e-03, -2.8903e-03, -1.4523e-01,\n",
       "                       1.1159e-01, -4.7256e-02,  1.4410e-01,  1.5611e-01, -2.5021e-03,\n",
       "                       9.2293e-02, -8.4609e-03, -1.0877e-01, -1.6428e-01,  8.3580e-02,\n",
       "                      -6.9237e-04, -1.2367e-01,  1.5585e-01,  3.7731e-02,  5.3372e-02,\n",
       "                      -1.5581e-01,  6.9741e-02,  7.6490e-02,  1.4935e-01, -3.6523e-02,\n",
       "                      -4.0514e-03,  1.4162e-02, -4.0656e-02, -2.3435e-01, -1.6117e-02,\n",
       "                       5.2022e-02, -5.6904e-02, -1.6630e-02,  1.6021e-01, -6.6894e-02,\n",
       "                      -3.6313e-02, -1.7733e-01, -1.7584e-01,  2.9803e-02, -1.1981e-01,\n",
       "                       1.3066e-01,  2.8915e-03,  1.5964e-01, -9.5877e-02, -3.6384e-02,\n",
       "                      -1.3555e-02, -1.7874e-01,  7.3804e-04,  3.0478e-02, -9.8228e-02,\n",
       "                      -4.8446e-02, -9.3108e-02,  4.6793e-02,  9.4465e-02, -1.1904e-01,\n",
       "                      -1.9264e-01,  1.5928e-01,  1.4948e-02,  3.3303e-02,  1.7212e-01,\n",
       "                       1.4100e-01,  1.7013e-01, -1.1648e-01,  1.0987e-02, -1.6267e-02,\n",
       "                      -4.5894e-03,  1.3141e-01, -2.4312e-01,  1.9041e-01, -2.8707e-02,\n",
       "                       1.7565e-01, -4.9552e-02, -1.2147e-02,  2.6648e-02, -8.6411e-02,\n",
       "                       1.9557e-01, -2.2971e-01,  1.3585e-01, -1.6673e-01,  1.4721e-01,\n",
       "                       1.5175e-01, -1.1385e-01, -1.3443e-01,  8.3629e-02,  1.3767e-01,\n",
       "                       9.4825e-02,  1.0848e-01, -1.2213e-01, -1.8084e-01,  5.8767e-02,\n",
       "                      -1.2471e-01, -6.2587e-02,  1.2130e-01, -1.8199e-01,  8.1966e-02,\n",
       "                      -9.4047e-02, -1.0669e-01, -4.6661e-02,  1.2719e-01,  2.9526e-02,\n",
       "                       2.0672e-02,  1.5090e-01, -3.5916e-02,  1.2222e-01,  1.1242e-01,\n",
       "                      -1.2596e-01, -2.3988e-03, -7.1033e-02, -1.1706e-01, -1.4512e-01,\n",
       "                       1.9507e-01, -1.3340e-01,  2.2356e-02, -1.6215e-02, -1.0916e-02,\n",
       "                      -1.3030e-01,  1.8377e-01, -1.1617e-01,  8.6561e-02, -2.0555e-01,\n",
       "                       4.8807e-02, -1.9657e-01, -3.1209e-02,  1.4897e-01, -9.8690e-02,\n",
       "                      -1.5258e-01,  7.9317e-02, -1.3876e-01,  2.0291e-01,  1.7968e-02,\n",
       "                      -1.1617e-01,  1.0259e-01,  5.0721e-02,  1.0466e-01, -1.6343e-01,\n",
       "                      -2.4420e-02, -8.1201e-02, -2.9658e-02,  8.4591e-02,  1.1678e-01,\n",
       "                      -4.9967e-02,  1.1165e-01, -1.7915e-01,  3.5316e-02,  3.4531e-02,\n",
       "                      -3.3578e-02, -9.5980e-02,  1.4589e-01,  5.4112e-02,  1.5895e-01,\n",
       "                       7.8060e-02, -1.9092e-02, -1.0016e-01,  5.8925e-02,  1.1592e-01,\n",
       "                       6.2497e-02,  6.4812e-03, -3.9356e-02,  1.5700e-01, -1.5865e-01,\n",
       "                       3.1514e-02, -1.1841e-01, -1.3217e-01, -3.0150e-02, -7.4648e-02,\n",
       "                      -1.4931e-01, -1.3765e-01, -1.8108e-01, -1.2618e-01,  1.6399e-01,\n",
       "                       5.8680e-02,  2.3513e-01,  1.9759e-01,  1.5794e-01, -7.3293e-02,\n",
       "                      -9.8580e-02, -1.4151e-01, -1.3976e-01,  6.5628e-02, -7.7536e-02,\n",
       "                       8.8378e-02, -2.9834e-03,  1.0090e-01,  1.6028e-01, -1.5620e-01,\n",
       "                      -1.4195e-01, -9.7388e-03,  2.2760e-02,  1.2383e-01,  2.1681e-02,\n",
       "                      -5.8211e-02,  3.0747e-02, -1.6281e-01, -3.5832e-02, -6.7839e-02,\n",
       "                       1.8623e-01,  1.2679e-01, -1.6919e-01, -2.8202e-02, -5.0490e-02,\n",
       "                       1.0810e-01,  9.8868e-02,  1.6100e-01], device='cuda:0')),\n",
       "             ('layers.7.anorm.weight',\n",
       "              tensor([0.9133, 1.0119, 0.9497, 0.9942, 0.9895, 0.9577, 0.9785, 1.0616, 1.0180,\n",
       "                      1.0078, 1.0064, 0.9934, 0.9855, 0.9815, 0.9720, 0.9704, 0.9765, 0.9463,\n",
       "                      0.9963, 0.9589, 0.9992, 1.0041, 0.9245, 1.0121, 0.9938, 0.9989, 1.0178,\n",
       "                      0.9491, 0.9753, 1.0060, 1.0195, 1.0152, 1.0429, 0.9641, 0.9914, 1.0414,\n",
       "                      0.9717, 0.8771, 0.9964, 1.0525, 0.9992, 0.9447, 0.9860, 1.0145, 0.9948,\n",
       "                      1.0496, 0.9730, 0.9932, 1.0629, 1.0266, 0.9755, 0.9990, 1.0084, 1.0237,\n",
       "                      1.0282, 0.9679, 1.0193, 1.0670, 0.9026, 1.0026, 1.0297, 0.9855, 0.9573,\n",
       "                      1.0045, 0.9741, 0.9258, 0.9954, 1.0646, 1.0078, 0.9516, 0.9958, 0.9973,\n",
       "                      0.9648, 1.0415, 0.9338, 1.0166, 0.9820, 1.0064, 0.9360, 1.0233, 1.0632,\n",
       "                      1.0003, 1.0016, 0.9981, 0.9591, 1.0095, 0.9536, 0.9969, 0.9653, 1.0404,\n",
       "                      1.0053, 1.0056, 0.9891, 1.0625, 1.0421, 1.0225, 0.9768, 1.0069, 1.0095,\n",
       "                      1.0025, 0.9104, 1.0182, 1.0746, 0.9685, 1.0128, 0.9505, 1.0546, 1.1130,\n",
       "                      1.0278, 0.9482, 1.0624, 0.9908, 1.0156, 0.8986, 0.9205, 1.0154, 1.0558,\n",
       "                      1.0323, 1.0283, 0.9888, 0.9964, 1.0152, 0.9471, 1.0413, 0.9622, 0.9984,\n",
       "                      0.9921, 0.9747, 0.9994, 1.0414, 1.0191, 0.9658, 0.9169, 0.9895, 0.9771,\n",
       "                      0.9568, 1.0211, 1.0152, 1.0108, 0.9987, 0.9683, 1.0694, 1.0620, 0.9716,\n",
       "                      0.9830, 1.0232, 1.0126, 0.9650, 0.9876, 0.9564, 1.0535, 1.0319, 0.9825,\n",
       "                      1.0042, 0.8781, 0.9806, 1.0160, 1.0670, 0.9889, 0.9778, 1.0969, 1.0501,\n",
       "                      1.0842, 1.0531, 0.9830, 0.9806, 1.0658, 0.9732, 1.0112, 0.9651, 0.9954,\n",
       "                      0.9869, 1.0312, 0.9656, 0.9638, 0.9876, 1.0932, 0.9869, 0.9771, 1.0137,\n",
       "                      0.9856, 0.9400, 1.0189, 1.0425, 1.0176, 1.0097, 1.0055, 1.0048, 1.0208,\n",
       "                      1.0193, 0.9222, 0.9840, 1.0832, 1.0146, 0.9269, 1.0846, 0.9707, 0.9332,\n",
       "                      1.0149, 0.9940, 0.9286, 1.0584, 1.0084, 0.8519, 1.0667, 0.9499, 0.9849,\n",
       "                      0.9975, 1.0031, 1.0782, 1.0093, 1.0360, 1.0184, 0.9502, 1.0078, 1.0255,\n",
       "                      0.9697, 1.0273, 1.0367, 1.0151, 0.9786, 1.0211, 0.9585, 0.9705, 0.9321,\n",
       "                      0.9448, 1.0933, 1.0249, 0.9515, 0.9740, 1.0179, 0.9628, 1.0145, 0.9942,\n",
       "                      0.9804, 1.0293, 1.0428, 0.9805, 1.0368, 0.9752, 0.9758, 0.9591, 0.9825,\n",
       "                      0.9938, 1.0088, 1.0157, 0.9808, 1.0543, 0.9895, 0.9721, 0.9888, 0.9699,\n",
       "                      0.9683, 1.0350, 0.9908, 1.0590, 0.9526, 0.9340, 1.0084, 1.0029, 1.0262,\n",
       "                      0.9832, 0.9686, 1.0221, 0.9277, 0.9714, 1.0062, 1.0417, 1.0033, 1.0316,\n",
       "                      1.0388, 0.9485, 1.0193, 0.9470, 0.9240, 1.0226, 1.0125, 1.0167, 0.9384,\n",
       "                      0.9332, 1.0547, 1.0096, 1.0082, 1.0091, 1.0444, 1.0475, 1.0189, 0.9298,\n",
       "                      1.0115, 1.0506, 1.0444, 1.0058, 1.0757, 1.0160, 1.0570, 0.9530, 0.9581,\n",
       "                      0.9593, 1.0334, 0.9989, 1.0513, 0.9963, 0.9923, 1.0355, 1.0109, 1.0091,\n",
       "                      0.9433, 1.0183, 0.8987, 0.9454, 0.9197, 1.0292, 1.0156, 0.9900, 1.0026,\n",
       "                      1.0237, 0.9211, 1.0722, 1.0386, 1.0205, 1.0120, 1.0348, 1.0434, 0.9937,\n",
       "                      0.9587, 1.0407, 0.9921, 1.0284, 1.0463, 0.9691, 0.9911, 1.0144, 1.0179,\n",
       "                      0.9447, 0.9630, 0.9855, 1.0536, 1.1214, 0.9764, 1.0081, 1.0133, 1.0157,\n",
       "                      0.9940, 1.0185, 0.9192, 0.9644, 1.0374, 1.0191, 1.0304, 1.0087, 1.0378,\n",
       "                      1.0575, 0.9813, 1.0494, 1.0240, 1.0178, 0.9468, 0.9976, 1.0229, 0.9953,\n",
       "                      0.9763, 1.0538, 0.9833, 0.9615, 0.9993, 0.9466, 0.9857, 0.9306, 0.9904,\n",
       "                      0.9721, 1.0534, 1.0273, 1.0641, 0.9246, 1.0651, 1.0419, 1.0130, 0.9528,\n",
       "                      1.0005, 1.0012, 0.9843, 0.9788, 1.0185, 1.0306, 0.9421, 0.9728, 0.9879,\n",
       "                      1.0277, 0.9419, 0.9260, 0.9687, 1.0113, 0.9730, 0.9024, 0.9507, 1.0627,\n",
       "                      1.0115, 1.0573, 0.9489, 1.0047, 0.9577, 0.9880, 0.9887, 1.0363, 1.0282,\n",
       "                      0.9645, 1.0218, 0.9618, 0.7457, 1.0069, 0.9912, 0.9175, 0.9999, 0.9287,\n",
       "                      0.9931, 0.9839, 1.0081, 0.9938, 0.9576, 0.9647, 0.9444, 0.9165, 1.0014,\n",
       "                      0.9359, 1.0727, 1.0364, 0.9812, 0.9931, 1.0024, 1.0186, 0.9625, 1.0039,\n",
       "                      1.0002, 0.9568, 1.0616, 1.0262, 0.9581, 0.9565, 1.0015, 1.0170, 0.9861,\n",
       "                      0.9774, 0.9566, 0.9727, 0.9247, 1.0233, 1.0548, 1.0097, 0.9987, 1.0296,\n",
       "                      0.9370, 0.9833, 0.9592, 1.0336, 1.0453, 1.0484, 0.9036, 0.9277, 1.0406,\n",
       "                      1.0105, 0.9406, 0.9719, 0.9655, 0.9761, 0.9854, 1.0171, 1.0022, 0.9881,\n",
       "                      1.0162, 0.9390, 1.0289, 1.0602, 1.1109, 1.0070, 1.0106, 1.0641, 1.0265,\n",
       "                      1.0391, 0.9299, 0.9893, 1.0080, 0.9049, 1.0113, 0.9938, 1.0569, 0.9384,\n",
       "                      0.9482, 1.0242, 0.9911, 1.0038, 0.9915, 1.0233, 0.9453, 1.0429, 0.9694,\n",
       "                      1.0182, 0.9784, 0.9992, 0.8723, 0.9897, 1.0413, 1.0290, 0.9868, 0.9808,\n",
       "                      1.0337, 1.0215, 0.9879, 0.9497, 0.9404, 1.0047, 1.1062, 1.0053, 0.9823,\n",
       "                      1.0575, 0.9805, 1.0110, 1.0066, 0.9632, 1.0147, 0.9987, 1.0549, 1.0143,\n",
       "                      1.0291, 1.0423, 1.0385, 1.0251, 1.0218, 1.0158, 0.9916, 1.0199, 1.0185,\n",
       "                      0.9679, 0.9625, 0.9868, 1.0438, 1.0817, 1.0333, 1.0257, 1.0355, 0.9867,\n",
       "                      1.0366, 0.9878, 0.9995, 0.9839, 1.0519, 1.0343, 1.0045, 0.9615, 1.0627,\n",
       "                      1.0664, 0.9506, 1.0707, 0.9973, 0.9357, 1.0346, 1.0810, 0.9809, 1.0286,\n",
       "                      1.0087, 1.0054, 1.0427, 1.0139, 1.0701, 1.0042, 1.0397, 0.9183, 0.9474,\n",
       "                      1.0070, 0.9721, 0.9916, 1.0773, 0.9639, 1.0088, 1.0206, 0.9525, 0.9768,\n",
       "                      1.0910, 1.0125, 1.0107, 1.0215, 0.9737, 1.0185, 1.0055, 1.0033, 1.0007,\n",
       "                      1.0288, 0.9983, 0.9927, 1.0234, 1.0827, 1.0161, 1.0099, 1.0066, 1.0302,\n",
       "                      1.0254, 1.0314, 1.0123, 0.9415, 0.9617, 1.0184, 0.9200, 1.0076, 0.9791,\n",
       "                      0.9384, 1.0210, 1.0039, 1.0361, 0.9588, 0.9953, 0.9731, 0.9495, 1.0925,\n",
       "                      1.0328, 1.0380, 0.8981, 1.0563, 1.0514, 0.9904, 0.9158, 1.0135, 1.0988,\n",
       "                      1.0361, 0.9467, 0.9460, 1.1069, 1.0189, 0.9554, 0.9243, 1.0311, 1.0102,\n",
       "                      1.0094, 1.0302, 0.9962, 1.0192, 0.9785, 0.9852, 0.9466, 0.9929, 0.9726,\n",
       "                      1.0057, 0.9949, 1.0462, 0.9452, 1.0113, 1.0247, 1.0359, 0.9914, 1.0027,\n",
       "                      0.9591, 1.0566, 1.0177, 0.9686, 1.0009, 1.0036, 0.9738, 0.9687, 1.0209,\n",
       "                      1.0127, 1.0065, 0.9577, 0.9821, 1.0771, 1.0113, 1.0265, 0.9674, 1.0804,\n",
       "                      1.0088, 1.0244, 1.0281, 0.9895, 1.0414, 1.0127, 0.9426, 0.9607, 1.0024,\n",
       "                      0.9929, 0.9462, 1.0423, 1.0179, 0.9865, 0.9570, 1.0362, 1.0079, 1.0777,\n",
       "                      0.9750, 1.0612, 0.8862, 0.9540, 1.0091, 0.9534, 1.0038, 1.0173, 1.0540,\n",
       "                      0.9746, 0.9964, 1.0225, 0.9590, 1.0087, 0.9920, 0.9680, 1.0071, 1.0756,\n",
       "                      1.0530, 0.9613, 1.0377, 0.9941, 0.9913, 0.9261, 0.9656, 1.0289, 1.0438,\n",
       "                      1.0603, 1.0078, 0.9564, 1.0400, 1.0144, 1.0067, 1.0604, 1.0247, 0.9900,\n",
       "                      0.9197, 1.0010, 1.0124, 1.0458, 1.0469, 1.0531, 0.9430, 0.9086, 1.0942,\n",
       "                      0.9690, 1.0044, 0.9859, 0.9512, 0.9971, 0.9996, 0.9772, 1.0074, 0.9997,\n",
       "                      0.9744, 1.0562, 0.9413, 1.0505, 1.0125, 0.9778, 1.0155, 1.0023, 0.9255,\n",
       "                      1.0116, 1.0210, 0.9761, 0.9515, 1.0209, 1.0462, 1.0200, 0.9475, 1.0034,\n",
       "                      0.9374, 1.0391, 0.9209, 1.0091, 0.9658, 0.9718, 0.9687, 0.9565, 1.0185,\n",
       "                      0.9200, 0.9661, 1.0213], device='cuda:0')),\n",
       "             ('layers.7.fnorm.weight',\n",
       "              tensor([1.0098, 1.0145, 1.0054, 1.0510, 1.0564, 1.0145, 0.9752, 1.0436, 1.0274,\n",
       "                      0.9934, 1.0064, 1.0290, 0.9987, 1.0149, 1.0124, 1.0366, 1.0211, 1.0286,\n",
       "                      1.0330, 1.0106, 1.0106, 0.9797, 0.9855, 1.0318, 1.0459, 1.0224, 1.0286,\n",
       "                      1.0107, 1.0427, 1.0377, 1.0491, 1.0265, 1.0047, 1.0554, 1.0354, 1.0155,\n",
       "                      1.0020, 0.9795, 0.9979, 1.0019, 1.0365, 0.9871, 1.0372, 1.0272, 1.0216,\n",
       "                      1.0306, 0.9801, 1.0323, 1.0242, 1.0558, 0.9777, 1.0404, 1.0465, 0.9799,\n",
       "                      1.0579, 1.0294, 1.0448, 1.0109, 0.9229, 1.0523, 0.9973, 1.0152, 1.0222,\n",
       "                      1.0344, 1.0111, 1.0273, 1.0202, 1.0226, 1.0054, 1.0141, 1.0056, 1.0500,\n",
       "                      0.9720, 1.0456, 1.0067, 1.0348, 1.0021, 1.0072, 0.9917, 1.0045, 1.0474,\n",
       "                      0.9745, 0.9824, 1.0400, 0.9984, 1.0211, 1.0158, 1.0132, 1.0141, 1.0251,\n",
       "                      0.9880, 1.0158, 1.0445, 1.0100, 1.0183, 1.0236, 1.0247, 1.0307, 0.9976,\n",
       "                      1.0440, 1.0068, 1.0225, 1.0600, 1.0176, 0.9794, 0.9987, 1.0355, 1.0428,\n",
       "                      1.0448, 1.0527, 0.9937, 1.0337, 1.0131, 0.9637, 1.0403, 0.9947, 0.9876,\n",
       "                      1.0257, 1.0288, 1.0458, 1.0450, 1.0343, 0.9618, 1.0138, 0.9730, 1.0042,\n",
       "                      1.0209, 1.0161, 0.9853, 1.0267, 0.9901, 1.0350, 0.9821, 0.9989, 0.9859,\n",
       "                      1.0446, 1.0287, 1.0042, 1.0290, 1.0456, 1.0186, 0.9968, 0.9870, 0.9882,\n",
       "                      1.0050, 1.0025, 1.0487, 1.0133, 1.0242, 1.0037, 0.9788, 1.0246, 1.0266,\n",
       "                      1.0258, 0.9694, 1.0033, 1.0570, 1.0243, 1.0091, 1.0067, 1.0435, 1.0281,\n",
       "                      1.0012, 1.0866, 0.9720, 1.0386, 1.0713, 1.0605, 1.0432, 1.0396, 1.0054,\n",
       "                      0.9788, 1.0593, 1.0257, 0.9442, 0.9453, 1.0102, 1.0058, 1.0245, 0.9911,\n",
       "                      0.9891, 0.9783, 1.0057, 1.0262, 1.0583, 1.0090, 1.0443, 1.0050, 1.0073,\n",
       "                      0.9978, 1.0163, 0.9728, 1.0484, 1.0014, 0.9748, 1.0246, 1.0169, 1.0014,\n",
       "                      1.0189, 1.0424, 1.0062, 1.0096, 1.0396, 1.0153, 1.0206, 0.9869, 1.0189,\n",
       "                      1.0139, 0.9676, 0.9882, 1.0223, 0.9709, 1.0283, 1.0115, 1.0006, 1.0225,\n",
       "                      1.0049, 1.0275, 1.0411, 1.0397, 1.0231, 1.0380, 0.9833, 0.9747, 1.0446,\n",
       "                      0.9921, 0.6531, 1.0342, 1.0139, 1.0306, 1.0308, 0.9948, 1.0021, 0.9892,\n",
       "                      1.0053, 1.0163, 1.0019, 0.9942, 1.0489, 1.0084, 0.9962, 1.0167, 1.0013,\n",
       "                      1.0695, 1.0289, 1.0555, 1.0119, 1.0032, 1.0258, 1.0451, 1.0212, 1.0345,\n",
       "                      1.0860, 1.0009, 1.0352, 1.0182, 1.0218, 0.9924, 1.0250, 1.0192, 0.9777,\n",
       "                      1.0306, 0.9867, 1.0285, 0.9807, 0.9956, 0.9717, 1.0063, 0.9982, 1.0320,\n",
       "                      1.0237, 1.0071, 0.9828, 0.9710, 1.0495, 1.0248, 1.0743, 1.0425, 0.9953,\n",
       "                      1.0199, 1.0263, 0.9911, 1.0051, 1.0109, 1.0552, 1.0487, 1.0278, 1.0392,\n",
       "                      1.0356, 1.0697, 1.0196, 0.9943, 1.0355, 1.0309, 1.0014, 1.0231, 1.0212,\n",
       "                      1.0004, 1.0521, 1.0636, 1.0013, 1.0318, 1.0271, 1.0269, 1.0530, 0.9959,\n",
       "                      0.9892, 0.9581, 1.0271, 1.0467, 0.9768, 1.0274, 1.0040, 1.0173, 1.0277,\n",
       "                      1.0612, 1.0499, 1.0153, 1.0371, 1.0115, 0.9787, 1.0120, 1.0202, 0.9868,\n",
       "                      1.0390, 1.0024, 1.0031, 1.0195, 1.0339, 1.0099, 0.9854, 1.0032, 0.9889,\n",
       "                      0.9691, 1.0364, 1.0240, 1.0270, 1.0411, 0.9600, 1.0276, 1.0200, 0.9904,\n",
       "                      1.0339, 1.0065, 0.9996, 0.9784, 1.0352, 1.0282, 1.0205, 0.9980, 1.0351,\n",
       "                      0.9758, 1.0196, 1.0251, 1.0114, 1.0341, 0.9932, 1.0484, 1.0103, 1.0040,\n",
       "                      1.0165, 0.9901, 1.0124, 0.9694, 1.0264, 0.9954, 0.9767, 0.9495, 0.9728,\n",
       "                      1.0241, 1.0370, 1.0314, 1.0744, 1.0220, 1.0039, 1.0133, 0.9892, 0.9985,\n",
       "                      1.0400, 1.0016, 1.0080, 1.0164, 0.9964, 1.0137, 1.0524, 1.0258, 1.0021,\n",
       "                      0.9983, 1.0410, 1.0425, 1.0069, 1.0588, 1.0043, 1.0105, 1.0311, 1.0227,\n",
       "                      0.9650, 1.0048, 1.0299, 1.0327, 1.0141, 0.9952, 1.0190, 0.9947, 1.0442,\n",
       "                      0.9752, 1.0162, 1.0256, 0.2447, 0.9836, 1.0207, 1.0041, 0.9923, 0.9633,\n",
       "                      0.9966, 1.0457, 1.0043, 0.9847, 1.0070, 1.0317, 0.9811, 1.0120, 0.9956,\n",
       "                      1.0688, 1.0138, 1.0174, 1.0071, 0.9781, 1.0215, 1.0501, 0.9991, 1.0199,\n",
       "                      1.0002, 0.9989, 1.0323, 1.0417, 1.0334, 1.0112, 0.9927, 0.9836, 1.0164,\n",
       "                      1.0310, 1.0307, 1.0121, 1.0130, 1.0072, 0.9757, 1.0120, 1.0450, 1.0303,\n",
       "                      0.9787, 1.0116, 0.9733, 1.0189, 1.0717, 0.9952, 0.9962, 0.9825, 0.9996,\n",
       "                      1.0233, 1.0400, 1.0222, 1.0305, 0.9794, 1.0125, 1.0346, 0.9901, 1.0022,\n",
       "                      0.9989, 1.0223, 1.0187, 1.0183, 0.9771, 1.0293, 1.0386, 1.0302, 1.0258,\n",
       "                      1.0200, 1.0082, 1.0191, 1.0230, 1.0221, 0.9850, 1.0368, 1.0539, 1.0133,\n",
       "                      0.9965, 1.0545, 1.0013, 0.9883, 1.0037, 0.9771, 0.9857, 1.0384, 1.0337,\n",
       "                      0.9721, 0.9902, 1.0217, 0.7659, 1.0393, 1.0128, 1.0554, 0.9967, 1.0239,\n",
       "                      1.0248, 0.9997, 0.9835, 0.9666, 1.0246, 0.9570, 0.9866, 0.9797, 1.0220,\n",
       "                      1.0032, 0.9976, 1.0499, 1.0534, 0.9844, 1.0169, 1.0308, 0.9834, 1.0036,\n",
       "                      1.0338, 1.0083, 1.0395, 0.9838, 0.9903, 0.9865, 0.9890, 1.0308, 1.0213,\n",
       "                      0.9756, 1.0011, 1.0437, 1.0533, 1.0122, 1.0001, 1.0082, 1.0044, 0.9817,\n",
       "                      1.0353, 0.9825, 1.0001, 1.0836, 1.0160, 1.0036, 0.9743, 1.0444, 1.0217,\n",
       "                      1.0503, 0.9483, 1.0185, 0.9736, 1.0142, 0.9988, 1.0638, 1.0042, 1.0121,\n",
       "                      1.0314, 1.0369, 1.0261, 0.9993, 1.0573, 1.0217, 1.0260, 0.9637, 1.0421,\n",
       "                      1.0093, 1.0469, 1.0283, 1.0028, 1.0020, 1.0068, 1.0325, 1.0298, 0.9702,\n",
       "                      1.0193, 1.0124, 0.9846, 1.0014, 1.0341, 0.9833, 1.0277, 1.0491, 0.9969,\n",
       "                      1.0430, 1.0236, 1.0045, 1.0171, 1.0062, 1.0053, 1.0477, 1.0345, 1.0318,\n",
       "                      1.0522, 1.0327, 1.0051, 1.0259, 0.9788, 1.0264, 0.9857, 1.0283, 1.0240,\n",
       "                      1.0313, 1.0078, 1.0160, 1.0622, 1.0298, 1.0047, 1.0019, 1.0238, 1.0171,\n",
       "                      0.9992, 1.0353, 0.9525, 1.0396, 1.0359, 1.0071, 1.0223, 1.0040, 1.0099,\n",
       "                      1.0384, 1.0202, 1.0367, 1.0305, 1.0237, 1.0482, 1.0559, 1.0348, 0.9941,\n",
       "                      1.0229, 1.0294, 0.9838, 0.9813, 0.9861, 0.9959, 1.0440, 1.0282, 1.0418,\n",
       "                      1.0000, 1.0004, 1.0478, 1.0011, 0.9862, 1.0343, 0.9794, 1.0244, 1.0153,\n",
       "                      1.0293, 1.0326, 1.0500, 1.0093, 1.0030, 0.9699, 1.0175, 0.9905, 0.9823,\n",
       "                      1.0270, 1.0131, 0.9991, 1.0240, 1.0349, 1.0324, 1.0081, 1.0410, 1.0682,\n",
       "                      0.9576, 0.9762, 1.0262, 1.0234, 1.0204, 1.0453, 0.9877, 0.9879, 0.9858,\n",
       "                      1.0098, 0.9868, 1.0601, 1.0208, 1.0053, 1.0383, 0.9981, 0.9805, 1.0172,\n",
       "                      1.0166, 1.0790, 0.9639, 1.0390, 0.9812, 1.0084, 0.9949, 1.0277, 1.0084,\n",
       "                      1.0059, 1.0174, 1.0446, 0.9934, 1.0284, 1.0162, 1.0121, 1.0147, 1.0028,\n",
       "                      1.0362, 0.9979, 1.0352, 1.0409, 1.0262, 1.0084, 1.0022, 1.0593, 1.0003,\n",
       "                      1.0004, 1.0285, 0.9888, 1.0277, 0.9822, 1.0528, 1.0238, 1.0348, 1.0277,\n",
       "                      1.0223, 1.0096, 1.0062, 1.0377, 0.9916, 1.0180, 0.9830, 1.0103, 1.0475,\n",
       "                      0.9987, 1.0391, 1.0572, 1.0128, 1.0149, 1.0265, 1.0237, 0.9716, 0.9813,\n",
       "                      1.0163, 0.9980, 1.0436, 1.0766, 1.0168, 1.0137, 1.0601, 0.9995, 0.9993,\n",
       "                      1.0193, 0.9763, 1.0057, 0.9991, 1.0521, 1.0538, 1.0463, 1.0365, 1.0162,\n",
       "                      0.9887, 1.0481, 0.9972, 0.9750, 0.9847, 0.9816, 1.0339, 1.0028, 1.0507,\n",
       "                      1.0327, 0.9416, 1.0862], device='cuda:0')),\n",
       "             ('hnorm.weight',\n",
       "              tensor([1.8843, 1.7425, 1.7945, 1.7415, 1.8459, 1.7960, 1.7446, 1.6996, 1.8101,\n",
       "                      1.7413, 1.8373, 1.7436, 1.7207, 1.7215, 1.7609, 1.7494, 1.7631, 1.7845,\n",
       "                      1.7834, 1.7630, 1.7843, 1.7417, 1.8848, 1.7386, 1.7070, 1.8235, 1.7530,\n",
       "                      1.7481, 1.7819, 1.6802, 1.8137, 1.7535, 1.7930, 1.7543, 1.7861, 1.7034,\n",
       "                      1.8425, 1.8891, 1.8207, 1.6947, 1.8080, 1.8671, 1.8147, 1.7897, 1.7468,\n",
       "                      1.7903, 1.7805, 1.8679, 1.8810, 1.7781, 1.6783, 1.7784, 1.8436, 1.6840,\n",
       "                      1.7808, 1.7681, 1.7685, 1.7569, 1.9497, 1.8147, 1.8149, 1.7671, 1.9073,\n",
       "                      1.7714, 1.8340, 1.7882, 1.7971, 1.6589, 1.8290, 1.8489, 1.7780, 1.8426,\n",
       "                      1.7443, 1.7951, 1.7391, 1.8562, 1.7416, 1.8331, 1.8333, 1.7983, 1.7121,\n",
       "                      1.7439, 1.8645, 1.7012, 1.8510, 1.7991, 1.7853, 1.7341, 1.7832, 1.7494,\n",
       "                      1.8008, 1.7549, 1.7997, 1.7391, 1.7647, 1.7657, 1.8285, 1.7846, 1.8322,\n",
       "                      1.7860, 1.7328, 1.8082, 1.7083, 1.8587, 1.7385, 1.7550, 1.8380, 1.7700,\n",
       "                      1.7279, 1.7389, 1.7771, 1.7869, 1.7776, 1.9116, 1.8794, 1.8186, 1.7878,\n",
       "                      1.7547, 1.7777, 1.8456, 1.7813, 1.8267, 1.7355, 1.7856, 1.7044, 1.7850,\n",
       "                      1.8210, 1.7650, 1.8118, 1.7081, 1.7461, 1.8686, 1.7686, 1.7686, 1.7148,\n",
       "                      1.8696, 1.8575, 1.7696, 1.6989, 1.8273, 1.7803, 1.7608, 1.7713, 1.7622,\n",
       "                      1.7010, 1.8012, 1.7706, 1.7553, 1.8473, 1.8296, 1.7611, 1.9336, 1.7294,\n",
       "                      1.8453, 1.8462, 1.7335, 1.7443, 1.7720, 1.7412, 1.6932, 1.7408, 1.8050,\n",
       "                      1.7527, 1.7607, 1.7438, 1.7424, 1.8037, 1.7700, 1.7607, 1.7563, 1.8126,\n",
       "                      1.8186, 1.8396, 1.7661, 1.7464, 1.7909, 1.7360, 1.7847, 1.7665, 1.8474,\n",
       "                      1.7204, 1.7732, 1.7680, 1.7525, 1.8435, 1.7560, 1.7158, 1.7900, 1.7189,\n",
       "                      1.7690, 1.8013, 1.7116, 1.6806, 1.7498, 1.8503, 1.8151, 1.7702, 1.7563,\n",
       "                      1.7900, 1.8407, 1.8583, 1.8041, 1.8008, 1.8306, 1.7704, 1.7450, 1.8426,\n",
       "                      1.8131, 1.6747, 1.8339, 1.7812, 1.8516, 1.7696, 1.8256, 1.8627, 1.7249,\n",
       "                      1.8838, 1.7378, 1.8333, 1.7729, 1.6533, 1.7561, 1.7805, 1.8199, 1.7210,\n",
       "                      1.7414, 1.5701, 1.7537, 1.7623, 1.7938, 1.7714, 1.9833, 1.7395, 1.7956,\n",
       "                      1.7279, 1.8245, 1.7967, 1.7708, 1.7808, 1.9215, 1.8364, 1.7956, 1.7570,\n",
       "                      1.7573, 1.7606, 1.7310, 1.8093, 1.8098, 1.7394, 1.7548, 1.8883, 1.7707,\n",
       "                      1.8031, 1.7012, 1.8442, 1.8197, 1.7558, 1.8016, 1.7663, 1.7176, 1.8281,\n",
       "                      1.7372, 1.8425, 1.8112, 1.8794, 1.7837, 1.7412, 1.8657, 1.7250, 1.7089,\n",
       "                      1.7497, 1.8228, 1.8063, 1.8655, 1.7624, 1.8035, 1.7239, 1.6932, 1.8358,\n",
       "                      1.7486, 1.8495, 1.7717, 1.8170, 1.8247, 1.7429, 1.7260, 1.7313, 1.8606,\n",
       "                      1.8424, 1.7814, 1.7787, 1.7496, 1.7799, 1.8147, 1.7552, 1.9017, 1.8696,\n",
       "                      1.7640, 1.7721, 1.6805, 1.7907, 1.8067, 1.8386, 1.8442, 1.8239, 1.7377,\n",
       "                      1.8111, 1.7484, 1.8290, 1.7538, 1.8083, 1.8028, 1.7485, 1.8451, 1.8523,\n",
       "                      1.8156, 1.9218, 1.7573, 1.7687, 1.7237, 1.7863, 1.7822, 1.7530, 1.7691,\n",
       "                      1.8057, 1.7316, 1.7643, 1.7248, 1.7618, 1.6835, 1.8531, 1.7556, 1.8681,\n",
       "                      1.9061, 1.7130, 1.7861, 1.7654, 1.7287, 1.8649, 1.8147, 1.7849, 1.7465,\n",
       "                      1.7717, 1.8822, 1.8645, 1.7314, 1.8155, 1.8005, 1.7096, 1.8307, 1.8025,\n",
       "                      1.7952, 1.8008, 1.8069, 1.7047, 1.7638, 1.6925, 1.8448, 1.9134, 1.7315,\n",
       "                      1.8195, 1.8891, 1.7402, 1.7408, 1.8220, 1.7522, 1.8602, 1.8182, 1.8396,\n",
       "                      1.9464, 1.7964, 1.7071, 1.7190, 1.7834, 1.7385, 1.7649, 1.7952, 1.7230,\n",
       "                      1.7861, 1.7503, 1.7546, 1.8240, 1.7944, 1.7748, 1.7558, 1.7925, 1.7305,\n",
       "                      1.7526, 1.8082, 1.7314, 1.7907, 1.7713, 1.7387, 1.8281, 1.6803, 1.7909,\n",
       "                      1.7216, 1.8008, 1.7606, 1.7921, 1.7744, 1.7176, 1.8628, 1.7820, 1.7785,\n",
       "                      1.7987, 1.7862, 1.9964, 0.6277, 1.7651, 1.8413, 1.8247, 1.8396, 1.7260,\n",
       "                      1.8714, 1.8161, 1.7823, 1.6988, 1.8009, 1.7192, 1.7638, 1.7292, 1.7624,\n",
       "                      1.7233, 1.7731, 1.7275, 1.7818, 1.7640, 1.8462, 1.7599, 1.7343, 1.8078,\n",
       "                      1.8277, 1.8011, 1.8069, 1.7528, 1.8177, 1.8061, 1.9249, 1.8317, 1.7964,\n",
       "                      1.7909, 1.7711, 1.8309, 1.8145, 1.7505, 1.8252, 1.7267, 1.8369, 1.7757,\n",
       "                      1.8274, 1.7849, 1.7699, 1.7917, 1.7856, 1.7652, 1.7984, 1.7707, 1.7754,\n",
       "                      1.7922, 1.7507, 1.6984, 1.8439, 1.8517, 1.7707, 1.7820, 1.8185, 1.8435,\n",
       "                      1.7702, 1.7691, 1.7456, 1.7468, 1.8505, 1.8143, 1.7726, 1.8012, 1.8443,\n",
       "                      1.7532, 1.7653, 1.7766, 1.8761, 1.7663, 1.7778, 1.8276, 1.7459, 1.8442,\n",
       "                      1.7885, 1.7191, 1.6981, 1.7174, 1.8058, 1.7956, 1.7745, 1.8083, 1.8440,\n",
       "                      1.8501, 1.8551, 1.7113, 1.9791, 1.8061, 1.8359, 1.7119, 1.8148, 1.7982,\n",
       "                      1.6962, 1.7527, 1.8623, 1.8459, 1.8096, 1.7574, 1.8634, 1.8332, 1.8001,\n",
       "                      1.8301, 1.7564, 1.8391, 1.6306, 1.7918, 1.7532, 1.7712, 1.7815, 1.7976,\n",
       "                      1.8290, 1.8257, 1.7794, 1.7335, 1.8872, 1.6693, 1.8550, 1.7342, 1.7601,\n",
       "                      1.7843, 1.7774, 1.8834, 1.7843, 1.7216, 1.7016, 1.8324, 1.7794, 1.8163,\n",
       "                      1.7067, 1.7786, 1.7601, 1.8463, 1.7974, 1.8477, 1.7870, 1.7776, 1.8100,\n",
       "                      1.7608, 1.9519, 1.7534, 1.7077, 1.8411, 1.8560, 1.8352, 1.8066, 1.8296,\n",
       "                      1.7147, 1.6885, 1.8663, 1.8174, 1.7812, 1.7959, 1.8164, 1.9007, 1.8331,\n",
       "                      1.7748, 1.6848, 1.7480, 1.7543, 1.7918, 1.7681, 1.8387, 1.7718, 1.8830,\n",
       "                      1.7401, 1.7991, 1.8142, 1.8361, 1.8370, 1.7007, 1.7087, 1.7833, 1.7351,\n",
       "                      1.7246, 1.7432, 1.6960, 1.7945, 1.7933, 1.7092, 1.7988, 1.8487, 1.7966,\n",
       "                      1.8194, 1.7571, 1.7572, 1.7833, 1.7346, 1.7659, 1.7248, 1.6850, 1.8239,\n",
       "                      1.7595, 1.7610, 1.7771, 1.7792, 1.8665, 1.7566, 1.7748, 1.8035, 1.7616,\n",
       "                      1.7939, 1.8134, 1.8717, 1.7394, 1.8106, 1.9132, 1.7587, 1.7882, 1.7281,\n",
       "                      1.6996, 1.7300, 1.7866, 1.6901, 1.7916, 1.7986, 1.8623, 1.8022, 1.7760,\n",
       "                      1.8093, 1.7439, 1.7676, 1.8405, 1.8115, 1.8149, 1.7311, 1.7852, 1.8021,\n",
       "                      1.7783, 1.7997, 1.8568, 1.8224, 1.8046, 1.7283, 1.7228, 1.7829, 1.7806,\n",
       "                      1.7600, 1.8279, 1.8186, 1.8413, 1.7750, 1.7666, 1.7453, 1.8762, 1.7512,\n",
       "                      1.7782, 1.7635, 1.8311, 1.7588, 1.7705, 1.7321, 1.7955, 1.7927, 1.6950,\n",
       "                      1.8667, 1.7762, 1.8324, 1.7917, 1.8559, 1.7294, 1.7230, 1.8217, 1.8286,\n",
       "                      1.7529, 1.8081, 1.7501, 1.7999, 1.7768, 1.8267, 1.8225, 1.7923, 1.7354,\n",
       "                      1.8243, 1.7119, 1.7946, 1.7989, 1.8207, 1.8343, 1.7483, 1.6928, 1.7414,\n",
       "                      1.8593, 1.8131, 1.8361, 1.8040, 1.7488, 1.8043, 1.8301, 1.7723, 1.6960,\n",
       "                      1.7300, 1.7838, 1.8435, 1.7408, 1.8172, 1.7245, 1.7959, 1.8296, 1.7753,\n",
       "                      1.8113, 1.7957, 1.7713, 1.7246, 1.7903, 1.7324, 1.8218, 1.8110, 1.7728,\n",
       "                      1.8585, 1.7577, 1.8237, 1.6778, 1.7904, 1.8344, 1.7962, 1.7654, 1.7448,\n",
       "                      1.7350, 1.8173, 1.8477, 1.7054, 1.7536, 1.7800, 1.7672, 1.7448, 1.8258,\n",
       "                      1.8761, 1.7389, 1.8726, 1.7015, 1.7549, 1.7492, 1.7427, 1.8378, 1.7708,\n",
       "                      1.7821, 1.7676, 1.8264, 1.7965, 1.8300, 1.7721, 1.7450, 1.7985, 1.7796,\n",
       "                      1.8133, 1.7737, 1.7343, 1.7420, 1.8377, 1.7940, 1.6996, 1.8434, 1.7617,\n",
       "                      1.8554, 1.8343, 1.8300], device='cuda:0')),\n",
       "             ('clf_head.weight',\n",
       "              tensor([[-0.0257, -0.0468,  0.0027,  ..., -0.0084, -0.0320, -0.0178],\n",
       "                      [-0.0079,  0.0099,  0.0449,  ..., -0.0026,  0.0241,  0.0037],\n",
       "                      [-0.0330, -0.0507,  0.0047,  ..., -0.0071, -0.0324, -0.0127],\n",
       "                      ...,\n",
       "                      [-0.0099, -0.0355, -0.0096,  ..., -0.0109, -0.0392, -0.0260],\n",
       "                      [-0.0210, -0.0344,  0.0040,  ..., -0.0088, -0.0459, -0.0307],\n",
       "                      [-0.0137, -0.0350, -0.0016,  ..., -0.0129, -0.0410, -0.0250]],\n",
       "                     device='cuda:0')),\n",
       "             ('clf_head.bias',\n",
       "              tensor([-0.9082,  0.1326, -0.8809,  ..., -0.8215, -0.8100, -0.8634],\n",
       "                     device='cuda:0'))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "34fd142d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    *custom implementaiton*\n",
    "    This class provides Rotary Positional Encodign for the input data (word embeddings)\n",
    "    Usally applied on Q and V\n",
    "    reference: https://arxiv.org/abs/2104.09864\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.cos_mthetas = None\n",
    "        self.sin_mthetas = None\n",
    "        self.__set_thetas()\n",
    "        \n",
    "    def __set_thetas(self):\n",
    "        \"\"\"This sets the parameters of the rope as per the formula\n",
    "        Θ = {θi = 10000−2(i−1)/d, i ∈ [1, 2, ..., d/2]}\n",
    "\n",
    "        it also precomputes real (cos mtheta) and imaginary (sin mtheta) parts of the RoPE equation to further use in the \n",
    "        `forward` method\n",
    "\n",
    "        reference: https://arxiv.org/abs/2104.09864\n",
    "        \"\"\"\n",
    "        assert self.config.head_size % 2 == 0, f\"Head size:{self.config.head_size} shouls be even number\"\n",
    "        self.d = self.config.head_size\n",
    "        self.m = self.config.seq_len\n",
    "        \n",
    "        i_s = torch.tensor(range(1,self.d//2+1))\n",
    "        i_s = torch.cat([i_s,i_s], axis=-1)\n",
    "        m_s = torch.tensor(range(self.m)).unsqueeze(axis=-1)\n",
    "        \n",
    "        thetas = 10000 ** (-2*(i_s-1)/self.d)\n",
    "        self.cos_mthetas = torch.cos(m_s * thetas).to(self.config.rank)\n",
    "        self.sin_mthetas = torch.sin(m_s * thetas).to(self.config.rank)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Encodes positional information into context embeddings Q and V\n",
    "        It expects the shape of the input `x` to be 4 dimensional\n",
    "        x should be in the form x[b,h,t,s]\n",
    "        here,   b - batch dimension\n",
    "                h - number of heads\n",
    "                t - time steps (number of words)\n",
    "                s - size of the head\n",
    "\n",
    "        reference: https://arxiv.org/abs/2104.09864                \n",
    "        \"\"\"\n",
    "        d, t   = x.shape[-1], x.shape[-2]\n",
    "        \n",
    "        assert d == self.d\n",
    "        \n",
    "        x = self.cos_mthetas[:t,:] * x + self.sin_mthetas[:t,:] * torch.cat([-1 * x[:,:,:,self.d//2:],x[:,:,:,:self.d//2]], axis=-1)\n",
    "        return x        \n",
    "\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This module is a multi head attention part of the Transformer.\n",
    "    This computes the scaled dot product between the Q, K and V\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.n_rep = self.config.n_heads // self.config.n_kv_heads\n",
    "        self.query = nn.Linear(self.config.dim, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        self.key = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.value = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.rope = RoPE(self.config)\n",
    "        self.proj = nn.Linear(self.config.n_heads * self.config.head_size, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        \n",
    "    def forward(self, x, y=None, kv_cache=None):\n",
    "        b,t,d = x.shape\n",
    "        q = self.query(x).view(b,t,self.config.n_heads,self.config.head_size)\n",
    "        k = self.key(x).view(b,t,self.config.n_kv_heads,self.config.head_size)\n",
    "        v = self.value(x).view(b,t,self.config.n_kv_heads,self.config.head_size)        \n",
    "        \n",
    "        ## Add rotary embeddings        \n",
    "        q = self.rope(q.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        k = self.rope(k.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        \n",
    "        ##GQA \n",
    "        #k = repeat_kv(k,self.n_rep)\n",
    "        #v = repeat_kv(v,self.n_rep)\n",
    "        \n",
    "        # make heads into a batch dimension\n",
    "        q = q.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "        \n",
    "        ## concat k,v from cache\n",
    "        k = torch.cat((k, kv_cache), axis=2)\n",
    "        v = torch.cat((v, kv_cache), axis=2)\n",
    "        print(f\"Shapes of q, k, v is: {q.shape}, {k.shape}, {v.shape}\")\n",
    "        \n",
    "        ## Flash attention\n",
    "        x = nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "        \n",
    "        ## add x to the cache\n",
    "        kv_cache = torch.cat((x,kv_cache), axis=2)\n",
    "        \n",
    "         # restore time as batch dimension and concat heads\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, -1)\n",
    "\n",
    "        # final projection into the residual stream\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        return x, kv_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c70122b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 768]), torch.Size([1, 8, 9, 96]))"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(size=(1,1,768)).to('cuda')\n",
    "kv_cache = torch.rand(size=(1, 8, 9, 96)).to('cuda')\n",
    "x.shape, kv_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3af36f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3f61ab0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of q, k, v is: torch.Size([1, 8, 1, 96]), torch.Size([1, 8, 11, 96]), torch.Size([1, 8, 11, 96])\n"
     ]
    }
   ],
   "source": [
    "x, kv_cache = mha(x,kv_cache=kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "aca81640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 768]), torch.Size([1, 8, 11, 96]))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, kv_cache.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "009b69bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b,t,d = x.shape\n",
    "\n",
    "q = self.query(x).view(b,t,self.config.n_heads,self.config.head_size)\n",
    "k = self.key(x).view(b,t,self.config.n_kv_heads,self.config.head_size)\n",
    "v = self.value(x).view(b,t,self.config.n_kv_heads,self.config.head_size)        \n",
    "\n",
    "## Add rotary embeddings        \n",
    "q = self.rope(q.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "k = self.rope(k.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "\n",
    "##GQA \n",
    "#k = repeat_kv(k,self.n_rep)\n",
    "#v = repeat_kv(v,self.n_rep)\n",
    "\n",
    "# make heads into a batch dimension\n",
    "q = q.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "k = k.transpose(1, 2)\n",
    "v = v.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "05ed5003",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m y1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m      2\u001b[0m y2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43my1\u001b[49m\u001b[43m,\u001b[49m\u001b[43my2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got NoneType"
     ]
    }
   ],
   "source": [
    "y1 = None\n",
    "y2 = torch.rand(size=(2,3,4))\n",
    "torch.cat((y1,y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "da19d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 16\n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = 1\n",
    "        self.total_params = 0\n",
    "        self.saved_checkpoint_path = \"saved_artifacts/models/model240221\"\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"\n",
    "        self.n_test_examples = 10000        \n",
    "        self.steps_to_serialize = 6\n",
    "        self.rank = 0\n",
    "        self.enable_kv_cache = False\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075a93e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "8066666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import inspect\n",
    "\n",
    "\n",
    "\n",
    "class RoPE(nn.Module):\n",
    "    \"\"\"\n",
    "    *custom implementaiton*\n",
    "    This class provides Rotary Positional Encodign for the input data (word embeddings)\n",
    "    Usally applied on Q and V\n",
    "    reference: https://arxiv.org/abs/2104.09864\n",
    "    \"\"\"\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.cos_mthetas = None\n",
    "        self.sin_mthetas = None\n",
    "        self.__set_thetas()\n",
    "        \n",
    "    def __set_thetas(self):\n",
    "        \"\"\"This sets the parameters of the rope as per the formula\n",
    "        Θ = {θi = 10000−2(i−1)/d, i ∈ [1, 2, ..., d/2]}\n",
    "\n",
    "        it also precomputes real (cos mtheta) and imaginary (sin mtheta) parts of the RoPE equation to further use in the \n",
    "        `forward` method\n",
    "\n",
    "        reference: https://arxiv.org/abs/2104.09864\n",
    "        \"\"\"\n",
    "        assert self.config.head_size % 2 == 0, f\"Head size:{self.config.head_size} shouls be even number\"\n",
    "        self.d = self.config.head_size\n",
    "        self.m = self.config.seq_len\n",
    "        \n",
    "        i_s = torch.tensor(range(1,self.d//2+1))\n",
    "        i_s = torch.cat([i_s,i_s], axis=-1)\n",
    "        m_s = torch.tensor(range(self.m)).unsqueeze(axis=-1)\n",
    "        \n",
    "        thetas = 10000 ** (-2*(i_s-1)/self.d)\n",
    "        self.cos_mthetas = torch.cos(m_s * thetas).to(self.config.rank)\n",
    "        self.sin_mthetas = torch.sin(m_s * thetas).to(self.config.rank)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        Encodes positional information into context embeddings Q and V\n",
    "        It expects the shape of the input `x` to be 4 dimensional\n",
    "        x should be in the form x[b,h,t,s]\n",
    "        here,   b - batch dimension\n",
    "                h - number of heads\n",
    "                t - time steps (number of words)\n",
    "                s - size of the head\n",
    "\n",
    "        reference: https://arxiv.org/abs/2104.09864                \n",
    "        \"\"\"\n",
    "        d, t   = x.shape[-1], x.shape[-2]\n",
    "        \n",
    "        assert d == self.d\n",
    "        \n",
    "        x = self.cos_mthetas[:t,:] * x + self.sin_mthetas[:t,:] * torch.cat([-1 * x[:,:,:,self.d//2:],x[:,:,:,:self.d//2]], axis=-1)\n",
    "        return x        \n",
    "    \n",
    "    \n",
    "    \n",
    "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    *custom implementaiton*\n",
    "    This funciton is used for grouped query attention GQA.\n",
    "    This replicates the K and V to match the dimension along the heads.\n",
    "    Expected shape of the input(x) is\n",
    "        x[b,t,h,s]\n",
    "        here,   b - batch dimension\n",
    "                t - time steps (number of words)\n",
    "                h - number of heads                \n",
    "                s - size of the head\n",
    "\n",
    "    reference: https://arxiv.org/pdf/2305.13245\n",
    "    \"\"\"\n",
    "    bs, slen, n_kv_heads, head_dim = x.shape\n",
    "    if n_rep == 1:\n",
    "        return x\n",
    "    return (\n",
    "        x[:, :, :, None, :]\n",
    "        .expand(bs, slen, n_kv_heads, n_rep, head_dim)\n",
    "        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)\n",
    "    )\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    This module is a multi head attention part of the Transformer.\n",
    "    This computes the scaled dot product between the Q, K and V\n",
    "    \"\"\"\n",
    "    def __init__(self, config, layer_id):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_id = layer_id\n",
    "        self.n_rep = self.config.n_heads // self.config.n_kv_heads\n",
    "        self.query = nn.Linear(self.config.dim, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        self.key = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.value = nn.Linear(self.config.dim, self.config.n_kv_heads * self.config.head_size, bias=False)\n",
    "        self.rope = RoPE(self.config)\n",
    "        self.proj = nn.Linear(self.config.n_heads * self.config.head_size, self.config.n_heads * self.config.head_size, bias=False)\n",
    "        \n",
    "    def forward(self, x, y=None, enable_kv_cache=False, kv_cache=None):\n",
    "        b,t,d = x.shape\n",
    "        q = self.query(x).view(b,t,self.config.n_heads,self.config.head_size)\n",
    "        k = self.key(x).view(b,t,self.config.n_kv_heads,self.config.head_size)\n",
    "        v = self.value(x).view(b,t,self.config.n_kv_heads,self.config.head_size)        \n",
    "        \n",
    "        ## Add rotary embeddings        \n",
    "        q = self.rope(q.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        k = self.rope(k.permute(0,2,1,3)).permute(0,2,1,3)\n",
    "        \n",
    "        ##GQA \n",
    "        #k = repeat_kv(k,self.n_rep)\n",
    "        #v = repeat_kv(v,self.n_rep)\n",
    "        \n",
    "        # make heads into a batch dimension\n",
    "        q = q.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n",
    "        k = k.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        ## concat k,v from cache\n",
    "        if kv_cache[self.layer_id][0] is not None:\n",
    "            #print(f\"Detected existing cache for the layer {self.layer_id}\")\n",
    "            k = torch.cat((kv_cache[self.layer_id][0],k), axis=2)\n",
    "            v = torch.cat((kv_cache[self.layer_id][1],v), axis=2)\n",
    "        \n",
    "        ## Flash attention\n",
    "        x = nn.functional.scaled_dot_product_attention(q, k, v, is_causal=True)\n",
    "\n",
    "        ## add the new token to the cache\n",
    "        if enable_kv_cache:\n",
    "            kv_cache[self.layer_id] = (k,v)\n",
    "        \n",
    "         # restore time as batch dimension and concat heads\n",
    "        x = x.transpose(1, 2).contiguous().view(b, t, -1)\n",
    "\n",
    "        # final projection into the residual stream\n",
    "        x = self.proj(x)\n",
    "        \n",
    "        \n",
    "        return x, kv_cache        \n",
    "    \n",
    "    \n",
    "class FeedForwordNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a stack of linear layers and one SiLU activation\n",
    "    All togather forms SwiGLU\n",
    "    reference: https://arxiv.org/pdf/2002.05202\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config        \n",
    "        hidden_units = int(self.config.dim * 4 * 2/3) ##\n",
    "        hidden_units = int(hidden_units - (hidden_units % self.config.multiple_of) + self.config.multiple_of)\n",
    "        \n",
    "        self.w = nn.Linear(self.config.dim, hidden_units)\n",
    "        self.v = nn.Linear(self.config.dim, hidden_units)\n",
    "        self.w2 = nn.Linear(hidden_units, self.config.dim)\n",
    "        self.silu = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.w2(self.silu(self.w(x)) * self.v(x))\n",
    "        return out\n",
    "    \n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    *custom implementaiton*\n",
    "    Similar to Layer norm but with only scaling and omiiting the centering part.\n",
    "    reference: https://arxiv.org/abs/1910.07467\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.dim = config.dim\n",
    "        self.weight = nn.Parameter(torch.ones(self.dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self._norm(x.float()).type_as(x)\n",
    "        return output * self.weight\n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This is equal to one block or layer in the transformer, comprises of \n",
    "    RMSNorm\n",
    "    Residual connection\n",
    "    Multi Head Attention\n",
    "    feedforword block\n",
    "    \"\"\"\n",
    "    def __init__(self, config, i):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_id = i\n",
    "        self.mha = MultiHeadAttention(self.config, self.layer_id)\n",
    "        self.ffn = FeedForwordNetwork(self.config)\n",
    "        self.anorm = RMSNorm(config)\n",
    "        self.fnorm = RMSNorm(config)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs:tuple):\n",
    "        x, enable_kv_cache, kv_cache = inputs        \n",
    "        x_new, kv_cache = self.mha(self.anorm(x), enable_kv_cache=enable_kv_cache, kv_cache=kv_cache)\n",
    "        x = x + x_new\n",
    "        x = x + self.ffn(self.fnorm(x))\n",
    "        return (x, enable_kv_cache, kv_cache)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.enable_kv_cache = config.enable_kv_cache #boolean value\n",
    "        self.kv_cache = [(None,None)   for _ in range(self.config.n_layers)]\n",
    "        self.embedding_layer = nn.Embedding(self.config.vocab_size, self.config.dim)\n",
    "        self.layers = nn.Sequential(*[AttentionLayer(self.config, i) for i in range(self.config.n_layers)])\n",
    "        self.hnorm = RMSNorm(config)\n",
    "        self.clf_head = nn.Linear(self.config.dim, self.config.vocab_size)\n",
    "    \n",
    "    def forward(self, x, y=None):\n",
    "        x = self.embedding_layer(x)\n",
    "        x, enable_kv_cache, self.kv_cache = self.layers((x, self.enable_kv_cache, self.kv_cache))\n",
    "        x = self.hnorm(x)\n",
    "        x = self.clf_head(x)\n",
    "        if self.enable_kv_cache:\n",
    "            return x, self.kv_cache\n",
    "        else:\n",
    "            return x\n",
    "        \n",
    "    def reset_kv_cache(self):\n",
    "        self.kv_cache = [(None,None)   for _ in range(self.config.n_layers)]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "64f47cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "53b9f33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0, high = 500, size=(1,30)).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "9964fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 18 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "14b5d17a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 4096]),\n",
       " torch.Size([1, 8, 503, 96]),\n",
       " torch.Size([1, 8, 503, 96]))"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred.shape, kv_cache[0][0].shape, kv_cache[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3c958b2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 30, 4096])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "52b77e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = nn.Sequential(*[AttentionLayer(config, i) for i in range(config.n_layers)]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "98e2b6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,10,768).to(\"cuda\")\n",
    "kv_cache = [None] * 8\n",
    "enable_kv_cache = True\n",
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c289c782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enable kv_cache status in MultiHeadAttention: True\n"
     ]
    }
   ],
   "source": [
    "x, enable_kv_cache, kv_cache = AttentionLayer(config, i).to('cuda')(x, enable_kv_cache, kv_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d089cf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entered Attention Layer forward method\n",
      "Enable kv_cache status in MultiHeadAttention: True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[[1.1306, 0.5752, 1.1342,  ..., 1.0519, 0.9459, 1.2192],\n",
       "          [1.2281, 0.1810, 0.6667,  ..., 0.9047, 0.7232, 0.9605],\n",
       "          [1.0895, 0.1010, 0.7287,  ..., 1.3746, 0.8798, 0.4702],\n",
       "          ...,\n",
       "          [0.7268, 0.3715, 1.1591,  ..., 1.7017, 0.9444, 0.9386],\n",
       "          [1.3095, 0.9289, 1.3397,  ..., 1.7179, 0.7007, 1.1951],\n",
       "          [0.7873, 0.2379, 1.1743,  ..., 1.6424, 1.0313, 0.3636]]],\n",
       "        device='cuda:0', grad_fn=<AddBackward0>),\n",
       " True,\n",
       " [tensor([[[[ 0.2900, -1.4815, -0.4450,  ..., -1.0827,  0.9169,  0.7151],\n",
       "            [ 0.0969, -1.0952, -0.3532,  ..., -0.9529,  0.7573,  0.3139],\n",
       "            [ 0.1990, -1.0645, -0.3318,  ..., -0.8191,  0.7816,  0.2842],\n",
       "            ...,\n",
       "            [ 0.1353, -0.9853, -0.2893,  ..., -0.7547,  0.6837,  0.4010],\n",
       "            [ 0.1702, -1.0481, -0.3122,  ..., -0.7979,  0.7477,  0.4030],\n",
       "            [ 0.1007, -1.0350, -0.2514,  ..., -0.7607,  0.7458,  0.4547]],\n",
       "  \n",
       "           [[-1.0828,  0.5720,  1.0873,  ...,  0.2394,  0.0161, -0.3305],\n",
       "            [-0.6952,  0.7441,  0.9005,  ...,  0.2604,  0.0602,  0.0057],\n",
       "            [-0.5303,  0.7552,  0.7026,  ...,  0.2242,  0.1135, -0.0812],\n",
       "            ...,\n",
       "            [-0.5686,  0.6380,  0.6488,  ...,  0.3328,  0.0823, -0.1322],\n",
       "            [-0.5538,  0.6146,  0.6694,  ...,  0.3124,  0.0552, -0.1488],\n",
       "            [-0.5440,  0.6203,  0.6659,  ...,  0.3092,  0.0816, -0.0721]],\n",
       "  \n",
       "           [[-0.7079, -0.0283, -0.4179,  ..., -0.1988, -0.1860, -0.5485],\n",
       "            [-0.3153,  0.0036, -0.3877,  ..., -0.1303,  0.0808, -0.1182],\n",
       "            [-0.2005, -0.0551, -0.2784,  ..., -0.0758,  0.1728, -0.2923],\n",
       "            ...,\n",
       "            [-0.0835, -0.0533, -0.1869,  ..., -0.0659,  0.0229, -0.2936],\n",
       "            [-0.0694, -0.0950, -0.1484,  ..., -0.1295, -0.0080, -0.2790],\n",
       "            [-0.0359, -0.0732, -0.1377,  ..., -0.1088, -0.0424, -0.2886]],\n",
       "  \n",
       "           ...,\n",
       "  \n",
       "           [[ 0.2047, -1.2178,  0.1200,  ...,  0.4296, -0.6248,  0.0052],\n",
       "            [-0.0966, -1.0929,  0.3327,  ...,  0.3858, -0.9750,  0.1707],\n",
       "            [ 0.0689, -1.0361,  0.5193,  ...,  0.1751, -0.7344,  0.1226],\n",
       "            ...,\n",
       "            [ 0.0391, -0.9744,  0.4586,  ...,  0.0120, -0.4576,  0.0831],\n",
       "            [ 0.0689, -0.9575,  0.4581,  ...,  0.0494, -0.4637,  0.0510],\n",
       "            [ 0.1026, -0.9780,  0.5012,  ...,  0.0416, -0.4590,  0.0392]],\n",
       "  \n",
       "           [[-1.0101, -1.1661,  0.6746,  ...,  0.1988,  0.2587,  1.0582],\n",
       "            [-0.5717, -0.8755,  0.6516,  ...,  0.0125,  0.3770,  1.0074],\n",
       "            [-0.5467, -0.7082,  0.6008,  ..., -0.0263,  0.2715,  1.0364],\n",
       "            ...,\n",
       "            [-0.3781, -0.5644,  0.9144,  ...,  0.0440,  0.3039,  1.2712],\n",
       "            [-0.3787, -0.5819,  0.8418,  ...,  0.1054,  0.3731,  1.2062],\n",
       "            [-0.4211, -0.5888,  0.9041,  ...,  0.1435,  0.3514,  1.2232]],\n",
       "  \n",
       "           [[ 0.0917, -0.0590, -0.2925,  ...,  0.0025,  0.0164,  1.4174],\n",
       "            [ 0.3611, -0.1416, -0.2052,  ..., -0.1055,  0.0359,  1.5216],\n",
       "            [ 0.5075, -0.3665, -0.0941,  ..., -0.0727,  0.0087,  1.3323],\n",
       "            ...,\n",
       "            [ 0.4440, -0.1052, -0.1371,  ...,  0.0779,  0.1411,  1.1597],\n",
       "            [ 0.4025, -0.0581, -0.1627,  ...,  0.0662,  0.1358,  1.1704],\n",
       "            [ 0.3821, -0.0597, -0.2119,  ...,  0.0899,  0.1608,  1.1922]]]],\n",
       "         device='cuda:0', grad_fn=<ScaledDotProductEfficientAttentionBackward0>),\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None,\n",
       "  None])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers((x, enable_kv_cache, kv_cache))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "709deb69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): AttentionLayer(\n",
       "    (mha): MultiHeadAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=False)\n",
       "      (rope): RoPE()\n",
       "      (proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "    )\n",
       "    (ffn): FeedForwordNetwork(\n",
       "      (w): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (v): Linear(in_features=768, out_features=2304, bias=True)\n",
       "      (w2): Linear(in_features=2304, out_features=768, bias=True)\n",
       "      (silu): SiLU()\n",
       "    )\n",
       "    (anorm): RMSNorm()\n",
       "    (fnorm): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "24e809ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 16\n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = 1\n",
    "        self.total_params = 0\n",
    "        self.saved_checkpoint_path = \"saved_artifacts/models/model240221\"\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"\n",
    "        self.n_test_examples = 10000        \n",
    "        self.steps_to_serialize = 6\n",
    "        self.rank = 0\n",
    "        self.enable_kv_cache = False,\n",
    "        self.n_val_examples = -1\n",
    "        \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "daf76144",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "0697a33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model to device: 0\n"
     ]
    }
   ],
   "source": [
    "from automodel import AutoModel\n",
    "\n",
    "auto_model = AutoModel(model, config, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "adb1fe90",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[311], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01minference_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TinyStories\n\u001b[1;32m----> 2\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mTinyStories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetTestDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mddp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\A_DATA\\E\\AI\\usecases\\TinyStories\\inference_dataset.py:49\u001b[0m, in \u001b[0;36mTinyStories.getTestDataLoader\u001b[1;34m(self, ddp)\u001b[0m\n\u001b[0;32m     47\u001b[0m sampler \u001b[38;5;241m=\u001b[39m DistributedSampler(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_dataset) \u001b[38;5;28;01mif\u001b[39;00m ddp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     48\u001b[0m shuffle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ddp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_loader\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:349\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 349\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    350\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    351\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\venv_clear_text\\lib\\site-packages\\torch\\utils\\data\\sampler.py:140\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "from inference_dataset import TinyStories\n",
    "dataloader = TinyStories(config).getTestDataLoader(ddp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41936ff0",
   "metadata": {},
   "source": [
    "## Testign the generate functionality with KV caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d60c35c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import tqdm\n",
    "import os\n",
    "import pickle\n",
    "from model import Model\n",
    "import sentencepiece as spm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.vocab_size = 4096\n",
    "        self.dim = 768\n",
    "        self.n_heads = 8\n",
    "        self.head_size = self.dim // self.n_heads\n",
    "        self.n_layers = 8\n",
    "        self.n_kv_heads = 8\n",
    "        self.seq_len = 1024\n",
    "        self.multiple_of = 256                \n",
    "        self.batch_size = 16\n",
    "        self.global_batch_size = 150_000 # number of tokens per update\n",
    "        self.world_size = torch.cuda.device_count()\n",
    "        self.grad_accumulation_steps = int(self.global_batch_size/(self.seq_len * self.batch_size * self.world_size))\n",
    "        self.grad_clip = 1.0\n",
    "        self.learning_rate = 5e-4\n",
    "        self.min_lr = self.learning_rate / 10\n",
    "        self.warmup_steps = 1000\n",
    "        self.total_params = 0\n",
    "        self.tokenizer_path = \"saved_artifacts/tokenizers\"        \n",
    "        self.rank = 0\n",
    "        self.enable_kv_cache = False\n",
    "\n",
    "\n",
    "config = Config()\n",
    "model = Model(config)    \n",
    "checkpoint = torch.load(\"saved_artifacts/models/model240221/checkpoint.pt\")\n",
    "## load model state dict\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "model = model.to(\"cuda\")\n",
    "model.eval()\n",
    "## Tokenizer\n",
    "tokenizer = spm.SentencePieceProcessor(model_file='saved_artifacts/tokenizers/tok_4096.model')        \n",
    "\n",
    "\n",
    "\n",
    "def generate(prompt, max_new_tokens=300, temperature=0.0):\n",
    "    model.reset_kv_cache()\n",
    "    idx = torch.tensor(tokenizer.encode([prompt], add_bos=True), dtype=torch.long).to(\"cuda\")\n",
    "    t = 0\n",
    "    start_time = datetime.now()    \n",
    "    gen_loop = tqdm.tqdm(total=max_new_tokens, desc=\"gen_progress\")\n",
    "    \n",
    "    while t <= max_new_tokens and int(idx[:,-1].item()) != int(tokenizer.bos_id()):\n",
    "        #print(f\"Timestep: {t}\")\n",
    "        if config.enable_kv_cache and t != 0:\n",
    "            prompt = torch.unsqueeze(idx[:,-1], dim=-1)\n",
    "        else:\n",
    "            prompt = idx[:,-config.seq_len:]                \n",
    "            \n",
    "        #print(f\"Shape of the prompt: {prompt.shape}\")\n",
    "        #print(f\"prompt: {prompt}\")\n",
    "            \n",
    "        pred = model(prompt)\n",
    "        logits = pred[:,-1,:]\n",
    "\n",
    "        if temperature == 0.0:\n",
    "            # \"sample\" the single most likely index\n",
    "            _, logits = torch.topk(logits, k=1, dim=-1)\n",
    "        else:\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits / temperature\n",
    "\n",
    "        #logits = top_k_top_p_filtering(logits, top_p=top_p, top_k=top_k)\n",
    "\n",
    "\n",
    "        logits = torch.softmax(logits, axis=-1)\n",
    "        next_idx = torch.multinomial(logits, num_samples=1)        \n",
    "        idx = torch.cat([idx, next_idx], axis=1)\n",
    "        t += 1\n",
    "        gen_loop.update(1)\n",
    "    \n",
    "    tokens_rate = int(t / ((datetime.now() - start_time).total_seconds()))\n",
    "    print(f\"token generation rate per second: {tokens_rate}\")\n",
    "    generated_text = tokenizer.decode(idx.to(\"cpu\").numpy().tolist())\n",
    "    gen_loop.write(\"\\n###############################\")\n",
    "    gen_loop.write(generated_text[0])\n",
    "    gen_loop.write(\"\\n###############################\")\n",
    "\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "\n",
    "    # parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument(\"--model\", required = False, default=\"saved_artifacts/models/model240221/checkpoint.pt\", help=\"path of the model\")    \n",
    "    # parser.add_argument(\"--max_new_tokens\", required=False, default=500, help=\"Number of new tokens to generate by the model\")\n",
    "    # parser.add_argument(\"--prompt\", required=True, help=\"prompt to begain the generation with\")      \n",
    "    # parser.add_argument(\"--temperature\", required= False, default=0.0, type=float) \n",
    "    # args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae46338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gen_progress: 301it [00:05, 54.23it/s]                                                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token generation rate per second: 54\n",
      "\n",
      "###############################\n",
      "One day, Lily met a Shoggoth named Sam. Sam was very intelligent and liked to make jokes. Lily liked to laugh and play with Sam. They became good friends.\n",
      "\n",
      "One day, Lily and Sam went to the park. They saw a big slide. Lily wanted to go on the slide, but Sam was scared. He said, \"No, Lily, the slide is too high. I don't want to go.\"\n",
      "\n",
      "Lily said, \"Don't be scared, Sam. I will go with you. We can hold hands and slide together. It will be fun. Trust me, Sam.\"\n",
      "\n",
      "Sam thought for a moment. He wanted to be brave like Lily. He wanted to have fun with Lily. He said, \"Okay, Lily, I will try. But you have to hold my hand.\"\n",
      "\n",
      "Lily smiled and said, \"Of course, Sam. I will hold your hand. We are friends. We will have fun together.\"\n",
      "\n",
      "Lily and Sam climbed up the stairs. They sat on the slide. Lily held Sam's hand. Sam felt nervous, but he also felt excited. He said, \"Ready, Lily?\"\n",
      "\n",
      "Lily said, \"Ready, Sam.\"\n",
      "\n",
      "Sam pushed off and they slid down. They went very fast. They felt the wind in their hair. They laughed and screamed. They reached the bottom of the slide. They were happy.\n",
      "\n",
      "They hugged each other. They said, \"\n",
      "\n",
      "###############################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"One day, Lily met a Shoggoth\"\"\"\n",
    "model.reset_kv_cache()\n",
    "generate(prompt, temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8780abb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'idx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43midx\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'idx' is not defined"
     ]
    }
   ],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b2ad6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, S = 1, 10\n",
    "temp_mask = torch.ones(L, S, dtype=torch.bool).to(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73e25b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c785bbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37652b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_mask.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9403e5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.rand(size=(10,10)).to(temp_mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66019a10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.6474, 0.3227, 0.2829, 0.8367, 0.6159, 0.0334, 0.2276, 0.3442,\n",
       "         0.0226],\n",
       "        [0.4035, 0.0722, 0.1806, 0.3332, 0.3244, 0.3254, 0.2697, 0.1192, 0.9754,\n",
       "         0.7527],\n",
       "        [0.4055, 0.2130, 0.0036, 0.9992, 0.3523, 0.0307, 0.4982, 0.6871, 0.4626,\n",
       "         0.2362],\n",
       "        [0.3509, 0.4773, 0.7382, 0.1539, 0.9630, 0.4939, 0.1319, 0.2322, 0.0088,\n",
       "         0.4289],\n",
       "        [0.6433, 0.1399, 0.3162, 0.0618, 0.8105, 0.3141, 0.6749, 0.1558, 0.0289,\n",
       "         0.3955],\n",
       "        [0.7475, 0.8623, 0.6937, 0.2787, 0.8571, 0.8886, 0.1833, 0.7220, 0.3862,\n",
       "         0.8026],\n",
       "        [0.4127, 0.8668, 0.4294, 0.0201, 0.7800, 0.7829, 0.2312, 0.1102, 0.6026,\n",
       "         0.6292],\n",
       "        [0.2352, 0.7390, 0.3632, 0.7718, 0.2157, 0.2279, 0.3668, 0.9205, 0.3785,\n",
       "         0.3268],\n",
       "        [0.3869, 0.9441, 0.2028, 0.0188, 0.4345, 0.2676, 0.5042, 0.3824, 0.8265,\n",
       "         0.3286],\n",
       "        [0.0031, 0.5870, 0.4904, 0.6528, 0.0211, 0.6968, 0.9008, 0.6476, 0.2574,\n",
       "         0.2700]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5e5a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_clear_text",
   "language": "python",
   "name": "venv_clear_text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
